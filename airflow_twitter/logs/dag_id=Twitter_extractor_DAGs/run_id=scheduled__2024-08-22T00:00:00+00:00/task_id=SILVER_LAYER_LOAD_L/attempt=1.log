[2024-09-03 16:18:21,993] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 16:18:22,002] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 16:18:22,002] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:18:22,002] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:18:22,002] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:18:22,012] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 16:18:22,016] {standard_task_runner.py:52} INFO - Started process 280637 to run task
[2024-09-03 16:18:22,020] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '82', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1asoo91z', '--error-file', '/tmp/tmpurt67pd0']
[2024-09-03 16:18:22,022] {standard_task_runner.py:80} INFO - Job 82: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:18:22,083] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:18:22,148] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 16:18:22,153] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:18:22,155] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-22T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240822
[2024-09-03 16:18:23,548] {spark_submit.py:495} INFO - 24/09/03 16:18:23 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:18:23,548] {spark_submit.py:495} INFO - 24/09/03 16:18:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:18:23,998] {spark_submit.py:495} INFO - 24/09/03 16:18:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:18:24,792] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:18:24,805] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:18:24,840] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO ResourceUtils: ==============================================================
[2024-09-03 16:18:24,841] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:18:24,841] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO ResourceUtils: ==============================================================
[2024-09-03 16:18:24,841] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:18:24,863] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:18:24,879] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:18:24,879] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:18:24,936] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:18:24,937] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:18:24,937] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:18:24,937] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:18:24,937] {spark_submit.py:495} INFO - 24/09/03 16:18:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:18:25,122] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO Utils: Successfully started service 'sparkDriver' on port 45245.
[2024-09-03 16:18:25,148] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:18:25,182] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:18:25,203] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:18:25,205] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:18:25,209] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:18:25,226] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4fe9de2f-4044-4bfe-8dd2-72a55766974b
[2024-09-03 16:18:25,252] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:18:25,283] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:18:25,614] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:18:25,722] {spark_submit.py:495} INFO - 24/09/03 16:18:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:18:26,068] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:18:26,113] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36885.
[2024-09-03 16:18:26,113] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO NettyBlockTransferService: Server created on 192.168.2.128:36885
[2024-09-03 16:18:26,115] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:18:26,123] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36885, None)
[2024-09-03 16:18:26,134] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36885 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36885, None)
[2024-09-03 16:18:26,145] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36885, None)
[2024-09-03 16:18:26,146] {spark_submit.py:495} INFO - 24/09/03 16:18:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36885, None)
[2024-09-03 16:18:27,036] {spark_submit.py:495} INFO - 24/09/03 16:18:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:18:27,066] {spark_submit.py:495} INFO - 24/09/03 16:18:27 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 76, in <module>
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - define_extration(spark,
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 48, in define_extration
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - df = spark.read.json(src)
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 372, in json
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2024-09-03 16:18:28,759] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2024-09-03 16:18:28,778] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-22T00:00:00.00Z
[2024-09-03 16:18:28,852] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:18:28,890] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:18:28,918] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:18:28,955] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:18:28,956] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO BlockManager: BlockManager stopped
[2024-09-03 16:18:28,966] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:18:28,983] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:18:28,995] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:18:28,996] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:18:28,997] {spark_submit.py:495} INFO - 24/09/03 16:18:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-3def870a-db7b-45fe-9ae8-9aa3fc0d6280
[2024-09-03 16:18:29,006] {spark_submit.py:495} INFO - 24/09/03 16:18:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4325b15-7cb2-4a22-bf4f-40ee320b253e
[2024-09-03 16:18:29,009] {spark_submit.py:495} INFO - 24/09/03 16:18:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-3def870a-db7b-45fe-9ae8-9aa3fc0d6280/pyspark-acd78ee3-9dd7-4ea5-85a7-0faf56d34b23
[2024-09-03 16:18:29,076] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-22T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240822. Error code is: 1.
[2024-09-03 16:18:29,080] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T191821, end_date=20240903T191829
[2024-09-03 16:18:29,095] {standard_task_runner.py:92} ERROR - Failed to execute job 82 for task SILVER_LAYER_LOAD_L (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-22T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240822. Error code is: 1.; 280637)
[2024-09-03 16:18:29,103] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-09-03 16:18:29,119] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:22:24,575] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 16:22:24,582] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 16:22:24,582] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:22:24,582] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:22:24,582] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:22:24,594] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 16:22:24,597] {standard_task_runner.py:52} INFO - Started process 283342 to run task
[2024-09-03 16:22:24,600] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '82', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmphyphgv8c', '--error-file', '/tmp/tmpxlqd2caw']
[2024-09-03 16:22:24,601] {standard_task_runner.py:80} INFO - Job 82: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:22:24,638] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:22:24,693] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 16:22:24,697] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:22:24,698] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/ --process-data 20240822
[2024-09-03 16:22:26,025] {spark_submit.py:495} INFO - 24/09/03 16:22:26 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:22:26,026] {spark_submit.py:495} INFO - 24/09/03 16:22:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:22:26,682] {spark_submit.py:495} INFO - 24/09/03 16:22:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:22:27,354] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:22:27,367] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:22:27,408] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO ResourceUtils: ==============================================================
[2024-09-03 16:22:27,408] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:22:27,408] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO ResourceUtils: ==============================================================
[2024-09-03 16:22:27,409] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:22:27,438] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:22:27,454] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:22:27,455] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:22:27,502] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:22:27,502] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:22:27,503] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:22:27,503] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:22:27,504] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:22:27,672] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO Utils: Successfully started service 'sparkDriver' on port 45801.
[2024-09-03 16:22:27,696] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:22:27,722] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:22:27,737] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:22:27,738] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:22:27,742] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:22:27,752] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-03a161a3-6618-4b3e-9962-e2f749f2f0ff
[2024-09-03 16:22:27,774] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:22:27,789] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:22:27,968] {spark_submit.py:495} INFO - 24/09/03 16:22:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:22:28,018] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:22:28,236] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:22:28,276] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34277.
[2024-09-03 16:22:28,276] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO NettyBlockTransferService: Server created on 192.168.2.128:34277
[2024-09-03 16:22:28,278] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:22:28,284] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34277, None)
[2024-09-03 16:22:28,289] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34277 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34277, None)
[2024-09-03 16:22:28,293] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34277, None)
[2024-09-03 16:22:28,294] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34277, None)
[2024-09-03 16:22:28,751] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:22:28,752] {spark_submit.py:495} INFO - 24/09/03 16:22:28 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:22:29,552] {spark_submit.py:495} INFO - 24/09/03 16:22:29 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 16:22:29,600] {spark_submit.py:495} INFO - 24/09/03 16:22:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:22:31,277] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:22:31,278] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:22:31,281] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:22:31,539] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:22:31,579] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:22:31,581] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34277 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:22:31,606] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:31,625] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198826 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:31,794] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:31,813] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:31,814] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:31,815] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:31,820] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:31,843] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:31,938] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:22:31,942] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:22:31,942] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34277 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:22:31,943] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:31,955] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:31,960] {spark_submit.py:495} INFO - 24/09/03 16:22:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:22:32,023] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4967 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:32,048] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:22:32,364] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4522, partition values: [empty row]
[2024-09-03 16:22:32,623] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO CodeGenerator: Code generated in 149.414885 ms
[2024-09-03 16:22:32,689] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:22:32,697] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 685 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:32,699] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:22:32,703] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,845 s
[2024-09-03 16:22:32,806] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:32,808] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:22:32,810] {spark_submit.py:495} INFO - 24/09/03 16:22:32 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,020987 s
[2024-09-03 16:22:33,222] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:22:33,224] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:22:33,224] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:22:33,326] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:33,326] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:33,326] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:33,420] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO CodeGenerator: Code generated in 28.92525 ms
[2024-09-03 16:22:33,465] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO CodeGenerator: Code generated in 28.400666 ms
[2024-09-03 16:22:33,472] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:22:33,483] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:22:33,485] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34277 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:22:33,487] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:33,490] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198826 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:33,552] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:33,554] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:33,555] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:33,555] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:33,555] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:33,558] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:33,610] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:22:33,612] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:22:33,612] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34277 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:22:33,613] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:33,613] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:33,613] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:22:33,617] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:33,618] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:22:33,677] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:33,677] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:33,678] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:33,748] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO CodeGenerator: Code generated in 22.164139 ms
[2024-09-03 16:22:33,752] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4522, partition values: [empty row]
[2024-09-03 16:22:33,771] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO CodeGenerator: Code generated in 16.999297 ms
[2024-09-03 16:22:33,797] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO CodeGenerator: Code generated in 6.959691 ms
[2024-09-03 16:22:33,848] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileOutputCommitter: Saved output of task 'attempt_20240903162233357438114468785966_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/twitter_df/process_data=20240822/_temporary/0/task_20240903162233357438114468785966_0001_m_000000
[2024-09-03 16:22:33,849] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO SparkHadoopMapRedUtil: attempt_20240903162233357438114468785966_0001_m_000000_1: Committed
[2024-09-03 16:22:33,858] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:22:33,884] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 261 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:33,884] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,322 s
[2024-09-03 16:22:33,884] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:33,888] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:22:33,888] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:22:33,888] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,335100 s
[2024-09-03 16:22:33,955] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileFormatWriter: Write Job 0f793401-40b7-409b-8402-cf88f5ab56d1 committed.
[2024-09-03 16:22:33,955] {spark_submit.py:495} INFO - 24/09/03 16:22:33 INFO FileFormatWriter: Finished processing stats for write job 0f793401-40b7-409b-8402-cf88f5ab56d1.
[2024-09-03 16:22:34,056] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:22:34,056] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:22:34,056] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:22:34,074] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:34,075] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:34,075] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:34,102] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO CodeGenerator: Code generated in 9.575073 ms
[2024-09-03 16:22:34,108] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:22:34,118] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:22:34,119] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34277 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:22:34,120] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:34,121] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198826 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:34,136] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:34,138] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:34,138] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:34,138] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:34,138] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:34,139] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.9 KiB, free 364.8 MiB)
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34277 (size: 63.9 KiB, free: 366.1 MiB)
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:34,167] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:22:34,189] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:34,189] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:34,192] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:34,248] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO CodeGenerator: Code generated in 10.004082 ms
[2024-09-03 16:22:34,252] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4522, partition values: [empty row]
[2024-09-03 16:22:34,267] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO CodeGenerator: Code generated in 13.203232 ms
[2024-09-03 16:22:34,279] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409031622348948690570249448457_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/user_df/process_data=20240822/_temporary/0/task_202409031622348948690570249448457_0002_m_000000
[2024-09-03 16:22:34,279] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkHadoopMapRedUtil: attempt_202409031622348948690570249448457_0002_m_000000_2: Committed
[2024-09-03 16:22:34,281] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:22:34,292] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 125 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:34,294] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,151 s
[2024-09-03 16:22:34,294] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:34,294] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:22:34,294] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:22:34,294] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,156777 s
[2024-09-03 16:22:34,312] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileFormatWriter: Write Job 903c4460-bf61-4ff3-933e-efa0103cddbe committed.
[2024-09-03 16:22:34,312] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO FileFormatWriter: Finished processing stats for write job 903c4460-bf61-4ff3-933e-efa0103cddbe.
[2024-09-03 16:22:34,352] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:22:34,361] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:22:34,379] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:22:34,388] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:22:34,388] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO BlockManager: BlockManager stopped
[2024-09-03 16:22:34,394] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:22:34,397] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:22:34,403] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:22:34,403] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:22:34,403] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2eeea6b-1979-4392-a07d-424926e031a5
[2024-09-03 16:22:34,407] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2eeea6b-1979-4392-a07d-424926e031a5/pyspark-52a827d3-4876-4420-a52d-e05a08a55b84
[2024-09-03 16:22:34,409] {spark_submit.py:495} INFO - 24/09/03 16:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b1fccb3-a234-4b1f-987f-54c73d344e68
[2024-09-03 16:22:34,518] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T192224, end_date=20240903T192234
[2024-09-03 16:22:34,576] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:22:34,606] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:40:15,431] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 16:40:15,436] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 16:40:15,436] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:40:15,436] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:40:15,436] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:40:15,444] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 16:40:15,446] {standard_task_runner.py:52} INFO - Started process 294013 to run task
[2024-09-03 16:40:15,449] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpsidj32nr', '--error-file', '/tmp/tmppncducs2']
[2024-09-03 16:40:15,449] {standard_task_runner.py:80} INFO - Job 81: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:40:15,480] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:40:15,522] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 16:40:15,527] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:40:15,528] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 16:40:16,623] {spark_submit.py:495} INFO - 24/09/03 16:40:16 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:40:16,623] {spark_submit.py:495} INFO - 24/09/03 16:40:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:40:16,939] {spark_submit.py:495} INFO - 24/09/03 16:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:40:17,452] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:40:17,461] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:40:17,496] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO ResourceUtils: ==============================================================
[2024-09-03 16:40:17,496] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:40:17,496] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO ResourceUtils: ==============================================================
[2024-09-03 16:40:17,497] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:40:17,515] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:40:17,527] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:40:17,528] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:40:17,565] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:40:17,566] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:40:17,566] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:40:17,566] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:40:17,566] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:40:17,727] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO Utils: Successfully started service 'sparkDriver' on port 41435.
[2024-09-03 16:40:17,759] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:40:17,785] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:40:17,799] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:40:17,800] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:40:17,805] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:40:17,816] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1003b579-5be6-4623-8ee4-a407d421e914
[2024-09-03 16:40:17,834] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:40:17,851] {spark_submit.py:495} INFO - 24/09/03 16:40:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:40:18,054] {spark_submit.py:495} INFO - 24/09/03 16:40:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:40:18,059] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:40:18,107] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:40:18,302] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:40:18,326] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37783.
[2024-09-03 16:40:18,326] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO NettyBlockTransferService: Server created on 192.168.2.128:37783
[2024-09-03 16:40:18,327] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:40:18,333] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 37783, None)
[2024-09-03 16:40:18,336] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:37783 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 37783, None)
[2024-09-03 16:40:18,339] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 37783, None)
[2024-09-03 16:40:18,340] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 37783, None)
[2024-09-03 16:40:18,776] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:40:18,776] {spark_submit.py:495} INFO - 24/09/03 16:40:18 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:40:19,562] {spark_submit.py:495} INFO - 24/09/03 16:40:19 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2024-09-03 16:40:19,625] {spark_submit.py:495} INFO - 24/09/03 16:40:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:40:21,309] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:21,310] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:21,313] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:40:21,559] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:21,606] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:40:21,609] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:37783 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:40:21,613] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:21,621] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198837 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:21,763] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:21,781] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:21,781] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:21,782] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:21,784] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:21,789] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:21,863] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:21,867] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:40:21,868] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:37783 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:40:21,868] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:21,880] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:21,881] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:40:21,926] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:21,941] {spark_submit.py:495} INFO - 24/09/03 16:40:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:40:22,187] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4533, partition values: [empty row]
[2024-09-03 16:40:22,412] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO CodeGenerator: Code generated in 137.323264 ms
[2024-09-03 16:40:22,457] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:40:22,471] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 555 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:22,561] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:40:22,565] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,762 s
[2024-09-03 16:40:22,571] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:22,571] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:40:22,578] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,810285 s
[2024-09-03 16:40:22,962] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:40:22,964] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:40:22,965] {spark_submit.py:495} INFO - 24/09/03 16:40:22 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:40:23,028] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:23,028] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:23,030] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:23,108] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 25.715345 ms
[2024-09-03 16:40:23,151] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 27.861711 ms
[2024-09-03 16:40:23,157] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:40:23,166] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:40:23,166] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:37783 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:40:23,167] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:23,170] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198837 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:23,226] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:23,227] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:23,227] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:23,227] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:23,228] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:23,230] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:23,269] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:40:23,273] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:40:23,273] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:37783 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:40:23,274] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:23,275] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:23,275] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:40:23,279] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:23,280] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:40:23,342] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:23,342] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:23,343] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:23,424] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 25.397384 ms
[2024-09-03 16:40:23,427] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4533, partition values: [empty row]
[2024-09-03 16:40:23,447] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 17.448421 ms
[2024-09-03 16:40:23,469] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 4.977584 ms
[2024-09-03 16:40:23,511] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: Saved output of task 'attempt_20240903164023385099267026531762_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_20240903164023385099267026531762_0001_m_000000
[2024-09-03 16:40:23,512] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkHadoopMapRedUtil: attempt_20240903164023385099267026531762_0001_m_000000_1: Committed
[2024-09-03 16:40:23,516] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:40:23,519] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 242 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:23,519] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:40:23,520] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,289 s
[2024-09-03 16:40:23,521] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:23,521] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:40:23,527] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,296348 s
[2024-09-03 16:40:23,544] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileFormatWriter: Write Job e80b443d-113b-4fcf-ab19-c3232e83124b committed.
[2024-09-03 16:40:23,548] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileFormatWriter: Finished processing stats for write job e80b443d-113b-4fcf-ab19-c3232e83124b.
[2024-09-03 16:40:23,607] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:23,608] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:23,608] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:40:23,617] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:23,617] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:23,617] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:23,723] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 33.322123 ms
[2024-09-03 16:40:23,728] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:40:23,736] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:40:23,737] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:37783 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:40:23,738] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:23,739] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198837 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:23,757] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:23,758] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:23,758] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:23,758] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:23,758] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:23,769] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:23,798] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:40:23,803] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:40:23,803] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:37783 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:40:23,806] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:23,807] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:23,807] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:40:23,808] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:23,810] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:40:23,826] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:23,827] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:23,827] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:23,876] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 9.772449 ms
[2024-09-03 16:40:23,889] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4533, partition values: [empty row]
[2024-09-03 16:40:23,903] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO CodeGenerator: Code generated in 11.188984 ms
[2024-09-03 16:40:23,912] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640233379352320506333402_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409031640233379352320506333402_0002_m_000000
[2024-09-03 16:40:23,913] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkHadoopMapRedUtil: attempt_202409031640233379352320506333402_0002_m_000000_2: Committed
[2024-09-03 16:40:23,914] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:40:23,917] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 108 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:23,917] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:40:23,919] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,148 s
[2024-09-03 16:40:23,920] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:23,920] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:40:23,920] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,162884 s
[2024-09-03 16:40:23,939] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileFormatWriter: Write Job 7884b18d-3b15-44d7-8423-100805349cb8 committed.
[2024-09-03 16:40:23,939] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO FileFormatWriter: Finished processing stats for write job 7884b18d-3b15-44d7-8423-100805349cb8.
[2024-09-03 16:40:23,972] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:40:23,982] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:40:23,995] {spark_submit.py:495} INFO - 24/09/03 16:40:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:40:24,005] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:40:24,005] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO BlockManager: BlockManager stopped
[2024-09-03 16:40:24,015] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:40:24,017] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:40:24,024] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:40:24,025] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:40:24,025] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-861e7cf6-67b7-49b5-8c11-b173bbe34f2b/pyspark-9616b50b-df35-4452-a5f0-a7d6cc57a90e
[2024-09-03 16:40:24,028] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-29abaee9-44c5-49e9-aea3-db1b14b8f55a
[2024-09-03 16:40:24,031] {spark_submit.py:495} INFO - 24/09/03 16:40:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-861e7cf6-67b7-49b5-8c11-b173bbe34f2b
[2024-09-03 16:40:24,101] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T194015, end_date=20240903T194024
[2024-09-03 16:40:24,146] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:40:24,161] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:53:58,151] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 17:53:58,156] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 17:53:58,156] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:53:58,156] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:53:58,156] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:53:58,164] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 17:53:58,167] {standard_task_runner.py:52} INFO - Started process 323971 to run task
[2024-09-03 17:53:58,169] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpmy1ntbs6', '--error-file', '/tmp/tmpz0abmsq_']
[2024-09-03 17:53:58,170] {standard_task_runner.py:80} INFO - Job 84: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:53:58,202] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:53:58,240] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 17:53:58,243] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:53:58,244] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 17:53:59,345] {spark_submit.py:495} INFO - 24/09/03 17:53:59 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:53:59,346] {spark_submit.py:495} INFO - 24/09/03 17:53:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:53:59,662] {spark_submit.py:495} INFO - 24/09/03 17:53:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:54:00,158] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:54:00,166] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:54:00,206] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO ResourceUtils: ==============================================================
[2024-09-03 17:54:00,206] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:54:00,206] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO ResourceUtils: ==============================================================
[2024-09-03 17:54:00,207] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:54:00,225] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:54:00,239] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:54:00,240] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:54:00,276] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:54:00,277] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:54:00,278] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:54:00,278] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:54:00,279] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:54:00,423] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO Utils: Successfully started service 'sparkDriver' on port 37401.
[2024-09-03 17:54:00,448] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:54:00,475] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:54:00,492] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:54:00,493] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:54:00,497] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:54:00,510] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-25d320c2-49b9-485a-960a-28b2fc2f4501
[2024-09-03 17:54:00,533] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:54:00,548] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:54:00,731] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:54:00,787] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:54:00,961] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:54:00,982] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34593.
[2024-09-03 17:54:00,983] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO NettyBlockTransferService: Server created on 192.168.2.128:34593
[2024-09-03 17:54:00,985] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:54:00,989] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34593, None)
[2024-09-03 17:54:00,992] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34593 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34593, None)
[2024-09-03 17:54:00,994] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34593, None)
[2024-09-03 17:54:00,995] {spark_submit.py:495} INFO - 24/09/03 17:54:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34593, None)
[2024-09-03 17:54:01,405] {spark_submit.py:495} INFO - 24/09/03 17:54:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:54:01,406] {spark_submit.py:495} INFO - 24/09/03 17:54:01 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:54:02,189] {spark_submit.py:495} INFO - 24/09/03 17:54:02 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.
[2024-09-03 17:54:02,239] {spark_submit.py:495} INFO - 24/09/03 17:54:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:54:03,924] {spark_submit.py:495} INFO - 24/09/03 17:54:03 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:54:03,925] {spark_submit.py:495} INFO - 24/09/03 17:54:03 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:54:03,928] {spark_submit.py:495} INFO - 24/09/03 17:54:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:54:04,171] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:54:04,213] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:54:04,215] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34593 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:54:04,221] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:04,229] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648843 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:54:04,367] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:04,380] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:54:04,382] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:54:04,382] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:54:04,383] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:54:04,387] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:54:04,471] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:54:04,475] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:54:04,477] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34593 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:54:04,478] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:54:04,493] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:54:04,494] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:54:04,573] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:54:04,593] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:54:04,852] {spark_submit.py:495} INFO - 24/09/03 17:54:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-454539, partition values: [empty row]
[2024-09-03 17:54:05,061] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO CodeGenerator: Code generated in 124.113986 ms
[2024-09-03 17:54:05,147] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 17:54:05,247] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 596 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:54:05,247] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:54:05,256] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,855 s
[2024-09-03 17:54:05,265] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:54:05,266] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:54:05,269] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,899977 s
[2024-09-03 17:54:05,671] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:54:05,672] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:54:05,672] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:54:05,742] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:05,742] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:05,742] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:05,830] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO CodeGenerator: Code generated in 29.626247 ms
[2024-09-03 17:54:05,866] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO CodeGenerator: Code generated in 23.465952 ms
[2024-09-03 17:54:05,872] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:54:05,880] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:54:05,881] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34593 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:54:05,882] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:05,884] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648843 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:54:05,937] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:05,939] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:54:05,939] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:54:05,939] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:54:05,939] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:54:05,941] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:54:05,983] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:54:05,985] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:54:05,986] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34593 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:54:05,987] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:54:05,987] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:54:05,988] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:54:05,993] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:54:05,995] {spark_submit.py:495} INFO - 24/09/03 17:54:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:54:06,049] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:06,050] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:06,050] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:06,103] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO CodeGenerator: Code generated in 17.596111 ms
[2024-09-03 17:54:06,107] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-454539, partition values: [empty row]
[2024-09-03 17:54:06,130] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO CodeGenerator: Code generated in 19.434139 ms
[2024-09-03 17:54:06,151] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO CodeGenerator: Code generated in 4.650014 ms
[2024-09-03 17:54:06,244] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: Saved output of task 'attempt_202409031754054777591711385037671_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_202409031754054777591711385037671_0001_m_000000
[2024-09-03 17:54:06,245] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkHadoopMapRedUtil: attempt_202409031754054777591711385037671_0001_m_000000_1: Committed
[2024-09-03 17:54:06,249] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:54:06,251] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 263 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:54:06,251] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:54:06,252] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,309 s
[2024-09-03 17:54:06,252] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:54:06,252] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:54:06,253] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,315852 s
[2024-09-03 17:54:06,289] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileFormatWriter: Write Job bae6b6a2-7775-49b4-9b8b-b722a3399364 committed.
[2024-09-03 17:54:06,292] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileFormatWriter: Finished processing stats for write job bae6b6a2-7775-49b4-9b8b-b722a3399364.
[2024-09-03 17:54:06,332] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:54:06,332] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:54:06,332] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:54:06,342] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:06,343] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:06,343] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:06,373] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO CodeGenerator: Code generated in 11.595356 ms
[2024-09-03 17:54:06,378] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:54:06,385] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:54:06,386] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34593 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:54:06,387] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:06,388] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648843 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:54:06,405] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:06,406] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:54:06,406] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:54:06,406] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:54:06,406] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:54:06,410] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:54:06,430] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:54:06,432] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:54:06,433] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34593 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:54:06,433] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:54:06,434] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:54:06,434] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:54:06,435] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:54:06,436] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:54:06,450] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:06,450] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:06,451] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:06,479] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO CodeGenerator: Code generated in 10.587825 ms
[2024-09-03 17:54:06,481] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-454539, partition values: [empty row]
[2024-09-03 17:54:06,494] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO CodeGenerator: Code generated in 10.84046 ms
[2024-09-03 17:54:06,529] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileOutputCommitter: Saved output of task 'attempt_202409031754062248738904483225335_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409031754062248738904483225335_0002_m_000000
[2024-09-03 17:54:06,529] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkHadoopMapRedUtil: attempt_202409031754062248738904483225335_0002_m_000000_2: Committed
[2024-09-03 17:54:06,531] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:54:06,533] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 98 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:54:06,533] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:54:06,534] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,123 s
[2024-09-03 17:54:06,534] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:54:06,534] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:54:06,537] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,129200 s
[2024-09-03 17:54:06,556] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileFormatWriter: Write Job 2a79dbde-3650-4940-bf4b-275687dc9822 committed.
[2024-09-03 17:54:06,556] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO FileFormatWriter: Finished processing stats for write job 2a79dbde-3650-4940-bf4b-275687dc9822.
[2024-09-03 17:54:06,591] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:54:06,601] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:54:06,613] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:54:06,620] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:54:06,620] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO BlockManager: BlockManager stopped
[2024-09-03 17:54:06,628] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:54:06,629] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:54:06,635] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:54:06,636] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:54:06,636] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-39838135-14d9-46b9-9498-b8451d0fbe57
[2024-09-03 17:54:06,641] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-95872721-2815-4265-b5af-bb98a7c6afe3
[2024-09-03 17:54:06,644] {spark_submit.py:495} INFO - 24/09/03 17:54:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-39838135-14d9-46b9-9498-b8451d0fbe57/pyspark-2a0d3c18-e846-427d-ac7b-bc1217467694
[2024-09-03 17:54:06,694] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T205358, end_date=20240903T205406
[2024-09-03 17:54:06,725] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:54:06,737] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:10:31,381] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 18:10:31,394] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 18:10:31,394] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:10:31,395] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:10:31,395] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:10:31,413] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 18:10:31,418] {standard_task_runner.py:52} INFO - Started process 332211 to run task
[2024-09-03 18:10:31,421] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2tl98gu5', '--error-file', '/tmp/tmpmdw65y8f']
[2024-09-03 18:10:31,422] {standard_task_runner.py:80} INFO - Job 84: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:10:31,475] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:10:31,552] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 18:10:31,557] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:10:31,558] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 18:10:33,250] {spark_submit.py:495} INFO - 24/09/03 18:10:33 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:10:33,252] {spark_submit.py:495} INFO - 24/09/03 18:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:10:33,749] {spark_submit.py:495} INFO - 24/09/03 18:10:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:10:34,598] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:10:34,613] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:10:34,703] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO ResourceUtils: ==============================================================
[2024-09-03 18:10:34,704] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:10:34,704] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO ResourceUtils: ==============================================================
[2024-09-03 18:10:34,704] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:10:34,740] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:10:34,760] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:10:34,764] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:10:34,833] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:10:34,834] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:10:34,835] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:10:34,835] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:10:34,836] {spark_submit.py:495} INFO - 24/09/03 18:10:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:10:35,083] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO Utils: Successfully started service 'sparkDriver' on port 40725.
[2024-09-03 18:10:35,115] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:10:35,161] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:10:35,185] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:10:35,187] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:10:35,192] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:10:35,215] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-81ba7718-d443-449d-abf0-536af690c9f7
[2024-09-03 18:10:35,242] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:10:35,265] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:10:35,626] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:10:35,710] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:10:35,993] {spark_submit.py:495} INFO - 24/09/03 18:10:35 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:10:36,026] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35191.
[2024-09-03 18:10:36,027] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO NettyBlockTransferService: Server created on 192.168.2.128:35191
[2024-09-03 18:10:36,031] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:10:36,039] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 35191, None)
[2024-09-03 18:10:36,044] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:35191 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 35191, None)
[2024-09-03 18:10:36,048] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 35191, None)
[2024-09-03 18:10:36,048] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 35191, None)
[2024-09-03 18:10:36,695] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:10:36,696] {spark_submit.py:495} INFO - 24/09/03 18:10:36 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:10:37,933] {spark_submit.py:495} INFO - 24/09/03 18:10:37 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2024-09-03 18:10:38,035] {spark_submit.py:495} INFO - 24/09/03 18:10:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:10:40,430] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:10:40,430] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:10:40,435] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:10:40,798] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:10:40,852] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:10:40,855] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:35191 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:10:40,875] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:40,892] {spark_submit.py:495} INFO - 24/09/03 18:10:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198834 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:10:41,089] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:41,112] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:10:41,112] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:10:41,113] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:10:41,117] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:10:41,127] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:10:41,281] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:10:41,284] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:10:41,284] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:35191 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:10:41,285] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:10:41,298] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:10:41,300] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:10:41,349] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:10:41,368] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:10:41,740] {spark_submit.py:495} INFO - 24/09/03 18:10:41 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4530, partition values: [empty row]
[2024-09-03 18:10:42,011] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO CodeGenerator: Code generated in 166.948491 ms
[2024-09-03 18:10:42,056] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:10:42,072] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 733 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:10:42,077] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:10:42,087] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,918 s
[2024-09-03 18:10:42,231] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:10:42,232] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:10:42,235] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,144979 s
[2024-09-03 18:10:42,719] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:10:42,720] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:10:42,721] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:10:42,784] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:42,785] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:42,786] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:42,855] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO CodeGenerator: Code generated in 26.196155 ms
[2024-09-03 18:10:42,895] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO CodeGenerator: Code generated in 25.886946 ms
[2024-09-03 18:10:42,900] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:10:42,907] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:10:42,909] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:35191 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:10:42,912] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:42,915] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198834 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:10:42,967] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:42,971] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:10:42,971] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:10:42,971] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:10:42,971] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:10:42,984] {spark_submit.py:495} INFO - 24/09/03 18:10:42 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:10:43,069] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:10:43,072] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:10:43,075] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:35191 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:10:43,076] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:10:43,078] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:10:43,078] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:10:43,085] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:10:43,085] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:10:43,148] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:43,149] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:43,149] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:43,208] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO CodeGenerator: Code generated in 17.794903 ms
[2024-09-03 18:10:43,212] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4530, partition values: [empty row]
[2024-09-03 18:10:43,232] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO CodeGenerator: Code generated in 17.181431 ms
[2024-09-03 18:10:43,253] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO CodeGenerator: Code generated in 4.294253 ms
[2024-09-03 18:10:43,290] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: Saved output of task 'attempt_202409031810425031599995291775644_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_202409031810425031599995291775644_0001_m_000000
[2024-09-03 18:10:43,292] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkHadoopMapRedUtil: attempt_202409031810425031599995291775644_0001_m_000000_1: Committed
[2024-09-03 18:10:43,300] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:10:43,306] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 225 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:10:43,307] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,320 s
[2024-09-03 18:10:43,308] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:10:43,309] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:10:43,309] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:10:43,311] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,341959 s
[2024-09-03 18:10:43,332] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileFormatWriter: Write Job 73bf9b88-ec42-4a3e-bb8c-da7859c99b8c committed.
[2024-09-03 18:10:43,336] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileFormatWriter: Finished processing stats for write job 73bf9b88-ec42-4a3e-bb8c-da7859c99b8c.
[2024-09-03 18:10:43,404] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:10:43,406] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:10:43,409] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:10:43,422] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:43,422] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:43,423] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:43,500] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO CodeGenerator: Code generated in 26.215814 ms
[2024-09-03 18:10:43,504] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:10:43,519] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:10:43,520] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:35191 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:10:43,520] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:43,526] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198834 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:10:43,560] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:43,562] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:10:43,562] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:10:43,562] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:10:43,562] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:10:43,565] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:10:43,627] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:10:43,631] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:10:43,632] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:35191 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:10:43,643] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:10:43,643] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:10:43,643] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:10:43,643] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:10:43,659] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:10:43,694] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:43,694] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:43,695] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:43,792] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO CodeGenerator: Code generated in 13.598815 ms
[2024-09-03 18:10:43,818] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4530, partition values: [empty row]
[2024-09-03 18:10:43,853] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO CodeGenerator: Code generated in 31.690736 ms
[2024-09-03 18:10:43,875] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileOutputCommitter: Saved output of task 'attempt_202409031810436802184041079957706_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409031810436802184041079957706_0002_m_000000
[2024-09-03 18:10:43,875] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkHadoopMapRedUtil: attempt_202409031810436802184041079957706_0002_m_000000_2: Committed
[2024-09-03 18:10:43,877] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:10:43,894] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 249 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:10:43,894] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:10:43,894] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,328 s
[2024-09-03 18:10:43,895] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:10:43,895] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:10:43,897] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,335435 s
[2024-09-03 18:10:43,921] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileFormatWriter: Write Job 2b25bf12-04e1-4582-8bf0-b29d60dde4ae committed.
[2024-09-03 18:10:43,923] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO FileFormatWriter: Finished processing stats for write job 2b25bf12-04e1-4582-8bf0-b29d60dde4ae.
[2024-09-03 18:10:43,955] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:10:43,966] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:10:43,979] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:10:43,990] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:10:43,991] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO BlockManager: BlockManager stopped
[2024-09-03 18:10:43,999] {spark_submit.py:495} INFO - 24/09/03 18:10:43 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:10:44,001] {spark_submit.py:495} INFO - 24/09/03 18:10:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:10:44,006] {spark_submit.py:495} INFO - 24/09/03 18:10:44 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:10:44,006] {spark_submit.py:495} INFO - 24/09/03 18:10:44 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:10:44,007] {spark_submit.py:495} INFO - 24/09/03 18:10:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-c67c6211-8884-4150-9f3f-d43c2229b059
[2024-09-03 18:10:44,010] {spark_submit.py:495} INFO - 24/09/03 18:10:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7b70248-9da9-45cb-9bd7-740231a40dab
[2024-09-03 18:10:44,014] {spark_submit.py:495} INFO - 24/09/03 18:10:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7b70248-9da9-45cb-9bd7-740231a40dab/pyspark-cc94fcc2-2f32-47da-8e2f-a287b0fdcf0b
[2024-09-03 18:10:44,126] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T211031, end_date=20240903T211044
[2024-09-03 18:10:44,180] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:10:44,199] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:14:45,440] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 18:14:45,445] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 18:14:45,445] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:14:45,445] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:14:45,445] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:14:45,456] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 18:14:45,459] {standard_task_runner.py:52} INFO - Started process 336229 to run task
[2024-09-03 18:14:45,462] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '83', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpjnhsgqk1', '--error-file', '/tmp/tmpzqavleg1']
[2024-09-03 18:14:45,463] {standard_task_runner.py:80} INFO - Job 83: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:14:45,498] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:14:45,552] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 18:14:45,557] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:14:45,558] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 18:14:46,768] {spark_submit.py:495} INFO - 24/09/03 18:14:46 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:14:46,768] {spark_submit.py:495} INFO - 24/09/03 18:14:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:14:47,211] {spark_submit.py:495} INFO - 24/09/03 18:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:14:47,828] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:14:47,838] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:14:47,883] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO ResourceUtils: ==============================================================
[2024-09-03 18:14:47,883] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:14:47,884] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO ResourceUtils: ==============================================================
[2024-09-03 18:14:47,885] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:14:47,911] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:14:47,928] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:14:47,929] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:14:47,975] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:14:47,976] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:14:47,977] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:14:47,978] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:14:47,978] {spark_submit.py:495} INFO - 24/09/03 18:14:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:14:48,177] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO Utils: Successfully started service 'sparkDriver' on port 33133.
[2024-09-03 18:14:48,202] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:14:48,236] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:14:48,253] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:14:48,255] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:14:48,259] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:14:48,271] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5eed5fbf-928a-4ec4-92a2-90703360e69d
[2024-09-03 18:14:48,294] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:14:48,309] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:14:48,535] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:14:48,617] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:14:48,811] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:14:48,840] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36057.
[2024-09-03 18:14:48,841] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO NettyBlockTransferService: Server created on 192.168.2.128:36057
[2024-09-03 18:14:48,842] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:14:48,848] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36057, None)
[2024-09-03 18:14:48,850] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36057 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36057, None)
[2024-09-03 18:14:48,854] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36057, None)
[2024-09-03 18:14:48,855] {spark_submit.py:495} INFO - 24/09/03 18:14:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36057, None)
[2024-09-03 18:14:49,576] {spark_submit.py:495} INFO - 24/09/03 18:14:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:14:49,582] {spark_submit.py:495} INFO - 24/09/03 18:14:49 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:14:50,886] {spark_submit.py:495} INFO - 24/09/03 18:14:50 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.
[2024-09-03 18:14:50,976] {spark_submit.py:495} INFO - 24/09/03 18:14:50 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2024-09-03 18:14:53,503] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:14:53,504] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:14:53,508] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:14:53,846] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:14:53,912] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:14:53,916] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36057 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:14:53,923] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:53,935] {spark_submit.py:495} INFO - 24/09/03 18:14:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198860 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:54,142] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:54,162] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:54,162] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:54,163] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:54,164] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:54,177] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:54,333] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:14:54,358] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:14:54,361] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36057 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:14:54,365] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:54,388] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:54,389] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:14:54,450] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:54,483] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:14:54,817] {spark_submit.py:495} INFO - 24/09/03 18:14:54 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4556, partition values: [empty row]
[2024-09-03 18:14:55,119] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO CodeGenerator: Code generated in 183.991729 ms
[2024-09-03 18:14:55,172] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 18:14:55,187] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 754 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:55,190] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:14:55,321] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,002 s
[2024-09-03 18:14:55,325] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:55,326] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:14:55,331] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,189056 s
[2024-09-03 18:14:55,861] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:14:55,864] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:14:55,864] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:14:55,958] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:55,958] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:55,960] {spark_submit.py:495} INFO - 24/09/03 18:14:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:56,062] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO CodeGenerator: Code generated in 34.808337 ms
[2024-09-03 18:14:56,123] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO CodeGenerator: Code generated in 38.950306 ms
[2024-09-03 18:14:56,132] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:14:56,141] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:14:56,142] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36057 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:14:56,144] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:56,147] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198860 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:56,223] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:56,228] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:56,228] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:56,228] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:56,228] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:56,233] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:56,293] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:14:56,297] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:14:56,297] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36057 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:14:56,297] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:56,297] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:56,297] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:14:56,305] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:56,306] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:14:56,381] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:56,382] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:56,383] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:56,565] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO CodeGenerator: Code generated in 101.502345 ms
[2024-09-03 18:14:56,571] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4556, partition values: [empty row]
[2024-09-03 18:14:56,631] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO CodeGenerator: Code generated in 51.027439 ms
[2024-09-03 18:14:56,673] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO CodeGenerator: Code generated in 8.827187 ms
[2024-09-03 18:14:56,729] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileOutputCommitter: Saved output of task 'attempt_202409031814561098741364839215104_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_202409031814561098741364839215104_0001_m_000000
[2024-09-03 18:14:56,729] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SparkHadoopMapRedUtil: attempt_202409031814561098741364839215104_0001_m_000000_1: Committed
[2024-09-03 18:14:56,740] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:14:56,742] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 441 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:56,745] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,513 s
[2024-09-03 18:14:56,745] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:56,745] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:14:56,746] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:14:56,748] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,524702 s
[2024-09-03 18:14:56,783] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileFormatWriter: Write Job 4dc62176-35a3-4070-a5a6-318108d432d5 committed.
[2024-09-03 18:14:56,788] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileFormatWriter: Finished processing stats for write job 4dc62176-35a3-4070-a5a6-318108d432d5.
[2024-09-03 18:14:56,878] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:14:56,878] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:14:56,879] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:14:56,898] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:56,898] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:56,900] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:56,961] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO CodeGenerator: Code generated in 18.82586 ms
[2024-09-03 18:14:56,972] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:14:56,989] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:14:56,989] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36057 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:14:56,991] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:56,994] {spark_submit.py:495} INFO - 24/09/03 18:14:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198860 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:57,038] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:57,039] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:57,040] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:57,040] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:57,040] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:57,044] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:57,117] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:14:57,123] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:14:57,124] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36057 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:14:57,128] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:57,130] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:57,130] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:14:57,134] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:57,135] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:14:57,157] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:57,158] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:57,158] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:57,232] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO CodeGenerator: Code generated in 17.685635 ms
[2024-09-03 18:14:57,238] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4556, partition values: [empty row]
[2024-09-03 18:14:57,264] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO CodeGenerator: Code generated in 23.016178 ms
[2024-09-03 18:14:57,279] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO FileOutputCommitter: Saved output of task 'attempt_202409031814572498585369436554508_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409031814572498585369436554508_0002_m_000000
[2024-09-03 18:14:57,280] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SparkHadoopMapRedUtil: attempt_202409031814572498585369436554508_0002_m_000000_2: Committed
[2024-09-03 18:14:57,282] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:14:57,283] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 151 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:57,284] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:14:57,290] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,239 s
[2024-09-03 18:14:57,291] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:57,291] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:14:57,291] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,250048 s
[2024-09-03 18:14:57,330] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO FileFormatWriter: Write Job c79a5378-7b2e-4408-b9d4-fd896d819ace committed.
[2024-09-03 18:14:57,331] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO FileFormatWriter: Finished processing stats for write job c79a5378-7b2e-4408-b9d4-fd896d819ace.
[2024-09-03 18:14:57,397] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:14:57,417] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:14:57,454] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:14:57,472] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:14:57,473] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO BlockManager: BlockManager stopped
[2024-09-03 18:14:57,483] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:14:57,490] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:14:57,507] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:14:57,508] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:14:57,509] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-04a671b3-1689-4be4-a31c-2760982d18bf
[2024-09-03 18:14:57,514] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc445127-58c5-4877-ac73-470c5dcf9309
[2024-09-03 18:14:57,521] {spark_submit.py:495} INFO - 24/09/03 18:14:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc445127-58c5-4877-ac73-470c5dcf9309/pyspark-bcf380ef-f909-418e-bbdb-7a1ed68740d0
[2024-09-03 18:14:57,615] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T211445, end_date=20240903T211457
[2024-09-03 18:14:57,659] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:14:57,677] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:20:44,457] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 18:20:44,466] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 18:20:44,467] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:20:44,467] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:20:44,467] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:20:44,479] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 18:20:44,481] {standard_task_runner.py:52} INFO - Started process 342449 to run task
[2024-09-03 18:20:44,489] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpzjowf8ok', '--error-file', '/tmp/tmpve9fhy2j']
[2024-09-03 18:20:44,490] {standard_task_runner.py:80} INFO - Job 84: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:20:44,539] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:20:44,620] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 18:20:44,625] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:20:44,626] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 18:20:45,891] {spark_submit.py:495} INFO - 24/09/03 18:20:45 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:20:45,892] {spark_submit.py:495} INFO - 24/09/03 18:20:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:20:46,420] {spark_submit.py:495} INFO - 24/09/03 18:20:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:20:47,111] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:20:47,124] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:20:47,210] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO ResourceUtils: ==============================================================
[2024-09-03 18:20:47,213] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:20:47,214] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO ResourceUtils: ==============================================================
[2024-09-03 18:20:47,215] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:20:47,244] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:20:47,267] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:20:47,269] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:20:47,334] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:20:47,335] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:20:47,335] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:20:47,335] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:20:47,335] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:20:47,557] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO Utils: Successfully started service 'sparkDriver' on port 45951.
[2024-09-03 18:20:47,592] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:20:47,639] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:20:47,665] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:20:47,666] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:20:47,698] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:20:47,748] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e896c09-5438-455b-92ab-40b753063e24
[2024-09-03 18:20:47,797] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:20:47,840] {spark_submit.py:495} INFO - 24/09/03 18:20:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:20:48,246] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:20:48,298] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:20:48,524] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:20:48,555] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38345.
[2024-09-03 18:20:48,555] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO NettyBlockTransferService: Server created on 192.168.2.128:38345
[2024-09-03 18:20:48,556] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:20:48,564] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38345, None)
[2024-09-03 18:20:48,567] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38345 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38345, None)
[2024-09-03 18:20:48,581] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38345, None)
[2024-09-03 18:20:48,588] {spark_submit.py:495} INFO - 24/09/03 18:20:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38345, None)
[2024-09-03 18:20:49,306] {spark_submit.py:495} INFO - 24/09/03 18:20:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:20:49,309] {spark_submit.py:495} INFO - 24/09/03 18:20:49 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:20:50,371] {spark_submit.py:495} INFO - 24/09/03 18:20:50 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.
[2024-09-03 18:20:50,447] {spark_submit.py:495} INFO - 24/09/03 18:20:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:20:52,734] {spark_submit.py:495} INFO - 24/09/03 18:20:52 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:20:52,735] {spark_submit.py:495} INFO - 24/09/03 18:20:52 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:20:52,739] {spark_submit.py:495} INFO - 24/09/03 18:20:52 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:20:53,102] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:20:53,171] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:20:53,177] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38345 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:20:53,201] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:53,213] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649663 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:20:53,419] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:53,438] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:20:53,439] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:20:53,440] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:20:53,442] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:20:53,455] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:20:53,568] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:20:53,571] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:20:53,572] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38345 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:20:53,573] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:20:53,591] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:20:53,594] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:20:53,671] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:20:53,696] {spark_submit.py:495} INFO - 24/09/03 18:20:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:20:54,081] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-455359, partition values: [empty row]
[2024-09-03 18:20:54,426] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO CodeGenerator: Code generated in 185.030011 ms
[2024-09-03 18:20:54,539] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:20:54,714] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 910 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:20:54,721] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:20:54,729] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,249 s
[2024-09-03 18:20:54,736] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:20:54,737] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:20:54,743] {spark_submit.py:495} INFO - 24/09/03 18:20:54 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,320320 s
[2024-09-03 18:20:55,305] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:20:55,308] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:20:55,308] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:20:55,405] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:55,405] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:55,407] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:55,523] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO CodeGenerator: Code generated in 40.953999 ms
[2024-09-03 18:20:55,581] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO CodeGenerator: Code generated in 37.085149 ms
[2024-09-03 18:20:55,588] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:20:55,598] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:20:55,600] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38345 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:20:55,601] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:55,606] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649663 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:20:55,687] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:55,690] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:20:55,691] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:20:55,691] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:20:55,692] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:20:55,694] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:20:55,767] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:20:55,770] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:20:55,773] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38345 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:20:55,775] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:20:55,777] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:20:55,778] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:20:55,786] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:20:55,786] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:20:55,858] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:55,859] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:55,863] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:55,958] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO CodeGenerator: Code generated in 43.004544 ms
[2024-09-03 18:20:55,965] {spark_submit.py:495} INFO - 24/09/03 18:20:55 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-455359, partition values: [empty row]
[2024-09-03 18:20:56,001] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO CodeGenerator: Code generated in 30.991354 ms
[2024-09-03 18:20:56,052] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO CodeGenerator: Code generated in 14.691025 ms
[2024-09-03 18:20:56,221] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileOutputCommitter: Saved output of task 'attempt_20240903182055567258544315548603_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_20240903182055567258544315548603_0001_m_000000
[2024-09-03 18:20:56,222] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkHadoopMapRedUtil: attempt_20240903182055567258544315548603_0001_m_000000_1: Committed
[2024-09-03 18:20:56,233] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:20:56,241] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 460 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:20:56,242] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:20:56,244] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,547 s
[2024-09-03 18:20:56,244] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:20:56,246] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:20:56,246] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,557934 s
[2024-09-03 18:20:56,285] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileFormatWriter: Write Job d9a7620b-7649-4311-9bf4-c48beb385e56 committed.
[2024-09-03 18:20:56,288] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileFormatWriter: Finished processing stats for write job d9a7620b-7649-4311-9bf4-c48beb385e56.
[2024-09-03 18:20:56,365] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:20:56,366] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:20:56,367] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:20:56,400] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:56,400] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:56,401] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:56,450] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO CodeGenerator: Code generated in 16.827131 ms
[2024-09-03 18:20:56,455] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:20:56,466] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:20:56,468] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38345 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:20:56,469] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:56,470] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649663 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:20:56,488] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:56,490] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:20:56,490] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:20:56,491] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:20:56,491] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:20:56,495] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:20:56,517] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:20:56,520] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:20:56,521] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38345 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:20:56,522] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:20:56,524] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:20:56,526] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:20:56,533] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:20:56,533] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:20:56,555] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:56,555] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:56,556] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:56,619] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO CodeGenerator: Code generated in 16.211327 ms
[2024-09-03 18:20:56,633] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-455359, partition values: [empty row]
[2024-09-03 18:20:56,664] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO CodeGenerator: Code generated in 26.084929 ms
[2024-09-03 18:20:56,730] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileOutputCommitter: Saved output of task 'attempt_202409031820567823591490099788564_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409031820567823591490099788564_0002_m_000000
[2024-09-03 18:20:56,731] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkHadoopMapRedUtil: attempt_202409031820567823591490099788564_0002_m_000000_2: Committed
[2024-09-03 18:20:56,731] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:20:56,735] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 203 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:20:56,736] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:20:56,738] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,242 s
[2024-09-03 18:20:56,738] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:20:56,738] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:20:56,738] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,248729 s
[2024-09-03 18:20:56,764] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileFormatWriter: Write Job 833bef2d-538a-440d-8447-bf96d87b01db committed.
[2024-09-03 18:20:56,765] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO FileFormatWriter: Finished processing stats for write job 833bef2d-538a-440d-8447-bf96d87b01db.
[2024-09-03 18:20:56,856] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:20:56,884] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:20:56,887] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.128:38345 in memory (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:20:56,918] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:20:56,931] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:20:56,931] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO BlockManager: BlockManager stopped
[2024-09-03 18:20:56,938] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:20:56,944] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:20:56,952] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:20:56,953] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:20:56,953] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cbe24ce-657c-412c-b9b1-5d34ef4a374b
[2024-09-03 18:20:56,958] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-772423ec-bf74-42dc-82a9-a73fac861346
[2024-09-03 18:20:56,963] {spark_submit.py:495} INFO - 24/09/03 18:20:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cbe24ce-657c-412c-b9b1-5d34ef4a374b/pyspark-4b65484e-cd11-4e91-afa9-65e756cec2e3
[2024-09-03 18:20:57,043] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240903T212044, end_date=20240903T212057
[2024-09-03 18:20:57,074] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:20:57,089] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:42:17,894] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 21:42:17,903] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 21:42:17,903] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:42:17,904] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:42:17,904] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:42:17,917] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 21:42:17,920] {standard_task_runner.py:52} INFO - Started process 416423 to run task
[2024-09-03 21:42:17,926] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp4ae3fh5g', '--error-file', '/tmp/tmp33ud8e5m']
[2024-09-03 21:42:17,926] {standard_task_runner.py:80} INFO - Job 81: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:42:17,966] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:42:18,021] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 21:42:18,028] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:42:18,029] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 21:42:19,484] {spark_submit.py:495} INFO - 24/09/03 21:42:19 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:42:19,484] {spark_submit.py:495} INFO - 24/09/03 21:42:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:42:19,997] {spark_submit.py:495} INFO - 24/09/03 21:42:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:42:20,839] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:42:20,850] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:42:20,896] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO ResourceUtils: ==============================================================
[2024-09-03 21:42:20,896] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:42:20,897] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO ResourceUtils: ==============================================================
[2024-09-03 21:42:20,897] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:42:20,920] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:42:20,935] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:42:20,937] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:42:20,989] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:42:20,990] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:42:20,990] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:42:20,993] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:42:20,994] {spark_submit.py:495} INFO - 24/09/03 21:42:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:42:21,223] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO Utils: Successfully started service 'sparkDriver' on port 42645.
[2024-09-03 21:42:21,261] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:42:21,300] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:42:21,321] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:42:21,322] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:42:21,326] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:42:21,341] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e8806fa-a24b-49fe-99f5-4a6c13af155b
[2024-09-03 21:42:21,366] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:42:21,390] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:42:21,679] {spark_submit.py:495} INFO - 24/09/03 21:42:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:42:21,697] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:42:21,775] {spark_submit.py:495} INFO - 24/09/03 21:42:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:42:22,023] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:42:22,060] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43939.
[2024-09-03 21:42:22,061] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO NettyBlockTransferService: Server created on 192.168.2.128:43939
[2024-09-03 21:42:22,061] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:42:22,069] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 43939, None)
[2024-09-03 21:42:22,072] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:43939 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 43939, None)
[2024-09-03 21:42:22,081] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 43939, None)
[2024-09-03 21:42:22,081] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 43939, None)
[2024-09-03 21:42:22,875] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:42:22,876] {spark_submit.py:495} INFO - 24/09/03 21:42:22 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:42:24,018] {spark_submit.py:495} INFO - 24/09/03 21:42:24 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
[2024-09-03 21:42:24,088] {spark_submit.py:495} INFO - 24/09/03 21:42:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 21:42:26,485] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:42:26,486] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:42:26,492] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:42:26,828] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:42:26,883] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:42:26,888] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:43939 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:42:26,898] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:26,906] {spark_submit.py:495} INFO - 24/09/03 21:42:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648107 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:42:27,149] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:27,172] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:42:27,173] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:42:27,173] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:42:27,177] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:42:27,185] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:42:27,304] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:42:27,308] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:42:27,309] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:43939 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:42:27,310] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:42:27,325] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:42:27,326] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:42:27,395] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:42:27,416] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:42:27,865] {spark_submit.py:495} INFO - 24/09/03 21:42:27 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-453803, partition values: [empty row]
[2024-09-03 21:42:28,201] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO CodeGenerator: Code generated in 200.91152 ms
[2024-09-03 21:42:28,332] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 21:42:28,480] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 972 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:42:28,484] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:42:28,491] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,292 s
[2024-09-03 21:42:28,502] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:42:28,503] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:42:28,513] {spark_submit.py:495} INFO - 24/09/03 21:42:28 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,356860 s
[2024-09-03 21:42:29,063] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:42:29,068] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:42:29,069] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:42:29,175] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:42:29,175] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:42:29,176] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:42:29,280] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO CodeGenerator: Code generated in 36.86693 ms
[2024-09-03 21:42:29,329] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO CodeGenerator: Code generated in 32.897065 ms
[2024-09-03 21:42:29,337] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:42:29,347] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:42:29,349] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:43939 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:42:29,350] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:29,354] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648107 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:42:29,429] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:29,433] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:42:29,434] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:42:29,436] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:42:29,438] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:42:29,439] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:42:29,513] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:42:29,517] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:42:29,520] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:43939 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:42:29,521] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:42:29,523] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:42:29,523] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:42:29,530] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:42:29,531] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:42:29,613] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:42:29,614] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:42:29,614] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:42:29,700] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO CodeGenerator: Code generated in 22.825262 ms
[2024-09-03 21:42:29,706] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-453803, partition values: [empty row]
[2024-09-03 21:42:29,745] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO CodeGenerator: Code generated in 31.565561 ms
[2024-09-03 21:42:29,788] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO CodeGenerator: Code generated in 10.899561 ms
[2024-09-03 21:42:29,955] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409032142292481990211169181333_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_202409032142292481990211169181333_0001_m_000000
[2024-09-03 21:42:29,958] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO SparkHadoopMapRedUtil: attempt_202409032142292481990211169181333_0001_m_000000_1: Committed
[2024-09-03 21:42:29,966] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:42:29,974] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 448 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:42:29,979] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:42:29,982] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,538 s
[2024-09-03 21:42:29,983] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:42:29,985] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:42:29,992] {spark_submit.py:495} INFO - 24/09/03 21:42:29 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,561473 s
[2024-09-03 21:42:30,017] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileFormatWriter: Write Job dfdf2361-9622-4716-8468-66f988f1e066 committed.
[2024-09-03 21:42:30,022] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileFormatWriter: Finished processing stats for write job dfdf2361-9622-4716-8468-66f988f1e066.
[2024-09-03 21:42:30,089] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:42:30,089] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:42:30,090] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:42:30,108] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:42:30,109] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:42:30,110] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:42:30,150] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO CodeGenerator: Code generated in 14.06311 ms
[2024-09-03 21:42:30,157] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:42:30,169] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:42:30,170] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:43939 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:42:30,171] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:30,173] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648107 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:42:30,197] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:30,200] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:42:30,200] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:42:30,200] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:42:30,200] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:42:30,203] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:42:30,228] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:42:30,230] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:42:30,231] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:43939 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:42:30,232] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:42:30,233] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:42:30,233] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:42:30,237] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:42:30,238] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:42:30,254] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:42:30,254] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:42:30,255] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:42:30,310] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO CodeGenerator: Code generated in 20.475018 ms
[2024-09-03 21:42:30,312] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-453803, partition values: [empty row]
[2024-09-03 21:42:30,342] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO CodeGenerator: Code generated in 26.817622 ms
[2024-09-03 21:42:30,412] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_202409032142305040491382613156288_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409032142305040491382613156288_0002_m_000000
[2024-09-03 21:42:30,412] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkHadoopMapRedUtil: attempt_202409032142305040491382613156288_0002_m_000000_2: Committed
[2024-09-03 21:42:30,413] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:42:30,416] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 177 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:42:30,418] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,210 s
[2024-09-03 21:42:30,418] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:42:30,418] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:42:30,419] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:42:30,419] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,217072 s
[2024-09-03 21:42:30,446] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileFormatWriter: Write Job 56d67ec7-1342-44d2-9d82-7e804413e452 committed.
[2024-09-03 21:42:30,448] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO FileFormatWriter: Finished processing stats for write job 56d67ec7-1342-44d2-9d82-7e804413e452.
[2024-09-03 21:42:30,514] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:42:30,526] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:42:30,546] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:42:30,557] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:42:30,559] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO BlockManager: BlockManager stopped
[2024-09-03 21:42:30,574] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:42:30,579] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:42:30,590] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:42:30,592] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:42:30,594] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-256cbead-7536-4b79-ac8f-51a4d312dec4
[2024-09-03 21:42:30,598] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-256cbead-7536-4b79-ac8f-51a4d312dec4/pyspark-94d9c19f-6d17-4c72-9b94-4f518e8d9f85
[2024-09-03 21:42:30,611] {spark_submit.py:495} INFO - 24/09/03 21:42:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3622069-f46f-4f2e-8e49-ccc04c93dca3
[2024-09-03 21:42:30,701] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240904T004217, end_date=20240904T004230
[2024-09-03 21:42:30,751] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:42:30,770] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:50:24,540] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 22:50:24,546] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [queued]>
[2024-09-03 22:50:24,546] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:50:24,546] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:50:24,546] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:50:24,557] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-22 00:00:00+00:00
[2024-09-03 22:50:24,559] {standard_task_runner.py:52} INFO - Started process 449104 to run task
[2024-09-03 22:50:24,563] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-22T00:00:00+00:00', '--job-id', '128', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpwdqjc4zs', '--error-file', '/tmp/tmp03xvjk61']
[2024-09-03 22:50:24,564] {standard_task_runner.py:80} INFO - Job 128: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:50:24,603] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-22T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:50:24,649] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-22T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-22T00:00:00+00:00
[2024-09-03 22:50:24,653] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:50:24,654] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240822
[2024-09-03 22:50:25,718] {spark_submit.py:495} INFO - 24/09/03 22:50:25 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:50:25,719] {spark_submit.py:495} INFO - 24/09/03 22:50:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:50:26,044] {spark_submit.py:495} INFO - 24/09/03 22:50:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:50:26,553] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:50:26,561] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:50:26,600] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO ResourceUtils: ==============================================================
[2024-09-03 22:50:26,601] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:50:26,601] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO ResourceUtils: ==============================================================
[2024-09-03 22:50:26,601] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:50:26,620] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:50:26,633] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:50:26,634] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:50:26,682] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:50:26,683] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:50:26,683] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:50:26,683] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:50:26,684] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:50:26,832] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO Utils: Successfully started service 'sparkDriver' on port 40733.
[2024-09-03 22:50:26,855] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:50:26,881] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:50:26,896] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:50:26,897] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:50:26,902] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:50:26,916] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2cc2164c-f45f-443c-a1a6-a90461c75ae9
[2024-09-03 22:50:26,934] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:50:26,948] {spark_submit.py:495} INFO - 24/09/03 22:50:26 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:50:27,139] {spark_submit.py:495} INFO - 24/09/03 22:50:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:50:27,150] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:50:27,237] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:50:27,567] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:50:27,607] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40549.
[2024-09-03 22:50:27,607] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO NettyBlockTransferService: Server created on 192.168.2.128:40549
[2024-09-03 22:50:27,609] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:50:27,621] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40549, None)
[2024-09-03 22:50:27,625] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40549 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40549, None)
[2024-09-03 22:50:27,629] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40549, None)
[2024-09-03 22:50:27,632] {spark_submit.py:495} INFO - 24/09/03 22:50:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40549, None)
[2024-09-03 22:50:28,329] {spark_submit.py:495} INFO - 24/09/03 22:50:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:50:28,331] {spark_submit.py:495} INFO - 24/09/03 22:50:28 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:50:29,371] {spark_submit.py:495} INFO - 24/09/03 22:50:29 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
[2024-09-03 22:50:29,458] {spark_submit.py:495} INFO - 24/09/03 22:50:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2024-09-03 22:50:32,336] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:50:32,338] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:50:32,342] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:50:32,682] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:50:32,740] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:50:32,746] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40549 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:50:32,755] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:32,765] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198881 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:50:32,942] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:32,961] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:50:32,962] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:50:32,963] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:50:32,965] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:50:32,972] {spark_submit.py:495} INFO - 24/09/03 22:50:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:50:33,070] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:50:33,073] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:50:33,075] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40549 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:50:33,075] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:50:33,087] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:50:33,088] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:50:33,152] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:50:33,181] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:50:33,750] {spark_submit.py:495} INFO - 24/09/03 22:50:33 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4577, partition values: [empty row]
[2024-09-03 22:50:34,096] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO CodeGenerator: Code generated in 210.26077 ms
[2024-09-03 22:50:34,145] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 22:50:34,158] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1024 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:50:34,162] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:50:34,269] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,271 s
[2024-09-03 22:50:34,274] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:50:34,274] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:50:34,277] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,333576 s
[2024-09-03 22:50:34,713] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:50:34,714] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:50:34,715] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:50:34,811] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:34,811] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:34,812] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:34,914] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO CodeGenerator: Code generated in 40.549015 ms
[2024-09-03 22:50:34,965] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO CodeGenerator: Code generated in 33.518228 ms
[2024-09-03 22:50:34,974] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:50:34,986] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:50:34,987] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40549 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:50:34,989] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:34,993] {spark_submit.py:495} INFO - 24/09/03 22:50:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198881 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:50:35,078] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:35,081] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:50:35,081] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:50:35,082] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:50:35,082] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:50:35,082] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:50:35,137] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:50:35,139] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:50:35,140] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40549 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:50:35,141] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:50:35,141] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:50:35,142] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:50:35,145] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:50:35,146] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:50:35,224] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:35,224] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:35,225] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:35,301] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO CodeGenerator: Code generated in 26.622733 ms
[2024-09-03 22:50:35,307] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4577, partition values: [empty row]
[2024-09-03 22:50:35,334] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO CodeGenerator: Code generated in 24.162355 ms
[2024-09-03 22:50:35,366] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO CodeGenerator: Code generated in 6.036325 ms
[2024-09-03 22:50:35,412] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: Saved output of task 'attempt_20240903225035785359046460549663_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240822/_temporary/0/task_20240903225035785359046460549663_0001_m_000000
[2024-09-03 22:50:35,413] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkHadoopMapRedUtil: attempt_20240903225035785359046460549663_0001_m_000000_1: Committed
[2024-09-03 22:50:35,419] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:50:35,430] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 287 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:50:35,430] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:50:35,434] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,349 s
[2024-09-03 22:50:35,434] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:50:35,434] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:50:35,437] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,356960 s
[2024-09-03 22:50:35,485] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileFormatWriter: Write Job d7bb81b6-b487-422d-ad9d-fe4f1020956e committed.
[2024-09-03 22:50:35,491] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileFormatWriter: Finished processing stats for write job d7bb81b6-b487-422d-ad9d-fe4f1020956e.
[2024-09-03 22:50:35,579] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:50:35,579] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:50:35,579] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:50:35,594] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:35,594] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:35,595] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:35,666] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO CodeGenerator: Code generated in 17.542454 ms
[2024-09-03 22:50:35,675] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:50:35,686] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:50:35,688] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40549 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:50:35,690] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:35,691] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198881 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:50:35,716] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:35,719] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:50:35,719] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:50:35,719] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:50:35,719] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:50:35,724] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:50:35,749] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:50:35,751] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:50:35,753] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40549 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:50:35,755] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:50:35,756] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:50:35,756] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:50:35,757] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:50:35,758] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:50:35,780] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:35,780] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:35,791] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:35,848] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO CodeGenerator: Code generated in 20.185789 ms
[2024-09-03 22:50:35,851] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-22T00:00:00.00Z/data_engineer_20240822.json, range: 0-4577, partition values: [empty row]
[2024-09-03 22:50:35,874] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO CodeGenerator: Code generated in 20.702058 ms
[2024-09-03 22:50:35,885] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileOutputCommitter: Saved output of task 'attempt_202409032250354787297332600157703_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240822/_temporary/0/task_202409032250354787297332600157703_0002_m_000000
[2024-09-03 22:50:35,885] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkHadoopMapRedUtil: attempt_202409032250354787297332600157703_0002_m_000000_2: Committed
[2024-09-03 22:50:35,888] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:50:35,892] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 135 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:50:35,892] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:50:35,893] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,169 s
[2024-09-03 22:50:35,893] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:50:35,893] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:50:35,895] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,178918 s
[2024-09-03 22:50:35,912] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileFormatWriter: Write Job e7939ba5-1f51-4ac5-8192-93a3a0af6674 committed.
[2024-09-03 22:50:35,913] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO FileFormatWriter: Finished processing stats for write job e7939ba5-1f51-4ac5-8192-93a3a0af6674.
[2024-09-03 22:50:35,950] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:50:35,960] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:50:35,977] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:50:35,997] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:50:35,998] {spark_submit.py:495} INFO - 24/09/03 22:50:35 INFO BlockManager: BlockManager stopped
[2024-09-03 22:50:36,008] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:50:36,012] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:50:36,019] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:50:36,020] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:50:36,021] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b46628-7800-47a1-960f-f50dce46355c
[2024-09-03 22:50:36,023] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b46628-7800-47a1-960f-f50dce46355c/pyspark-28a78f10-07bd-4af8-b9c0-e183d793755c
[2024-09-03 22:50:36,026] {spark_submit.py:495} INFO - 24/09/03 22:50:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-46f47b74-762b-4242-8492-6cc16bb4223a
[2024-09-03 22:50:36,097] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240822T000000, start_date=20240904T015024, end_date=20240904T015036
[2024-09-03 22:50:36,123] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:50:36,147] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
