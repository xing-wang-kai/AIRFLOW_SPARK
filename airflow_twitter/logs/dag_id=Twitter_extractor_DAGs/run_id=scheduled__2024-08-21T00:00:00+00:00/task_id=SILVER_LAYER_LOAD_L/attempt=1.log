[2024-09-03 16:17:38,783] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 16:17:38,787] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 16:17:38,788] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:17:38,788] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:17:38,788] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:17:38,798] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 16:17:38,801] {standard_task_runner.py:52} INFO - Started process 280211 to run task
[2024-09-03 16:17:38,804] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp3vtoje3u', '--error-file', '/tmp/tmp9m_cuigd']
[2024-09-03 16:17:38,804] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:17:38,841] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:17:38,885] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 16:17:38,888] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:17:38,889] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-21T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240821
[2024-09-03 16:17:39,916] {spark_submit.py:495} INFO - 24/09/03 16:17:39 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:17:39,917] {spark_submit.py:495} INFO - 24/09/03 16:17:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:17:40,320] {spark_submit.py:495} INFO - 24/09/03 16:17:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:17:40,857] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:17:40,865] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:17:40,914] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO ResourceUtils: ==============================================================
[2024-09-03 16:17:40,914] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:17:40,914] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO ResourceUtils: ==============================================================
[2024-09-03 16:17:40,915] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:17:40,940] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:17:40,953] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:17:40,953] {spark_submit.py:495} INFO - 24/09/03 16:17:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:17:41,000] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:17:41,000] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:17:41,001] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:17:41,001] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:17:41,002] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:17:41,203] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO Utils: Successfully started service 'sparkDriver' on port 37027.
[2024-09-03 16:17:41,235] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:17:41,283] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:17:41,305] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:17:41,306] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:17:41,310] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:17:41,328] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-64df0612-7d39-4a91-afce-25666e3f3a86
[2024-09-03 16:17:41,356] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:17:41,377] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:17:41,653] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:17:41,724] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:17:41,951] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:17:41,972] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34193.
[2024-09-03 16:17:41,973] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO NettyBlockTransferService: Server created on 192.168.2.128:34193
[2024-09-03 16:17:41,975] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:17:41,981] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34193, None)
[2024-09-03 16:17:41,985] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34193 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34193, None)
[2024-09-03 16:17:41,990] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34193, None)
[2024-09-03 16:17:41,991] {spark_submit.py:495} INFO - 24/09/03 16:17:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34193, None)
[2024-09-03 16:17:42,602] {spark_submit.py:495} INFO - 24/09/03 16:17:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:17:42,604] {spark_submit.py:495} INFO - 24/09/03 16:17:42 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 76, in <module>
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - define_extration(spark,
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 48, in define_extration
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - df = spark.read.json(src)
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 372, in json
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2024-09-03 16:17:43,597] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2024-09-03 16:17:43,607] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-21T00:00:00.00Z
[2024-09-03 16:17:43,669] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:17:43,686] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:17:43,702] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:17:43,722] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:17:43,723] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO BlockManager: BlockManager stopped
[2024-09-03 16:17:43,733] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:17:43,737] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:17:43,754] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:17:43,756] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:17:43,760] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ba5391b-d6d1-4c35-86a7-7cf43d0d89b3
[2024-09-03 16:17:43,765] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-87543e0b-7f86-43ec-9be9-ab6898c172db/pyspark-ae89d1cb-4695-4005-816a-9756629eba46
[2024-09-03 16:17:43,769] {spark_submit.py:495} INFO - 24/09/03 16:17:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-87543e0b-7f86-43ec-9be9-ab6898c172db
[2024-09-03 16:17:43,844] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-21T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240821. Error code is: 1.
[2024-09-03 16:17:43,850] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T191738, end_date=20240903T191743
[2024-09-03 16:17:43,862] {standard_task_runner.py:92} ERROR - Failed to execute job 79 for task SILVER_LAYER_LOAD_L (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-21T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240821. Error code is: 1.; 280211)
[2024-09-03 16:17:43,871] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-09-03 16:17:43,890] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:21:55,161] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 16:21:55,168] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 16:21:55,168] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:21:55,169] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:21:55,169] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:21:55,182] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 16:21:55,184] {standard_task_runner.py:52} INFO - Started process 282856 to run task
[2024-09-03 16:21:55,187] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmptpazxjh8', '--error-file', '/tmp/tmp4usdbz97']
[2024-09-03 16:21:55,188] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:21:55,220] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:21:55,260] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 16:21:55,264] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:21:55,265] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/ --process-data 20240821
[2024-09-03 16:21:56,303] {spark_submit.py:495} INFO - 24/09/03 16:21:56 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:21:56,303] {spark_submit.py:495} INFO - 24/09/03 16:21:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:21:56,619] {spark_submit.py:495} INFO - 24/09/03 16:21:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:21:57,123] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:21:57,131] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:21:57,174] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO ResourceUtils: ==============================================================
[2024-09-03 16:21:57,174] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:21:57,174] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO ResourceUtils: ==============================================================
[2024-09-03 16:21:57,175] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:21:57,202] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:21:57,214] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:21:57,215] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:21:57,258] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:21:57,258] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:21:57,258] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:21:57,258] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:21:57,258] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:21:57,410] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO Utils: Successfully started service 'sparkDriver' on port 42907.
[2024-09-03 16:21:57,432] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:21:57,459] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:21:57,476] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:21:57,477] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:21:57,480] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:21:57,490] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83be233b-7e81-4799-bdc0-2e8f524930ce
[2024-09-03 16:21:57,509] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:21:57,523] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:21:57,739] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:21:57,790] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:21:57,965] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:21:57,986] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34355.
[2024-09-03 16:21:57,986] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO NettyBlockTransferService: Server created on 192.168.2.128:34355
[2024-09-03 16:21:57,988] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:21:57,992] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34355, None)
[2024-09-03 16:21:57,998] {spark_submit.py:495} INFO - 24/09/03 16:21:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34355 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34355, None)
[2024-09-03 16:21:58,000] {spark_submit.py:495} INFO - 24/09/03 16:21:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34355, None)
[2024-09-03 16:21:58,001] {spark_submit.py:495} INFO - 24/09/03 16:21:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34355, None)
[2024-09-03 16:21:58,455] {spark_submit.py:495} INFO - 24/09/03 16:21:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:21:58,455] {spark_submit.py:495} INFO - 24/09/03 16:21:58 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:21:59,211] {spark_submit.py:495} INFO - 24/09/03 16:21:59 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 16:21:59,271] {spark_submit.py:495} INFO - 24/09/03 16:21:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:22:00,953] {spark_submit.py:495} INFO - 24/09/03 16:22:00 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:22:00,954] {spark_submit.py:495} INFO - 24/09/03 16:22:00 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:22:00,958] {spark_submit.py:495} INFO - 24/09/03 16:22:00 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:22:01,205] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:22:01,254] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:22:01,256] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34355 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:22:01,261] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:01,271] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198819 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:01,413] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:01,429] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:01,431] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:01,431] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:01,433] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:01,438] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:01,522] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:22:01,526] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:22:01,527] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34355 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:22:01,528] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:01,540] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:01,541] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:22:01,589] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4967 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:01,605] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:22:01,873] {spark_submit.py:495} INFO - 24/09/03 16:22:01 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4515, partition values: [empty row]
[2024-09-03 16:22:02,093] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO CodeGenerator: Code generated in 130.78061 ms
[2024-09-03 16:22:02,135] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:22:02,147] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 567 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:02,148] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:22:02,253] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,803 s
[2024-09-03 16:22:02,255] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:02,256] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:22:02,259] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,845050 s
[2024-09-03 16:22:02,628] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:22:02,629] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:22:02,630] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:22:02,702] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:02,702] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:02,703] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:02,772] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO CodeGenerator: Code generated in 27.295012 ms
[2024-09-03 16:22:02,817] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO CodeGenerator: Code generated in 31.799853 ms
[2024-09-03 16:22:02,834] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:22:02,843] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:22:02,844] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34355 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:22:02,845] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:02,847] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198819 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:02,899] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:02,901] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:02,901] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:02,902] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:02,902] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:02,902] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:02,949] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:22:02,952] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:22:02,952] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34355 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:22:02,953] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:02,953] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:02,953] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:22:02,957] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:02,957] {spark_submit.py:495} INFO - 24/09/03 16:22:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:22:03,007] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:03,007] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:03,008] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:03,064] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO CodeGenerator: Code generated in 18.680673 ms
[2024-09-03 16:22:03,068] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4515, partition values: [empty row]
[2024-09-03 16:22:03,089] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO CodeGenerator: Code generated in 18.448452 ms
[2024-09-03 16:22:03,111] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO CodeGenerator: Code generated in 5.081213 ms
[2024-09-03 16:22:03,144] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: Saved output of task 'attempt_202409031622022323713249902990002_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/twitter_df/process_data=20240821/_temporary/0/task_202409031622022323713249902990002_0001_m_000000
[2024-09-03 16:22:03,144] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkHadoopMapRedUtil: attempt_202409031622022323713249902990002_0001_m_000000_1: Committed
[2024-09-03 16:22:03,150] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:22:03,153] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 199 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:03,154] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,247 s
[2024-09-03 16:22:03,154] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:03,154] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:22:03,154] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:22:03,155] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,255870 s
[2024-09-03 16:22:03,177] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileFormatWriter: Write Job f4132b1f-1787-4c5a-83df-672eee201438 committed.
[2024-09-03 16:22:03,180] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileFormatWriter: Finished processing stats for write job f4132b1f-1787-4c5a-83df-672eee201438.
[2024-09-03 16:22:03,223] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:22:03,223] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:22:03,224] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:22:03,237] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:03,237] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:03,238] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:03,271] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO CodeGenerator: Code generated in 13.946303 ms
[2024-09-03 16:22:03,276] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:22:03,284] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:22:03,286] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34355 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:22:03,287] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:03,288] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198819 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:03,304] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:03,305] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:03,305] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:03,306] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:03,306] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:03,306] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:03,331] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:22:03,335] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.9 KiB, free 364.8 MiB)
[2024-09-03 16:22:03,336] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34355 (size: 63.9 KiB, free: 366.1 MiB)
[2024-09-03 16:22:03,337] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:03,338] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:03,338] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:22:03,339] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:03,343] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:22:03,352] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:03,352] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:03,353] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:03,383] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO CodeGenerator: Code generated in 10.204413 ms
[2024-09-03 16:22:03,386] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4515, partition values: [empty row]
[2024-09-03 16:22:03,403] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO CodeGenerator: Code generated in 14.443198 ms
[2024-09-03 16:22:03,412] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileOutputCommitter: Saved output of task 'attempt_202409031622037321796326902418665_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/user_df/process_data=20240821/_temporary/0/task_202409031622037321796326902418665_0002_m_000000
[2024-09-03 16:22:03,413] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkHadoopMapRedUtil: attempt_202409031622037321796326902418665_0002_m_000000_2: Committed
[2024-09-03 16:22:03,415] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:22:03,418] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 79 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:03,420] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,112 s
[2024-09-03 16:22:03,421] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:03,422] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:22:03,423] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:22:03,423] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,118162 s
[2024-09-03 16:22:03,460] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileFormatWriter: Write Job 77d87bce-35e2-4b2a-974c-dbef29aeb54c committed.
[2024-09-03 16:22:03,461] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO FileFormatWriter: Finished processing stats for write job 77d87bce-35e2-4b2a-974c-dbef29aeb54c.
[2024-09-03 16:22:03,490] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:22:03,501] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:22:03,519] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:22:03,527] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:22:03,527] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO BlockManager: BlockManager stopped
[2024-09-03 16:22:03,537] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:22:03,540] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:22:03,544] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:22:03,544] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:22:03,544] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0e37ad0-4ad8-4b07-ad7b-99dfb9767a20
[2024-09-03 16:22:03,548] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f91da9e-60f7-45a3-b263-97320412d382/pyspark-705d37d3-52a3-49f3-8b50-de1a4559f457
[2024-09-03 16:22:03,550] {spark_submit.py:495} INFO - 24/09/03 16:22:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f91da9e-60f7-45a3-b263-97320412d382
[2024-09-03 16:22:03,613] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T192155, end_date=20240903T192203
[2024-09-03 16:22:03,637] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:22:03,647] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:40:03,855] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 16:40:03,861] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 16:40:03,862] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:40:03,862] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:40:03,862] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:40:03,870] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 16:40:03,873] {standard_task_runner.py:52} INFO - Started process 293732 to run task
[2024-09-03 16:40:03,875] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpvxpyzy2f', '--error-file', '/tmp/tmpddxazeho']
[2024-09-03 16:40:03,876] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:40:03,907] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:40:03,947] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 16:40:03,951] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:40:03,952] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 16:40:04,930] {spark_submit.py:495} INFO - 24/09/03 16:40:04 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:40:04,931] {spark_submit.py:495} INFO - 24/09/03 16:40:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:40:05,254] {spark_submit.py:495} INFO - 24/09/03 16:40:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:40:05,793] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:40:05,804] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:40:05,856] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO ResourceUtils: ==============================================================
[2024-09-03 16:40:05,857] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:40:05,857] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO ResourceUtils: ==============================================================
[2024-09-03 16:40:05,857] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:40:05,883] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:40:05,897] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:40:05,898] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:40:05,940] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:40:05,940] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:40:05,940] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:40:05,940] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:40:05,941] {spark_submit.py:495} INFO - 24/09/03 16:40:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:40:06,099] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO Utils: Successfully started service 'sparkDriver' on port 43001.
[2024-09-03 16:40:06,123] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:40:06,151] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:40:06,167] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:40:06,168] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:40:06,170] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:40:06,182] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-61b26e2b-f89f-40d0-9277-5256d0af2140
[2024-09-03 16:40:06,206] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:40:06,222] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:40:06,459] {spark_submit.py:495} INFO - 24/09/03 16:40:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:40:06,469] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:40:06,528] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:40:06,731] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:40:06,752] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35579.
[2024-09-03 16:40:06,753] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO NettyBlockTransferService: Server created on 192.168.2.128:35579
[2024-09-03 16:40:06,755] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:40:06,762] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 35579, None)
[2024-09-03 16:40:06,766] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:35579 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 35579, None)
[2024-09-03 16:40:06,768] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 35579, None)
[2024-09-03 16:40:06,770] {spark_submit.py:495} INFO - 24/09/03 16:40:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 35579, None)
[2024-09-03 16:40:07,257] {spark_submit.py:495} INFO - 24/09/03 16:40:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:40:07,258] {spark_submit.py:495} INFO - 24/09/03 16:40:07 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:40:08,022] {spark_submit.py:495} INFO - 24/09/03 16:40:08 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 16:40:08,076] {spark_submit.py:495} INFO - 24/09/03 16:40:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:40:09,758] {spark_submit.py:495} INFO - 24/09/03 16:40:09 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:09,759] {spark_submit.py:495} INFO - 24/09/03 16:40:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:09,762] {spark_submit.py:495} INFO - 24/09/03 16:40:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:40:10,000] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:10,046] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:40:10,049] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:35579 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:40:10,056] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:10,079] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:10,233] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:10,246] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:10,246] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:10,247] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:10,248] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:10,262] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:10,341] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:10,344] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:40:10,344] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:35579 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:40:10,346] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:10,358] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:10,359] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:40:10,407] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:10,427] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:40:10,683] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455536, partition values: [empty row]
[2024-09-03 16:40:10,902] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO CodeGenerator: Code generated in 131.160538 ms
[2024-09-03 16:40:10,998] {spark_submit.py:495} INFO - 24/09/03 16:40:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 16:40:11,115] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 610 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:11,117] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:40:11,125] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,843 s
[2024-09-03 16:40:11,128] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:11,129] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:40:11,132] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,898090 s
[2024-09-03 16:40:11,503] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:40:11,505] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:40:11,506] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:40:11,580] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:11,580] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:11,582] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:11,710] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO CodeGenerator: Code generated in 37.00893 ms
[2024-09-03 16:40:11,751] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO CodeGenerator: Code generated in 26.132778 ms
[2024-09-03 16:40:11,756] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:40:11,762] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:40:11,763] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:35579 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:40:11,764] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:11,767] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:11,818] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:11,819] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:11,819] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:11,819] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:11,820] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:11,823] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:11,867] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:40:11,869] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:40:11,869] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:35579 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:40:11,870] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:11,870] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:11,870] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:40:11,875] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:11,880] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:40:11,921] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:11,921] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:11,922] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:11,978] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO CodeGenerator: Code generated in 19.173337 ms
[2024-09-03 16:40:11,981] {spark_submit.py:495} INFO - 24/09/03 16:40:11 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455536, partition values: [empty row]
[2024-09-03 16:40:12,002] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO CodeGenerator: Code generated in 17.737183 ms
[2024-09-03 16:40:12,032] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO CodeGenerator: Code generated in 5.841447 ms
[2024-09-03 16:40:12,128] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640111136610976363018361_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409031640111136610976363018361_0001_m_000000
[2024-09-03 16:40:12,129] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkHadoopMapRedUtil: attempt_202409031640111136610976363018361_0001_m_000000_1: Committed
[2024-09-03 16:40:12,133] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:40:12,135] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 263 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:12,136] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,310 s
[2024-09-03 16:40:12,136] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:12,136] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:40:12,136] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:40:12,138] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,319156 s
[2024-09-03 16:40:12,175] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileFormatWriter: Write Job b7ad07f2-6ffb-4fc6-bf67-b51af0b88b64 committed.
[2024-09-03 16:40:12,177] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileFormatWriter: Finished processing stats for write job b7ad07f2-6ffb-4fc6-bf67-b51af0b88b64.
[2024-09-03 16:40:12,225] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:12,226] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:12,227] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:40:12,240] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:12,241] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:12,241] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:12,281] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO CodeGenerator: Code generated in 10.961614 ms
[2024-09-03 16:40:12,285] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:40:12,292] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:40:12,292] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:35579 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:40:12,293] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:12,294] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:12,312] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:12,314] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:12,314] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:12,315] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:12,315] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:12,315] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:12,332] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:40:12,335] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:40:12,335] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:35579 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:40:12,336] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:12,336] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:12,336] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:40:12,338] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:12,338] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:40:12,349] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:12,350] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:12,350] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:12,383] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO CodeGenerator: Code generated in 11.169384 ms
[2024-09-03 16:40:12,386] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455536, partition values: [empty row]
[2024-09-03 16:40:12,404] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO CodeGenerator: Code generated in 14.925991 ms
[2024-09-03 16:40:12,435] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640124778547933747529264_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_202409031640124778547933747529264_0002_m_000000
[2024-09-03 16:40:12,435] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkHadoopMapRedUtil: attempt_202409031640124778547933747529264_0002_m_000000_2: Committed
[2024-09-03 16:40:12,436] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:40:12,438] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 100 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:12,438] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:40:12,439] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,122 s
[2024-09-03 16:40:12,439] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:12,439] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:40:12,439] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,127280 s
[2024-09-03 16:40:12,457] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileFormatWriter: Write Job 320e7db4-4960-4061-9fe3-456570f4e0b2 committed.
[2024-09-03 16:40:12,458] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO FileFormatWriter: Finished processing stats for write job 320e7db4-4960-4061-9fe3-456570f4e0b2.
[2024-09-03 16:40:12,489] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:40:12,499] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:40:12,508] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:40:12,517] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:40:12,517] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO BlockManager: BlockManager stopped
[2024-09-03 16:40:12,523] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:40:12,526] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:40:12,530] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:40:12,530] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:40:12,531] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-970d2ff5-0ba3-43a5-882d-ac133751155a
[2024-09-03 16:40:12,533] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-16c1ce4f-2ec7-473a-b852-157c0b06c619
[2024-09-03 16:40:12,535] {spark_submit.py:495} INFO - 24/09/03 16:40:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-970d2ff5-0ba3-43a5-882d-ac133751155a/pyspark-ebc7d745-9342-4df3-875b-db86e80e16f9
[2024-09-03 16:40:12,638] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T194003, end_date=20240903T194012
[2024-09-03 16:40:12,691] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:40:12,718] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:53:29,445] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 17:53:29,452] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 17:53:29,452] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:53:29,452] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:53:29,452] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:53:29,461] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 17:53:29,464] {standard_task_runner.py:52} INFO - Started process 323476 to run task
[2024-09-03 17:53:29,467] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpf33_1er9', '--error-file', '/tmp/tmp5tnf6zc4']
[2024-09-03 17:53:29,468] {standard_task_runner.py:80} INFO - Job 81: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:53:29,499] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:53:29,544] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 17:53:29,548] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:53:29,549] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 17:53:30,557] {spark_submit.py:495} INFO - 24/09/03 17:53:30 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:53:30,558] {spark_submit.py:495} INFO - 24/09/03 17:53:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:53:30,902] {spark_submit.py:495} INFO - 24/09/03 17:53:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:53:31,418] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:53:31,426] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:53:31,461] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO ResourceUtils: ==============================================================
[2024-09-03 17:53:31,461] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:53:31,462] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO ResourceUtils: ==============================================================
[2024-09-03 17:53:31,463] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:53:31,483] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:53:31,496] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:53:31,497] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:53:31,539] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:53:31,540] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:53:31,540] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:53:31,540] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:53:31,541] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:53:31,685] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO Utils: Successfully started service 'sparkDriver' on port 46333.
[2024-09-03 17:53:31,706] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:53:31,733] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:53:31,752] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:53:31,753] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:53:31,756] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:53:31,767] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-452c39df-a120-4492-9754-3ee36d40ba08
[2024-09-03 17:53:31,788] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:53:31,803] {spark_submit.py:495} INFO - 24/09/03 17:53:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:53:32,003] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:53:32,054] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:53:32,244] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:53:32,269] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36611.
[2024-09-03 17:53:32,269] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO NettyBlockTransferService: Server created on 192.168.2.128:36611
[2024-09-03 17:53:32,271] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:53:32,278] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36611, None)
[2024-09-03 17:53:32,282] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36611 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36611, None)
[2024-09-03 17:53:32,285] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36611, None)
[2024-09-03 17:53:32,286] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36611, None)
[2024-09-03 17:53:32,768] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:53:32,768] {spark_submit.py:495} INFO - 24/09/03 17:53:32 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:53:33,600] {spark_submit.py:495} INFO - 24/09/03 17:53:33 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 17:53:33,654] {spark_submit.py:495} INFO - 24/09/03 17:53:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:53:35,280] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:53:35,281] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:53:35,285] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:53:35,529] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:53:35,574] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:53:35,577] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36611 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:53:35,581] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:35,588] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649568 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:53:35,730] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:35,747] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:53:35,748] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:53:35,748] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:53:35,750] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:53:35,763] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:53:35,840] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:53:35,843] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:53:35,843] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36611 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:53:35,844] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:53:35,854] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:53:35,855] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:53:35,900] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:53:35,917] {spark_submit.py:495} INFO - 24/09/03 17:53:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:53:36,192] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455264, partition values: [empty row]
[2024-09-03 17:53:36,413] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO CodeGenerator: Code generated in 136.214751 ms
[2024-09-03 17:53:36,507] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 17:53:36,617] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 627 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:53:36,623] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:53:36,625] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,848 s
[2024-09-03 17:53:36,629] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:53:36,629] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:53:36,632] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,900639 s
[2024-09-03 17:53:36,996] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:53:36,998] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:53:36,999] {spark_submit.py:495} INFO - 24/09/03 17:53:36 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:53:37,067] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:37,067] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:37,069] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:37,153] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 24.127947 ms
[2024-09-03 17:53:37,193] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 26.041044 ms
[2024-09-03 17:53:37,199] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:53:37,205] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:53:37,206] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36611 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:53:37,207] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:37,211] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649568 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:53:37,277] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:37,281] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:53:37,281] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:53:37,281] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:53:37,282] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:53:37,288] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:53:37,323] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:53:37,325] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:53:37,326] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36611 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:53:37,326] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:53:37,327] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:53:37,327] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:53:37,330] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:53:37,331] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:53:37,378] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:37,379] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:37,379] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:37,439] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 26.252278 ms
[2024-09-03 17:53:37,444] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455264, partition values: [empty row]
[2024-09-03 17:53:37,481] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 34.600951 ms
[2024-09-03 17:53:37,513] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 8.595465 ms
[2024-09-03 17:53:37,630] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: Saved output of task 'attempt_202409031753375650010923362537056_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409031753375650010923362537056_0001_m_000000
[2024-09-03 17:53:37,631] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkHadoopMapRedUtil: attempt_202409031753375650010923362537056_0001_m_000000_1: Committed
[2024-09-03 17:53:37,635] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:53:37,639] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 312 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:53:37,641] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,352 s
[2024-09-03 17:53:37,642] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:53:37,643] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:53:37,643] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:53:37,644] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,365517 s
[2024-09-03 17:53:37,660] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileFormatWriter: Write Job 687e71ce-716e-4481-bd22-a3d1c61058dc committed.
[2024-09-03 17:53:37,662] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileFormatWriter: Finished processing stats for write job 687e71ce-716e-4481-bd22-a3d1c61058dc.
[2024-09-03 17:53:37,714] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:53:37,714] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:53:37,715] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:53:37,726] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:37,726] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:37,728] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:37,769] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 8.026321 ms
[2024-09-03 17:53:37,772] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:53:37,778] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:53:37,779] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36611 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:53:37,779] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:37,780] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649568 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:53:37,796] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:37,798] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:53:37,799] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:53:37,799] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:53:37,799] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:53:37,801] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:53:37,818] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:53:37,819] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:53:37,820] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36611 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:53:37,820] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:53:37,821] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:53:37,821] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:53:37,821] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:53:37,822] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:53:37,836] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:37,837] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:37,837] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:37,877] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 9.188557 ms
[2024-09-03 17:53:37,879] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455264, partition values: [empty row]
[2024-09-03 17:53:37,893] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO CodeGenerator: Code generated in 11.485221 ms
[2024-09-03 17:53:37,939] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileOutputCommitter: Saved output of task 'attempt_202409031753371559873703219446970_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_202409031753371559873703219446970_0002_m_000000
[2024-09-03 17:53:37,939] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO SparkHadoopMapRedUtil: attempt_202409031753371559873703219446970_0002_m_000000_2: Committed
[2024-09-03 17:53:37,942] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:53:37,944] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 123 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:53:37,944] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:53:37,945] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,144 s
[2024-09-03 17:53:37,945] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:53:37,945] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:53:37,945] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,148963 s
[2024-09-03 17:53:37,981] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileFormatWriter: Write Job 1cb2bfe5-8b03-4643-b905-5e06b049079d committed.
[2024-09-03 17:53:37,982] {spark_submit.py:495} INFO - 24/09/03 17:53:37 INFO FileFormatWriter: Finished processing stats for write job 1cb2bfe5-8b03-4643-b905-5e06b049079d.
[2024-09-03 17:53:38,075] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:53:38,095] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:53:38,114] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:53:38,131] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:53:38,131] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO BlockManager: BlockManager stopped
[2024-09-03 17:53:38,138] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:53:38,140] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:53:38,145] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:53:38,146] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:53:38,146] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-0653cf60-3cf4-4b7a-926f-c3e33a8605c8/pyspark-6a198b2b-4cbd-4fdb-9ec5-6fecb86e4f76
[2024-09-03 17:53:38,149] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-0653cf60-3cf4-4b7a-926f-c3e33a8605c8
[2024-09-03 17:53:38,151] {spark_submit.py:495} INFO - 24/09/03 17:53:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-db15260b-5ad0-4816-8a37-be86838bc06d
[2024-09-03 17:53:38,229] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T205329, end_date=20240903T205338
[2024-09-03 17:53:38,274] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:53:38,287] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:09:58,144] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 18:09:58,149] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 18:09:58,150] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:09:58,150] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:09:58,150] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:09:58,159] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 18:09:58,162] {standard_task_runner.py:52} INFO - Started process 331650 to run task
[2024-09-03 18:09:58,166] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmplc_e44f0', '--error-file', '/tmp/tmpwn18_mkf']
[2024-09-03 18:09:58,166] {standard_task_runner.py:80} INFO - Job 81: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:09:58,199] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:09:58,238] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 18:09:58,242] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:09:58,243] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 18:09:59,300] {spark_submit.py:495} INFO - 24/09/03 18:09:59 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:09:59,301] {spark_submit.py:495} INFO - 24/09/03 18:09:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:09:59,625] {spark_submit.py:495} INFO - 24/09/03 18:09:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:10:00,188] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:10:00,202] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:10:00,257] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO ResourceUtils: ==============================================================
[2024-09-03 18:10:00,258] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:10:00,258] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO ResourceUtils: ==============================================================
[2024-09-03 18:10:00,258] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:10:00,298] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:10:00,312] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:10:00,313] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:10:00,372] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:10:00,372] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:10:00,373] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:10:00,373] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:10:00,374] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:10:00,577] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO Utils: Successfully started service 'sparkDriver' on port 39889.
[2024-09-03 18:10:00,615] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:10:00,647] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:10:00,665] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:10:00,665] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:10:00,671] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:10:00,683] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f9aa2429-0e24-4c8c-9082-2ec26e2de574
[2024-09-03 18:10:00,712] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:10:00,730] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:10:00,921] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:10:00,973] {spark_submit.py:495} INFO - 24/09/03 18:10:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:10:01,223] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:10:01,248] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40913.
[2024-09-03 18:10:01,248] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO NettyBlockTransferService: Server created on 192.168.2.128:40913
[2024-09-03 18:10:01,250] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:10:01,255] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40913, None)
[2024-09-03 18:10:01,258] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40913 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40913, None)
[2024-09-03 18:10:01,264] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40913, None)
[2024-09-03 18:10:01,265] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40913, None)
[2024-09-03 18:10:01,744] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:10:01,745] {spark_submit.py:495} INFO - 24/09/03 18:10:01 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:10:02,530] {spark_submit.py:495} INFO - 24/09/03 18:10:02 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 18:10:02,591] {spark_submit.py:495} INFO - 24/09/03 18:10:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:10:04,362] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:10:04,363] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:10:04,368] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:10:04,731] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:10:04,817] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:10:04,821] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40913 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:10:04,832] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:04,849] {spark_submit.py:495} INFO - 24/09/03 18:10:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648610 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:10:05,172] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:05,191] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:10:05,192] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:10:05,192] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:10:05,193] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:10:05,200] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:10:05,310] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:10:05,315] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:10:05,316] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40913 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:10:05,317] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:10:05,331] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:10:05,333] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:10:05,398] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:10:05,425] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:10:05,904] {spark_submit.py:495} INFO - 24/09/03 18:10:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-454306, partition values: [empty row]
[2024-09-03 18:10:06,307] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO CodeGenerator: Code generated in 221.51424 ms
[2024-09-03 18:10:06,446] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:10:06,644] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1096 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:10:06,644] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:10:06,667] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,448 s
[2024-09-03 18:10:06,672] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:10:06,672] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:10:06,679] {spark_submit.py:495} INFO - 24/09/03 18:10:06 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,502623 s
[2024-09-03 18:10:07,251] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:10:07,253] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:10:07,254] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:10:07,354] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:07,354] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:07,355] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:07,455] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO CodeGenerator: Code generated in 41.392347 ms
[2024-09-03 18:10:07,520] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO CodeGenerator: Code generated in 46.800468 ms
[2024-09-03 18:10:07,528] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:10:07,549] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:10:07,552] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40913 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:10:07,554] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:07,559] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648610 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:10:07,642] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:07,646] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:10:07,647] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:10:07,647] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:10:07,648] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:10:07,654] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:10:07,724] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:10:07,729] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:10:07,732] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40913 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:10:07,734] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:10:07,735] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:10:07,736] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:10:07,740] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:10:07,740] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:10:07,839] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:07,839] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:07,840] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:07,938] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO CodeGenerator: Code generated in 32.195373 ms
[2024-09-03 18:10:07,941] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-454306, partition values: [empty row]
[2024-09-03 18:10:07,975] {spark_submit.py:495} INFO - 24/09/03 18:10:07 INFO CodeGenerator: Code generated in 28.146885 ms
[2024-09-03 18:10:08,017] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO CodeGenerator: Code generated in 15.043577 ms
[2024-09-03 18:10:08,292] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileOutputCommitter: Saved output of task 'attempt_202409031810076921296382424297325_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409031810076921296382424297325_0001_m_000000
[2024-09-03 18:10:08,292] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SparkHadoopMapRedUtil: attempt_202409031810076921296382424297325_0001_m_000000_1: Committed
[2024-09-03 18:10:08,303] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:10:08,314] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 576 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:10:08,315] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,659 s
[2024-09-03 18:10:08,315] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:10:08,316] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:10:08,317] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:10:08,317] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,673896 s
[2024-09-03 18:10:08,365] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileFormatWriter: Write Job 54151da7-06ee-455a-a662-fe8d04c3dfad committed.
[2024-09-03 18:10:08,375] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileFormatWriter: Finished processing stats for write job 54151da7-06ee-455a-a662-fe8d04c3dfad.
[2024-09-03 18:10:08,475] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:10:08,475] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:10:08,483] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:10:08,500] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:08,500] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:08,504] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:08,581] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO CodeGenerator: Code generated in 36.689887 ms
[2024-09-03 18:10:08,589] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:10:08,614] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:10:08,616] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40913 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:10:08,618] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:08,621] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648610 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:10:08,658] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:10:08,662] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:10:08,662] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:10:08,663] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:10:08,664] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:10:08,666] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:10:08,702] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:10:08,706] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:10:08,707] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40913 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:10:08,708] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:10:08,709] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:10:08,709] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:10:08,710] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:10:08,712] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:10:08,730] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:10:08,730] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:10:08,731] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:10:08,856] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO CodeGenerator: Code generated in 51.321092 ms
[2024-09-03 18:10:08,862] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-454306, partition values: [empty row]
[2024-09-03 18:10:08,912] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO CodeGenerator: Code generated in 42.304884 ms
[2024-09-03 18:10:08,977] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO FileOutputCommitter: Saved output of task 'attempt_202409031810087159978205233738876_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_202409031810087159978205233738876_0002_m_000000
[2024-09-03 18:10:08,978] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO SparkHadoopMapRedUtil: attempt_202409031810087159978205233738876_0002_m_000000_2: Committed
[2024-09-03 18:10:08,986] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:10:08,988] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 277 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:10:08,988] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:10:08,988] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,321 s
[2024-09-03 18:10:08,989] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:10:08,989] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:10:08,990] {spark_submit.py:495} INFO - 24/09/03 18:10:08 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,332105 s
[2024-09-03 18:10:09,020] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO FileFormatWriter: Write Job 74d9b1a9-59ad-4f09-bc22-57002f449986 committed.
[2024-09-03 18:10:09,020] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO FileFormatWriter: Finished processing stats for write job 74d9b1a9-59ad-4f09-bc22-57002f449986.
[2024-09-03 18:10:09,073] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:10:09,089] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:10:09,108] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:10:09,127] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:10:09,128] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO BlockManager: BlockManager stopped
[2024-09-03 18:10:09,138] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:10:09,141] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:10:09,154] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:10:09,156] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:10:09,156] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-c43ce05f-dd28-41e0-8773-fc7aebc88bd7
[2024-09-03 18:10:09,159] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-c43ce05f-dd28-41e0-8773-fc7aebc88bd7/pyspark-e2092434-6881-49d7-ab13-4e5226b0a35d
[2024-09-03 18:10:09,167] {spark_submit.py:495} INFO - 24/09/03 18:10:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-90f91470-7567-4a8f-a568-36391c0e1a11
[2024-09-03 18:10:09,256] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T210958, end_date=20240903T211009
[2024-09-03 18:10:09,293] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:10:09,311] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:14:32,170] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 18:14:32,175] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 18:14:32,175] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:14:32,176] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:14:32,176] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:14:32,186] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 18:14:32,189] {standard_task_runner.py:52} INFO - Started process 335912 to run task
[2024-09-03 18:14:32,191] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpg6kfc1s6', '--error-file', '/tmp/tmpcbsq5guo']
[2024-09-03 18:14:32,192] {standard_task_runner.py:80} INFO - Job 81: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:14:32,223] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:14:32,263] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 18:14:32,267] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:14:32,267] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 18:14:33,297] {spark_submit.py:495} INFO - 24/09/03 18:14:33 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:14:33,297] {spark_submit.py:495} INFO - 24/09/03 18:14:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:14:33,605] {spark_submit.py:495} INFO - 24/09/03 18:14:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:14:34,131] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:14:34,139] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:14:34,172] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO ResourceUtils: ==============================================================
[2024-09-03 18:14:34,173] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:14:34,173] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO ResourceUtils: ==============================================================
[2024-09-03 18:14:34,173] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:14:34,193] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:14:34,206] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:14:34,209] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:14:34,250] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:14:34,251] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:14:34,251] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:14:34,251] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:14:34,252] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:14:34,420] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO Utils: Successfully started service 'sparkDriver' on port 43953.
[2024-09-03 18:14:34,443] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:14:34,472] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:14:34,489] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:14:34,490] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:14:34,494] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:14:34,505] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e53da38-b7a1-44d3-879d-f5fc4030a09b
[2024-09-03 18:14:34,525] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:14:34,540] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:14:34,739] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:14:34,785] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:14:34,967] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:14:34,992] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44391.
[2024-09-03 18:14:34,993] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO NettyBlockTransferService: Server created on 192.168.2.128:44391
[2024-09-03 18:14:34,995] {spark_submit.py:495} INFO - 24/09/03 18:14:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:14:35,001] {spark_submit.py:495} INFO - 24/09/03 18:14:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 44391, None)
[2024-09-03 18:14:35,004] {spark_submit.py:495} INFO - 24/09/03 18:14:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:44391 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 44391, None)
[2024-09-03 18:14:35,007] {spark_submit.py:495} INFO - 24/09/03 18:14:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 44391, None)
[2024-09-03 18:14:35,009] {spark_submit.py:495} INFO - 24/09/03 18:14:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 44391, None)
[2024-09-03 18:14:35,498] {spark_submit.py:495} INFO - 24/09/03 18:14:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:14:35,499] {spark_submit.py:495} INFO - 24/09/03 18:14:35 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:14:36,295] {spark_submit.py:495} INFO - 24/09/03 18:14:36 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 18:14:36,352] {spark_submit.py:495} INFO - 24/09/03 18:14:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:14:37,959] {spark_submit.py:495} INFO - 24/09/03 18:14:37 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:14:37,960] {spark_submit.py:495} INFO - 24/09/03 18:14:37 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:14:37,964] {spark_submit.py:495} INFO - 24/09/03 18:14:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:14:38,233] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:14:38,278] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:14:38,281] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:44391 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:14:38,288] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:38,296] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650034 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:38,435] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:38,452] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:38,453] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:38,453] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:38,454] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:38,484] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:38,570] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:14:38,574] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:14:38,575] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:44391 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:14:38,576] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:38,585] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:38,586] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:14:38,634] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:38,653] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:14:38,848] {spark_submit.py:495} INFO - 24/09/03 18:14:38 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455730, partition values: [empty row]
[2024-09-03 18:14:39,117] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO CodeGenerator: Code generated in 137.780008 ms
[2024-09-03 18:14:39,199] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 18:14:39,216] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 593 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:39,327] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:14:39,339] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,833 s
[2024-09-03 18:14:39,339] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:39,339] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:14:39,340] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,903162 s
[2024-09-03 18:14:39,750] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:14:39,751] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:14:39,752] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:14:39,823] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:39,824] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:39,824] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:39,894] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO CodeGenerator: Code generated in 26.571004 ms
[2024-09-03 18:14:39,940] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO CodeGenerator: Code generated in 31.870514 ms
[2024-09-03 18:14:39,947] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:14:39,956] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:14:39,956] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:44391 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:14:39,958] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:39,962] {spark_submit.py:495} INFO - 24/09/03 18:14:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650034 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:40,023] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:40,024] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:40,025] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:40,025] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:40,025] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:40,025] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:40,078] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:14:40,081] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:14:40,082] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:44391 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:14:40,083] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:40,084] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:40,084] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:14:40,088] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:40,088] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:14:40,138] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:40,138] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:40,139] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:40,198] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO CodeGenerator: Code generated in 19.311159 ms
[2024-09-03 18:14:40,202] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455730, partition values: [empty row]
[2024-09-03 18:14:40,231] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO CodeGenerator: Code generated in 25.386515 ms
[2024-09-03 18:14:40,256] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO CodeGenerator: Code generated in 4.904477 ms
[2024-09-03 18:14:40,417] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: Saved output of task 'attempt_202409031814393014663588679430661_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409031814393014663588679430661_0001_m_000000
[2024-09-03 18:14:40,417] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkHadoopMapRedUtil: attempt_202409031814393014663588679430661_0001_m_000000_1: Committed
[2024-09-03 18:14:40,425] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:14:40,431] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 345 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:40,432] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,404 s
[2024-09-03 18:14:40,432] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:14:40,437] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:40,437] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:14:40,438] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,415222 s
[2024-09-03 18:14:40,466] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileFormatWriter: Write Job 1952c4e6-e25d-41d3-9a69-4eafa6304b14 committed.
[2024-09-03 18:14:40,471] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileFormatWriter: Finished processing stats for write job 1952c4e6-e25d-41d3-9a69-4eafa6304b14.
[2024-09-03 18:14:40,523] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:14:40,524] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:14:40,524] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:14:40,536] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:40,536] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:40,537] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:40,570] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO CodeGenerator: Code generated in 14.202844 ms
[2024-09-03 18:14:40,574] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:14:40,583] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:14:40,586] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:44391 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:14:40,587] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:40,589] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650034 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:40,605] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:40,607] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:40,607] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:40,608] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:40,611] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:40,611] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:40,628] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:14:40,630] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:14:40,630] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:44391 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:14:40,631] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:40,632] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:40,632] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:14:40,635] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:40,635] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:14:40,648] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:40,648] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:40,648] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:40,695] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO CodeGenerator: Code generated in 10.870527 ms
[2024-09-03 18:14:40,698] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-455730, partition values: [empty row]
[2024-09-03 18:14:40,715] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO CodeGenerator: Code generated in 13.599246 ms
[2024-09-03 18:14:40,754] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileOutputCommitter: Saved output of task 'attempt_202409031814408025526592434758430_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_202409031814408025526592434758430_0002_m_000000
[2024-09-03 18:14:40,755] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkHadoopMapRedUtil: attempt_202409031814408025526592434758430_0002_m_000000_2: Committed
[2024-09-03 18:14:40,757] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:14:40,759] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 127 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:40,759] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:14:40,762] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,151 s
[2024-09-03 18:14:40,763] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:40,763] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:14:40,763] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,156704 s
[2024-09-03 18:14:40,785] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileFormatWriter: Write Job 95654042-79c9-4626-aa1b-4b4296e90fe0 committed.
[2024-09-03 18:14:40,785] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO FileFormatWriter: Finished processing stats for write job 95654042-79c9-4626-aa1b-4b4296e90fe0.
[2024-09-03 18:14:40,822] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:14:40,831] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:14:40,843] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:14:40,872] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:14:40,874] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO BlockManager: BlockManager stopped
[2024-09-03 18:14:40,883] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:14:40,886] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:14:40,891] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:14:40,892] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:14:40,893] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-ad116f8e-0ec3-494a-a34b-e2404ba2858f
[2024-09-03 18:14:40,897] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-ad116f8e-0ec3-494a-a34b-e2404ba2858f/pyspark-453f8614-c288-4679-a2e4-2ce60cef213d
[2024-09-03 18:14:40,901] {spark_submit.py:495} INFO - 24/09/03 18:14:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-bed2bce6-e0af-4581-96f9-e74bb4924e88
[2024-09-03 18:14:40,965] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T211432, end_date=20240903T211440
[2024-09-03 18:14:40,981] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:14:40,994] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:20:14,973] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 18:20:14,980] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 18:20:14,981] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:20:14,981] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:20:14,981] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:20:14,992] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 18:20:14,994] {standard_task_runner.py:52} INFO - Started process 341956 to run task
[2024-09-03 18:20:14,997] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpwy5479kg', '--error-file', '/tmp/tmpw1rqdmt6']
[2024-09-03 18:20:14,997] {standard_task_runner.py:80} INFO - Job 81: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:20:15,028] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:20:15,071] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 18:20:15,075] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:20:15,076] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 18:20:16,113] {spark_submit.py:495} INFO - 24/09/03 18:20:16 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:20:16,113] {spark_submit.py:495} INFO - 24/09/03 18:20:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:20:16,411] {spark_submit.py:495} INFO - 24/09/03 18:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:20:16,924] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:20:16,939] {spark_submit.py:495} INFO - 24/09/03 18:20:16 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:20:16,977] {spark_submit.py:495} INFO - 24/09/03 18:20:16 INFO ResourceUtils: ==============================================================
[2024-09-03 18:20:16,978] {spark_submit.py:495} INFO - 24/09/03 18:20:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:20:16,979] {spark_submit.py:495} INFO - 24/09/03 18:20:16 INFO ResourceUtils: ==============================================================
[2024-09-03 18:20:16,979] {spark_submit.py:495} INFO - 24/09/03 18:20:16 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:20:17,003] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:20:17,018] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:20:17,018] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:20:17,056] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:20:17,057] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:20:17,057] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:20:17,057] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:20:17,058] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:20:17,203] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO Utils: Successfully started service 'sparkDriver' on port 33329.
[2024-09-03 18:20:17,224] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:20:17,249] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:20:17,264] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:20:17,265] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:20:17,268] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:20:17,280] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-99db894c-94e8-48cb-8020-6051c61a507d
[2024-09-03 18:20:17,298] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:20:17,313] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:20:17,497] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:20:17,546] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:20:17,754] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:20:17,777] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36199.
[2024-09-03 18:20:17,777] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO NettyBlockTransferService: Server created on 192.168.2.128:36199
[2024-09-03 18:20:17,778] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:20:17,783] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36199, None)
[2024-09-03 18:20:17,788] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36199 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36199, None)
[2024-09-03 18:20:17,789] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36199, None)
[2024-09-03 18:20:17,796] {spark_submit.py:495} INFO - 24/09/03 18:20:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36199, None)
[2024-09-03 18:20:18,243] {spark_submit.py:495} INFO - 24/09/03 18:20:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:20:18,244] {spark_submit.py:495} INFO - 24/09/03 18:20:18 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:20:18,997] {spark_submit.py:495} INFO - 24/09/03 18:20:18 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2024-09-03 18:20:19,059] {spark_submit.py:495} INFO - 24/09/03 18:20:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:20:20,729] {spark_submit.py:495} INFO - 24/09/03 18:20:20 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:20:20,730] {spark_submit.py:495} INFO - 24/09/03 18:20:20 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:20:20,732] {spark_submit.py:495} INFO - 24/09/03 18:20:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:20:20,973] {spark_submit.py:495} INFO - 24/09/03 18:20:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:20:21,019] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:20:21,022] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36199 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:20:21,032] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:21,040] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198756 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:20:21,175] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:21,191] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:20:21,191] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:20:21,191] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:20:21,192] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:20:21,197] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:20:21,274] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:20:21,277] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:20:21,278] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36199 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:20:21,278] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:20:21,288] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:20:21,289] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:20:21,336] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:20:21,350] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:20:21,653] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4452, partition values: [empty row]
[2024-09-03 18:20:21,886] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO CodeGenerator: Code generated in 133.893936 ms
[2024-09-03 18:20:21,934] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:20:21,948] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 623 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:20:21,949] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:20:21,954] {spark_submit.py:495} INFO - 24/09/03 18:20:21 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,743 s
[2024-09-03 18:20:22,065] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:20:22,066] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:20:22,070] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,892570 s
[2024-09-03 18:20:22,467] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:20:22,468] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:20:22,468] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:20:22,549] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:22,549] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:22,550] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:22,631] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO CodeGenerator: Code generated in 33.447281 ms
[2024-09-03 18:20:22,670] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO CodeGenerator: Code generated in 23.733385 ms
[2024-09-03 18:20:22,676] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:20:22,683] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:20:22,684] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36199 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:20:22,685] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:22,688] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198756 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:20:22,744] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:22,745] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:20:22,746] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:20:22,746] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:20:22,746] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:20:22,747] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:20:22,787] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:20:22,789] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:20:22,790] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36199 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:20:22,791] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:20:22,792] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:20:22,793] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:20:22,796] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:20:22,797] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:20:22,849] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:22,849] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:22,850] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:22,933] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO CodeGenerator: Code generated in 28.249542 ms
[2024-09-03 18:20:22,937] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4452, partition values: [empty row]
[2024-09-03 18:20:22,961] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO CodeGenerator: Code generated in 20.262318 ms
[2024-09-03 18:20:22,984] {spark_submit.py:495} INFO - 24/09/03 18:20:22 INFO CodeGenerator: Code generated in 4.655123 ms
[2024-09-03 18:20:23,026] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileOutputCommitter: Saved output of task 'attempt_202409031820222260457277297249201_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409031820222260457277297249201_0001_m_000000
[2024-09-03 18:20:23,027] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkHadoopMapRedUtil: attempt_202409031820222260457277297249201_0001_m_000000_1: Committed
[2024-09-03 18:20:23,032] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:20:23,057] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 259 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:20:23,057] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,308 s
[2024-09-03 18:20:23,058] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:20:23,060] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:20:23,061] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:20:23,062] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,314904 s
[2024-09-03 18:20:23,090] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileFormatWriter: Write Job 110f039b-0905-471b-ae0e-1361f33e2876 committed.
[2024-09-03 18:20:23,098] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileFormatWriter: Finished processing stats for write job 110f039b-0905-471b-ae0e-1361f33e2876.
[2024-09-03 18:20:23,181] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:20:23,181] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:20:23,181] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:20:23,212] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:23,212] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:23,213] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:23,258] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO CodeGenerator: Code generated in 15.154342 ms
[2024-09-03 18:20:23,262] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:20:23,270] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:20:23,271] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36199 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:20:23,272] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:23,274] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198756 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:20:23,289] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:20:23,290] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:20:23,291] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:20:23,291] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:20:23,291] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:20:23,297] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:20:23,312] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:20:23,314] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:20:23,315] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36199 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:20:23,316] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:20:23,317] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:20:23,317] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:20:23,319] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:20:23,320] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:20:23,345] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:20:23,345] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:20:23,345] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:20:23,389] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO CodeGenerator: Code generated in 12.779725 ms
[2024-09-03 18:20:23,392] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4452, partition values: [empty row]
[2024-09-03 18:20:23,406] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO CodeGenerator: Code generated in 10.356312 ms
[2024-09-03 18:20:23,417] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileOutputCommitter: Saved output of task 'attempt_202409031820236431074155919662483_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_202409031820236431074155919662483_0002_m_000000
[2024-09-03 18:20:23,417] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkHadoopMapRedUtil: attempt_202409031820236431074155919662483_0002_m_000000_2: Committed
[2024-09-03 18:20:23,419] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:20:23,425] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 106 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:20:23,426] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:20:23,427] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,127 s
[2024-09-03 18:20:23,427] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:20:23,427] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:20:23,427] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,137728 s
[2024-09-03 18:20:23,448] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileFormatWriter: Write Job d428fcca-9e3a-4842-946b-f4a8b8947d9b committed.
[2024-09-03 18:20:23,448] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO FileFormatWriter: Finished processing stats for write job d428fcca-9e3a-4842-946b-f4a8b8947d9b.
[2024-09-03 18:20:23,484] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:20:23,503] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:20:23,514] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:20:23,521] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:20:23,522] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO BlockManager: BlockManager stopped
[2024-09-03 18:20:23,529] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:20:23,531] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:20:23,539] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:20:23,539] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:20:23,540] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-229764c8-971c-4ace-86ff-efe4e8758b7b
[2024-09-03 18:20:23,543] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-229764c8-971c-4ace-86ff-efe4e8758b7b/pyspark-ec90170a-054f-4233-8c7b-3b8efad3c8e3
[2024-09-03 18:20:23,546] {spark_submit.py:495} INFO - 24/09/03 18:20:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-70cf80cd-8b11-4999-9b35-52081d7d6288
[2024-09-03 18:20:23,616] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240903T212014, end_date=20240903T212023
[2024-09-03 18:20:23,656] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:20:23,684] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:41:49,244] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 21:41:49,250] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 21:41:49,250] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:41:49,250] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:41:49,250] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:41:49,260] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 21:41:49,262] {standard_task_runner.py:52} INFO - Started process 415920 to run task
[2024-09-03 21:41:49,266] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1yoihoor', '--error-file', '/tmp/tmpxmqx23z7']
[2024-09-03 21:41:49,266] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:41:49,304] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:41:49,345] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 21:41:49,348] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:41:49,349] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 21:41:50,375] {spark_submit.py:495} INFO - 24/09/03 21:41:50 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:41:50,375] {spark_submit.py:495} INFO - 24/09/03 21:41:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:41:50,709] {spark_submit.py:495} INFO - 24/09/03 21:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:41:51,243] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:41:51,254] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:41:51,293] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO ResourceUtils: ==============================================================
[2024-09-03 21:41:51,293] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:41:51,294] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO ResourceUtils: ==============================================================
[2024-09-03 21:41:51,294] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:41:51,311] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:41:51,324] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:41:51,325] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:41:51,374] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:41:51,374] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:41:51,375] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:41:51,375] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:41:51,375] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:41:51,528] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO Utils: Successfully started service 'sparkDriver' on port 41859.
[2024-09-03 21:41:51,552] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:41:51,577] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:41:51,593] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:41:51,593] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:41:51,597] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:41:51,608] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e0a665bd-f86f-49ab-ba8a-0c3f6f0fd5b3
[2024-09-03 21:41:51,639] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:41:51,652] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:41:51,834] {spark_submit.py:495} INFO - 24/09/03 21:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:41:51,841] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:41:51,893] {spark_submit.py:495} INFO - 24/09/03 21:41:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:41:52,100] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:41:52,122] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46815.
[2024-09-03 21:41:52,123] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO NettyBlockTransferService: Server created on 192.168.2.128:46815
[2024-09-03 21:41:52,124] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:41:52,128] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46815, None)
[2024-09-03 21:41:52,132] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46815 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46815, None)
[2024-09-03 21:41:52,133] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46815, None)
[2024-09-03 21:41:52,133] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46815, None)
[2024-09-03 21:41:52,737] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:41:52,740] {spark_submit.py:495} INFO - 24/09/03 21:41:52 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:41:53,539] {spark_submit.py:495} INFO - 24/09/03 21:41:53 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 21:41:53,593] {spark_submit.py:495} INFO - 24/09/03 21:41:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:41:55,253] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:41:55,254] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:41:55,256] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:41:55,535] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:41:55,576] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:41:55,579] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46815 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:41:55,585] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:55,592] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:41:55,736] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:55,758] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:41:55,759] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:41:55,759] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:41:55,759] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:41:55,763] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:41:55,875] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:41:55,878] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:41:55,879] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46815 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:41:55,880] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:41:55,893] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:41:55,898] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:41:55,957] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:41:55,973] {spark_submit.py:495} INFO - 24/09/03 21:41:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:41:56,265] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4589, partition values: [empty row]
[2024-09-03 21:41:56,504] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO CodeGenerator: Code generated in 142.670177 ms
[2024-09-03 21:41:56,546] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 21:41:56,558] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 617 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:41:56,652] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:41:56,662] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,873 s
[2024-09-03 21:41:56,667] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:41:56,668] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:41:56,672] {spark_submit.py:495} INFO - 24/09/03 21:41:56 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,936702 s
[2024-09-03 21:41:57,082] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:41:57,083] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:41:57,083] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:41:57,157] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:57,158] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:57,158] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:57,245] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 27.451456 ms
[2024-09-03 21:41:57,286] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 25.371325 ms
[2024-09-03 21:41:57,293] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:41:57,300] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:41:57,301] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46815 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:41:57,302] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:57,304] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:41:57,360] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:57,362] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:41:57,362] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:41:57,363] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:41:57,363] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:41:57,363] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:41:57,415] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 21:41:57,419] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:41:57,421] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46815 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:41:57,422] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:41:57,424] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:41:57,424] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:41:57,429] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:41:57,431] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:41:57,480] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:57,481] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:57,482] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:57,550] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 24.845535 ms
[2024-09-03 21:41:57,556] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4589, partition values: [empty row]
[2024-09-03 21:41:57,576] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 17.122534 ms
[2024-09-03 21:41:57,600] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 5.446582 ms
[2024-09-03 21:41:57,641] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: Saved output of task 'attempt_202409032141577472408445766820825_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409032141577472408445766820825_0001_m_000000
[2024-09-03 21:41:57,642] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkHadoopMapRedUtil: attempt_202409032141577472408445766820825_0001_m_000000_1: Committed
[2024-09-03 21:41:57,646] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:41:57,652] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 225 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:41:57,653] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:41:57,655] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,289 s
[2024-09-03 21:41:57,655] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:41:57,655] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:41:57,656] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,295640 s
[2024-09-03 21:41:57,680] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileFormatWriter: Write Job c7c4d8a8-14d7-4c59-8718-3212a0c99ba3 committed.
[2024-09-03 21:41:57,683] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileFormatWriter: Finished processing stats for write job c7c4d8a8-14d7-4c59-8718-3212a0c99ba3.
[2024-09-03 21:41:57,721] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:41:57,721] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:41:57,721] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:41:57,732] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:57,733] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:57,733] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:57,762] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 9.898012 ms
[2024-09-03 21:41:57,766] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:41:57,777] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:41:57,778] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46815 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:41:57,778] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:57,780] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:41:57,795] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:57,797] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:41:57,797] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:41:57,797] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:41:57,797] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:41:57,801] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:41:57,816] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:41:57,818] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:41:57,820] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46815 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:41:57,821] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:41:57,822] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:41:57,822] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:41:57,824] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:41:57,827] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:41:57,837] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:57,837] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:57,838] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:57,872] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 13.585157 ms
[2024-09-03 21:41:57,874] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4589, partition values: [empty row]
[2024-09-03 21:41:57,889] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO CodeGenerator: Code generated in 12.144687 ms
[2024-09-03 21:41:57,900] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileOutputCommitter: Saved output of task 'attempt_20240903214157524446220877369860_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_20240903214157524446220877369860_0002_m_000000
[2024-09-03 21:41:57,901] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkHadoopMapRedUtil: attempt_20240903214157524446220877369860_0002_m_000000_2: Committed
[2024-09-03 21:41:57,904] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:41:57,908] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 84 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:41:57,908] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:41:57,909] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,106 s
[2024-09-03 21:41:57,909] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:41:57,909] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:41:57,909] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,112819 s
[2024-09-03 21:41:57,931] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileFormatWriter: Write Job c26f6b78-43e7-4b68-b73d-a075847a439a committed.
[2024-09-03 21:41:57,932] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO FileFormatWriter: Finished processing stats for write job c26f6b78-43e7-4b68-b73d-a075847a439a.
[2024-09-03 21:41:57,964] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:41:57,973] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:41:57,987] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:41:57,995] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:41:57,995] {spark_submit.py:495} INFO - 24/09/03 21:41:57 INFO BlockManager: BlockManager stopped
[2024-09-03 21:41:58,002] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:41:58,006] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:41:58,010] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:41:58,011] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:41:58,011] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-631aa55c-bc01-4505-9c69-cf3caa47f65b
[2024-09-03 21:41:58,020] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-631aa55c-bc01-4505-9c69-cf3caa47f65b/pyspark-21083338-27e3-4442-a490-da42fe97464c
[2024-09-03 21:41:58,022] {spark_submit.py:495} INFO - 24/09/03 21:41:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-dca1b546-c3d4-4b7e-a875-2e124e954142
[2024-09-03 21:41:58,086] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240904T004149, end_date=20240904T004158
[2024-09-03 21:41:58,113] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:41:58,157] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:49:58,034] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 22:49:58,040] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [queued]>
[2024-09-03 22:49:58,040] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:49:58,040] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:49:58,040] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:49:58,052] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-21 00:00:00+00:00
[2024-09-03 22:49:58,055] {standard_task_runner.py:52} INFO - Started process 448516 to run task
[2024-09-03 22:49:58,058] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-21T00:00:00+00:00', '--job-id', '125', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmprs1i899w', '--error-file', '/tmp/tmpso2y1g4s']
[2024-09-03 22:49:58,059] {standard_task_runner.py:80} INFO - Job 125: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:49:58,108] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-21T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:49:58,155] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-21T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-21T00:00:00+00:00
[2024-09-03 22:49:58,160] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:49:58,162] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240821
[2024-09-03 22:49:59,227] {spark_submit.py:495} INFO - 24/09/03 22:49:59 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:49:59,227] {spark_submit.py:495} INFO - 24/09/03 22:49:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:49:59,561] {spark_submit.py:495} INFO - 24/09/03 22:49:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:50:00,108] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:50:00,124] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:50:00,163] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO ResourceUtils: ==============================================================
[2024-09-03 22:50:00,164] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:50:00,164] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO ResourceUtils: ==============================================================
[2024-09-03 22:50:00,164] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:50:00,183] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:50:00,195] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:50:00,196] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:50:00,237] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:50:00,237] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:50:00,238] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:50:00,238] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:50:00,238] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:50:00,395] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO Utils: Successfully started service 'sparkDriver' on port 41901.
[2024-09-03 22:50:00,419] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:50:00,449] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:50:00,465] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:50:00,466] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:50:00,469] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:50:00,481] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5d37247f-b403-4b44-b65c-2b1662e29ee5
[2024-09-03 22:50:00,501] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:50:00,517] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:50:00,713] {spark_submit.py:495} INFO - 24/09/03 22:50:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:50:00,719] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:50:00,777] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:50:00,958] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:50:00,985] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44997.
[2024-09-03 22:50:00,985] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO NettyBlockTransferService: Server created on 192.168.2.128:44997
[2024-09-03 22:50:00,986] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:50:00,992] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 44997, None)
[2024-09-03 22:50:00,995] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:44997 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 44997, None)
[2024-09-03 22:50:00,997] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 44997, None)
[2024-09-03 22:50:00,998] {spark_submit.py:495} INFO - 24/09/03 22:50:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 44997, None)
[2024-09-03 22:50:01,488] {spark_submit.py:495} INFO - 24/09/03 22:50:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:50:01,489] {spark_submit.py:495} INFO - 24/09/03 22:50:01 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:50:02,281] {spark_submit.py:495} INFO - 24/09/03 22:50:02 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 22:50:02,334] {spark_submit.py:495} INFO - 24/09/03 22:50:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:50:04,156] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:50:04,157] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:50:04,157] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:50:04,430] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:50:04,478] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:50:04,481] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:44997 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:50:04,487] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:04,495] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198950 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:50:04,693] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:04,708] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:50:04,709] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:50:04,709] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:50:04,711] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:50:04,722] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:50:04,809] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:50:04,812] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:50:04,813] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:44997 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:50:04,814] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:50:04,825] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:50:04,827] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:50:04,880] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:50:04,900] {spark_submit.py:495} INFO - 24/09/03 22:50:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:50:05,120] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4646, partition values: [empty row]
[2024-09-03 22:50:05,356] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO CodeGenerator: Code generated in 141.555277 ms
[2024-09-03 22:50:05,405] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 22:50:05,414] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 547 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:50:05,417] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:50:05,550] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,812 s
[2024-09-03 22:50:05,554] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:50:05,554] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:50:05,567] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,868898 s
[2024-09-03 22:50:05,984] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:50:05,985] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:50:05,985] {spark_submit.py:495} INFO - 24/09/03 22:50:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:50:06,073] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:06,073] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:06,074] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:06,171] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 35.998048 ms
[2024-09-03 22:50:06,209] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 23.828127 ms
[2024-09-03 22:50:06,215] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:50:06,225] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:50:06,227] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:44997 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:50:06,227] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:06,230] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198950 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:50:06,288] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:06,289] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:50:06,290] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:50:06,290] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:50:06,290] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:50:06,291] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:50:06,333] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:50:06,337] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:50:06,338] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:44997 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:50:06,338] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:50:06,338] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:50:06,338] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:50:06,344] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:50:06,344] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:50:06,404] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:06,405] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:06,405] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:06,475] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 23.176065 ms
[2024-09-03 22:50:06,477] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4646, partition values: [empty row]
[2024-09-03 22:50:06,503] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 21.468546 ms
[2024-09-03 22:50:06,533] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 7.479845 ms
[2024-09-03 22:50:06,569] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: Saved output of task 'attempt_202409032250065888683321601148603_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240821/_temporary/0/task_202409032250065888683321601148603_0001_m_000000
[2024-09-03 22:50:06,569] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkHadoopMapRedUtil: attempt_202409032250065888683321601148603_0001_m_000000_1: Committed
[2024-09-03 22:50:06,573] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:50:06,582] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 243 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:50:06,583] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:50:06,585] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,292 s
[2024-09-03 22:50:06,585] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:50:06,586] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:50:06,586] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,297913 s
[2024-09-03 22:50:06,609] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileFormatWriter: Write Job 1f4c8c3d-80ac-46d9-b2a6-720148c3d54a committed.
[2024-09-03 22:50:06,616] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileFormatWriter: Finished processing stats for write job 1f4c8c3d-80ac-46d9-b2a6-720148c3d54a.
[2024-09-03 22:50:06,697] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:50:06,698] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:50:06,698] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:50:06,737] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:06,737] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:06,737] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:06,805] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 15.020909 ms
[2024-09-03 22:50:06,813] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:50:06,822] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:50:06,824] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:44997 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:50:06,825] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:06,826] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198950 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:50:06,844] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:50:06,846] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:50:06,846] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:50:06,846] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:50:06,846] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:50:06,850] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:50:06,870] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:50:06,872] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:50:06,874] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:44997 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:50:06,874] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:50:06,875] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:50:06,875] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:50:06,877] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:50:06,879] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:50:06,892] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:50:06,892] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:50:06,893] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:50:06,923] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 11.854374 ms
[2024-09-03 22:50:06,927] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-21T00:00:00.00Z/data_engineer_20240821.json, range: 0-4646, partition values: [empty row]
[2024-09-03 22:50:06,942] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO CodeGenerator: Code generated in 12.080475 ms
[2024-09-03 22:50:06,955] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileOutputCommitter: Saved output of task 'attempt_2024090322500690987508200226321_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240821/_temporary/0/task_2024090322500690987508200226321_0002_m_000000
[2024-09-03 22:50:06,956] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO SparkHadoopMapRedUtil: attempt_2024090322500690987508200226321_0002_m_000000_2: Committed
[2024-09-03 22:50:06,958] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:50:06,963] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 86 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:50:06,963] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:50:06,965] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,114 s
[2024-09-03 22:50:06,965] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:50:06,965] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:50:06,968] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,120687 s
[2024-09-03 22:50:06,991] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileFormatWriter: Write Job 68493ed8-e45c-469b-b927-c968a02c1847 committed.
[2024-09-03 22:50:06,991] {spark_submit.py:495} INFO - 24/09/03 22:50:06 INFO FileFormatWriter: Finished processing stats for write job 68493ed8-e45c-469b-b927-c968a02c1847.
[2024-09-03 22:50:07,023] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:50:07,030] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:50:07,044] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:50:07,051] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:50:07,052] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO BlockManager: BlockManager stopped
[2024-09-03 22:50:07,070] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:50:07,072] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:50:07,077] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:50:07,079] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:50:07,079] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf34a7d8-2fd4-4eaf-bb4c-1af7f1befb0a/pyspark-46c53a25-83ef-4e6c-9952-d3ab331055f3
[2024-09-03 22:50:07,081] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-18a2fb56-66c8-436a-851e-b56bf2584909
[2024-09-03 22:50:07,083] {spark_submit.py:495} INFO - 24/09/03 22:50:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf34a7d8-2fd4-4eaf-bb4c-1af7f1befb0a
[2024-09-03 22:50:07,154] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240821T000000, start_date=20240904T014958, end_date=20240904T015007
[2024-09-03 22:50:07,186] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:50:07,204] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
