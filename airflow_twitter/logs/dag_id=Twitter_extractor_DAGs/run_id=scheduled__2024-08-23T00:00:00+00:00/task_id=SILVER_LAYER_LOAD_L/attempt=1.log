[2024-09-03 16:18:32,037] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 16:18:32,049] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 16:18:32,049] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:18:32,049] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:18:32,049] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:18:32,061] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 16:18:32,069] {standard_task_runner.py:52} INFO - Started process 280867 to run task
[2024-09-03 16:18:32,073] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpofh4596v', '--error-file', '/tmp/tmphfpf0ld3']
[2024-09-03 16:18:32,073] {standard_task_runner.py:80} INFO - Job 84: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:18:32,121] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:18:32,173] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 16:18:32,177] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:18:32,178] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-23T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240823
[2024-09-03 16:18:33,238] {spark_submit.py:495} INFO - 24/09/03 16:18:33 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:18:33,239] {spark_submit.py:495} INFO - 24/09/03 16:18:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:18:33,645] {spark_submit.py:495} INFO - 24/09/03 16:18:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:18:34,301] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:18:34,315] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:18:34,373] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO ResourceUtils: ==============================================================
[2024-09-03 16:18:34,374] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:18:34,376] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO ResourceUtils: ==============================================================
[2024-09-03 16:18:34,376] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:18:34,410] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:18:34,427] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:18:34,429] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:18:34,485] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:18:34,485] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:18:34,485] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:18:34,486] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:18:34,486] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:18:34,737] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO Utils: Successfully started service 'sparkDriver' on port 40197.
[2024-09-03 16:18:34,769] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:18:34,804] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:18:34,826] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:18:34,827] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:18:34,831] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:18:34,842] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-742e3bb1-5f5b-4c57-b313-f410cbf1c210
[2024-09-03 16:18:34,862] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:18:34,880] {spark_submit.py:495} INFO - 24/09/03 16:18:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:18:35,119] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:18:35,197] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:18:35,457] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:18:35,497] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42155.
[2024-09-03 16:18:35,498] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO NettyBlockTransferService: Server created on 192.168.2.128:42155
[2024-09-03 16:18:35,500] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:18:35,506] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 42155, None)
[2024-09-03 16:18:35,511] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:42155 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 42155, None)
[2024-09-03 16:18:35,514] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 42155, None)
[2024-09-03 16:18:35,516] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 42155, None)
[2024-09-03 16:18:35,987] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:18:35,987] {spark_submit.py:495} INFO - 24/09/03 16:18:35 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:18:37,149] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2024-09-03 16:18:37,149] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 76, in <module>
[2024-09-03 16:18:37,150] {spark_submit.py:495} INFO - define_extration(spark,
[2024-09-03 16:18:37,150] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 48, in define_extration
[2024-09-03 16:18:37,151] {spark_submit.py:495} INFO - df = spark.read.json(src)
[2024-09-03 16:18:37,151] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 372, in json
[2024-09-03 16:18:37,151] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2024-09-03 16:18:37,152] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2024-09-03 16:18:37,157] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-23T00:00:00.00Z
[2024-09-03 16:18:37,189] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:18:37,201] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:18:37,212] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:18:37,223] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:18:37,224] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO BlockManager: BlockManager stopped
[2024-09-03 16:18:37,234] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:18:37,238] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:18:37,244] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:18:37,245] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:18:37,247] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-8ad2b1de-9ece-4beb-ad5d-1df51c53182c
[2024-09-03 16:18:37,250] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4586309-f789-4bd0-8f63-25d69f6592e1
[2024-09-03 16:18:37,253] {spark_submit.py:495} INFO - 24/09/03 16:18:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4586309-f789-4bd0-8f63-25d69f6592e1/pyspark-b5d05e95-f0c9-4577-9a2c-4f1289e59069
[2024-09-03 16:18:37,298] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-23T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240823. Error code is: 1.
[2024-09-03 16:18:37,302] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T191832, end_date=20240903T191837
[2024-09-03 16:18:37,315] {standard_task_runner.py:92} ERROR - Failed to execute job 84 for task SILVER_LAYER_LOAD_L (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-23T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240823. Error code is: 1.; 280867)
[2024-09-03 16:18:37,368] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-09-03 16:18:37,377] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:18:37,386] {dagrun.py:547} ERROR - Marking run <DagRun Twitter_extractor_DAGs @ 2024-08-23 00:00:00+00:00: scheduled__2024-08-23T00:00:00+00:00, externally triggered: False> failed
[2024-09-03 16:18:37,386] {dagrun.py:607} INFO - DagRun Finished: dag_id=Twitter_extractor_DAGs, execution_date=2024-08-23 00:00:00+00:00, run_id=scheduled__2024-08-23T00:00:00+00:00, run_start_date=2024-09-03 19:18:03.466478+00:00, run_end_date=2024-09-03 19:18:37.386317+00:00, run_duration=33.919839, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-23 00:00:00+00:00, data_interval_end=2024-08-24 00:00:00+00:00, dag_hash=fd8e3ef9df6c6572c08a1d1aed58f54d
[2024-09-03 16:22:37,178] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 16:22:37,183] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 16:22:37,184] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:22:37,184] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:22:37,184] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:22:37,195] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 16:22:37,197] {standard_task_runner.py:52} INFO - Started process 283712 to run task
[2024-09-03 16:22:37,199] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpgol3becw', '--error-file', '/tmp/tmp5p93w9u0']
[2024-09-03 16:22:37,200] {standard_task_runner.py:80} INFO - Job 84: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:22:37,239] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:22:37,286] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 16:22:37,290] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:22:37,292] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/ --process-data 20240823
[2024-09-03 16:22:38,316] {spark_submit.py:495} INFO - 24/09/03 16:22:38 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:22:38,317] {spark_submit.py:495} INFO - 24/09/03 16:22:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:22:38,691] {spark_submit.py:495} INFO - 24/09/03 16:22:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:22:39,182] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:22:39,186] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:22:39,228] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO ResourceUtils: ==============================================================
[2024-09-03 16:22:39,228] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:22:39,229] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO ResourceUtils: ==============================================================
[2024-09-03 16:22:39,230] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:22:39,254] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:22:39,271] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:22:39,271] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:22:39,309] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:22:39,310] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:22:39,310] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:22:39,311] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:22:39,311] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:22:39,469] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO Utils: Successfully started service 'sparkDriver' on port 44237.
[2024-09-03 16:22:39,491] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:22:39,514] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:22:39,527] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:22:39,528] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:22:39,531] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:22:39,543] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-affc8d96-57d5-454c-b4b0-53ca7b771efb
[2024-09-03 16:22:39,560] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:22:39,573] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:22:39,786] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:22:39,837] {spark_submit.py:495} INFO - 24/09/03 16:22:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:22:40,038] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:22:40,060] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33675.
[2024-09-03 16:22:40,060] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO NettyBlockTransferService: Server created on 192.168.2.128:33675
[2024-09-03 16:22:40,062] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:22:40,066] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 33675, None)
[2024-09-03 16:22:40,070] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:33675 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 33675, None)
[2024-09-03 16:22:40,074] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 33675, None)
[2024-09-03 16:22:40,075] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 33675, None)
[2024-09-03 16:22:40,621] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:22:40,621] {spark_submit.py:495} INFO - 24/09/03 16:22:40 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:22:41,386] {spark_submit.py:495} INFO - 24/09/03 16:22:41 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.
[2024-09-03 16:22:41,436] {spark_submit.py:495} INFO - 24/09/03 16:22:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:22:43,157] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:22:43,158] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:22:43,162] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:22:43,438] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:22:43,485] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:22:43,488] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:33675 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:22:43,493] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:43,500] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649886 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:43,633] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:43,646] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:43,647] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:43,647] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:43,649] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:43,656] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:43,722] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:22:43,725] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:22:43,726] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:33675 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:22:43,726] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:43,736] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:43,737] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:22:43,776] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4967 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:43,790] {spark_submit.py:495} INFO - 24/09/03 16:22:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:22:44,019] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455582, partition values: [empty row]
[2024-09-03 16:22:44,217] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO CodeGenerator: Code generated in 119.916666 ms
[2024-09-03 16:22:44,302] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 16:22:44,405] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 547 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:44,409] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:22:44,413] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,745 s
[2024-09-03 16:22:44,415] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:44,416] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:22:44,417] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,783445 s
[2024-09-03 16:22:44,774] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:22:44,775] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:22:44,775] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:22:44,848] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:44,848] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:44,848] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:44,988] {spark_submit.py:495} INFO - 24/09/03 16:22:44 INFO CodeGenerator: Code generated in 49.538844 ms
[2024-09-03 16:22:45,037] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 31.690588 ms
[2024-09-03 16:22:45,045] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:22:45,054] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:22:45,056] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:33675 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:22:45,057] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:45,062] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649886 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:45,118] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:45,120] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:45,121] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:45,122] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:45,122] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:45,122] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:45,179] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:22:45,181] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:22:45,182] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:33675 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:22:45,183] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:45,184] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:45,186] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:22:45,194] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:45,195] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:22:45,253] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:45,254] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:45,255] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:45,335] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 25.944985 ms
[2024-09-03 16:22:45,339] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455582, partition values: [empty row]
[2024-09-03 16:22:45,364] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 20.716546 ms
[2024-09-03 16:22:45,389] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 5.634722 ms
[2024-09-03 16:22:45,622] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: Saved output of task 'attempt_202409031622452499822482838575550_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/twitter_df/process_data=20240823/_temporary/0/task_202409031622452499822482838575550_0001_m_000000
[2024-09-03 16:22:45,623] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkHadoopMapRedUtil: attempt_202409031622452499822482838575550_0001_m_000000_1: Committed
[2024-09-03 16:22:45,628] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:22:45,632] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 443 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:45,634] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:22:45,643] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,510 s
[2024-09-03 16:22:45,643] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:45,643] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:22:45,645] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,526554 s
[2024-09-03 16:22:45,691] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileFormatWriter: Write Job 8b1e131c-bd90-447e-bf04-42189ebb7797 committed.
[2024-09-03 16:22:45,703] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileFormatWriter: Finished processing stats for write job 8b1e131c-bd90-447e-bf04-42189ebb7797.
[2024-09-03 16:22:45,764] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:22:45,766] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:22:45,770] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:22:45,791] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:45,792] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:45,794] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:45,835] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 11.978422 ms
[2024-09-03 16:22:45,839] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:22:45,846] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:22:45,847] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:33675 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:22:45,847] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:45,848] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649886 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:22:45,866] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:22:45,866] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:22:45,867] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:22:45,867] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:22:45,867] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:22:45,869] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:22:45,885] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:22:45,887] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.9 KiB, free 364.8 MiB)
[2024-09-03 16:22:45,888] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:33675 (size: 63.9 KiB, free: 366.1 MiB)
[2024-09-03 16:22:45,889] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:22:45,889] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:22:45,889] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:22:45,890] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:22:45,891] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:22:45,909] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:22:45,909] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:22:45,909] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:22:45,933] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 7.692641 ms
[2024-09-03 16:22:45,939] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455582, partition values: [empty row]
[2024-09-03 16:22:45,955] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO CodeGenerator: Code generated in 12.435384 ms
[2024-09-03 16:22:45,985] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO FileOutputCommitter: Saved output of task 'attempt_202409031622453587814658009364532_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/user_df/process_data=20240823/_temporary/0/task_202409031622453587814658009364532_0002_m_000000
[2024-09-03 16:22:45,985] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO SparkHadoopMapRedUtil: attempt_202409031622453587814658009364532_0002_m_000000_2: Committed
[2024-09-03 16:22:45,986] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:22:45,989] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 99 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:22:45,989] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:22:45,990] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,120 s
[2024-09-03 16:22:45,991] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:22:45,992] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:22:45,994] {spark_submit.py:495} INFO - 24/09/03 16:22:45 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,125127 s
[2024-09-03 16:22:46,014] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO FileFormatWriter: Write Job 7ff86edf-c479-4aaf-be91-dcebd1f70158 committed.
[2024-09-03 16:22:46,014] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO FileFormatWriter: Finished processing stats for write job 7ff86edf-c479-4aaf-be91-dcebd1f70158.
[2024-09-03 16:22:46,053] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:22:46,065] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:22:46,077] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:22:46,085] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:22:46,086] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO BlockManager: BlockManager stopped
[2024-09-03 16:22:46,096] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:22:46,099] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:22:46,105] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:22:46,106] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:22:46,106] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-426c5e29-4806-4de7-b095-0956ccd113e2/pyspark-5fbe8c03-1e79-4db2-bcbf-675383754a52
[2024-09-03 16:22:46,110] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-452792da-0769-44ff-a53e-306e2a61fbf1
[2024-09-03 16:22:46,112] {spark_submit.py:495} INFO - 24/09/03 16:22:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-426c5e29-4806-4de7-b095-0956ccd113e2
[2024-09-03 16:22:46,182] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T192237, end_date=20240903T192246
[2024-09-03 16:22:46,235] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:22:46,270] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:40:26,694] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 16:40:26,701] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 16:40:26,702] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:40:26,702] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:40:26,702] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:40:26,711] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 16:40:26,714] {standard_task_runner.py:52} INFO - Started process 294262 to run task
[2024-09-03 16:40:26,718] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '83', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp7y_qao3d', '--error-file', '/tmp/tmpjf2veump']
[2024-09-03 16:40:26,719] {standard_task_runner.py:80} INFO - Job 83: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:40:26,752] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:40:26,789] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 16:40:26,792] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:40:26,793] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 16:40:27,772] {spark_submit.py:495} INFO - 24/09/03 16:40:27 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:40:27,773] {spark_submit.py:495} INFO - 24/09/03 16:40:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:40:28,073] {spark_submit.py:495} INFO - 24/09/03 16:40:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:40:28,570] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:40:28,579] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:40:28,613] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO ResourceUtils: ==============================================================
[2024-09-03 16:40:28,614] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:40:28,614] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO ResourceUtils: ==============================================================
[2024-09-03 16:40:28,615] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:40:28,633] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:40:28,644] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:40:28,645] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:40:28,685] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:40:28,685] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:40:28,686] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:40:28,686] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:40:28,686] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:40:28,834] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO Utils: Successfully started service 'sparkDriver' on port 45031.
[2024-09-03 16:40:28,859] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:40:28,891] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:40:28,906] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:40:28,906] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:40:28,909] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:40:28,921] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-470065c6-b51a-43c2-b62d-039254d37bf4
[2024-09-03 16:40:28,939] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:40:28,954] {spark_submit.py:495} INFO - 24/09/03 16:40:28 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:40:29,122] {spark_submit.py:495} INFO - 24/09/03 16:40:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:40:29,128] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:40:29,187] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:40:29,372] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:40:29,396] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41215.
[2024-09-03 16:40:29,396] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO NettyBlockTransferService: Server created on 192.168.2.128:41215
[2024-09-03 16:40:29,397] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:40:29,405] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 41215, None)
[2024-09-03 16:40:29,412] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:41215 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 41215, None)
[2024-09-03 16:40:29,413] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 41215, None)
[2024-09-03 16:40:29,413] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 41215, None)
[2024-09-03 16:40:29,827] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:40:29,827] {spark_submit.py:495} INFO - 24/09/03 16:40:29 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:40:30,596] {spark_submit.py:495} INFO - 24/09/03 16:40:30 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 16:40:30,645] {spark_submit.py:495} INFO - 24/09/03 16:40:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:40:32,200] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:32,201] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:32,207] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:40:32,438] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:32,486] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:40:32,490] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:41215 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:40:32,494] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:32,503] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198804 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:32,640] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:32,656] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:32,657] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:32,657] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:32,658] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:32,667] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:32,738] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:32,741] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:40:32,742] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:41215 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:40:32,742] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:32,753] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:32,754] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:40:32,800] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:32,814] {spark_submit.py:495} INFO - 24/09/03 16:40:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:40:33,114] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4500, partition values: [empty row]
[2024-09-03 16:40:33,318] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO CodeGenerator: Code generated in 121.959938 ms
[2024-09-03 16:40:33,359] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:40:33,370] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 580 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:33,373] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:40:33,486] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,798 s
[2024-09-03 16:40:33,490] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:33,490] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:40:33,499] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,857587 s
[2024-09-03 16:40:33,857] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:40:33,859] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:40:33,859] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:40:33,923] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:33,923] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:33,923] {spark_submit.py:495} INFO - 24/09/03 16:40:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:34,006] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 30.08684 ms
[2024-09-03 16:40:34,064] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 35.099331 ms
[2024-09-03 16:40:34,071] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:40:34,081] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:40:34,082] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:41215 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:40:34,084] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:34,087] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198804 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:34,138] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:34,140] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:34,140] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:34,140] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:34,141] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:34,145] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:34,187] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:40:34,189] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:40:34,190] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:41215 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:40:34,191] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:34,192] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:34,193] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:40:34,196] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:34,198] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:40:34,249] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:34,250] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:34,251] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:34,318] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 20.502344 ms
[2024-09-03 16:40:34,322] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4500, partition values: [empty row]
[2024-09-03 16:40:34,348] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 23.58473 ms
[2024-09-03 16:40:34,370] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 5.964585 ms
[2024-09-03 16:40:34,411] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640348975330729458950218_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_202409031640348975330729458950218_0001_m_000000
[2024-09-03 16:40:34,412] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkHadoopMapRedUtil: attempt_202409031640348975330729458950218_0001_m_000000_1: Committed
[2024-09-03 16:40:34,418] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:40:34,426] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 232 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:34,426] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:40:34,427] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,285 s
[2024-09-03 16:40:34,427] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:34,427] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:40:34,428] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,289412 s
[2024-09-03 16:40:34,458] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileFormatWriter: Write Job aa23704a-91ad-48a7-9c0a-8f0681ba84a4 committed.
[2024-09-03 16:40:34,461] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileFormatWriter: Finished processing stats for write job aa23704a-91ad-48a7-9c0a-8f0681ba84a4.
[2024-09-03 16:40:34,537] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:34,537] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:34,538] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:40:34,544] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:34,544] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:34,545] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:34,570] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 9.074356 ms
[2024-09-03 16:40:34,573] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:40:34,582] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:40:34,583] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:41215 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:40:34,584] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:34,585] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198804 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:34,600] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:34,601] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:34,601] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:34,601] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:34,602] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:34,603] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:34,621] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:40:34,624] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:40:34,624] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:41215 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:40:34,625] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:34,625] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:34,626] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:40:34,627] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:34,627] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:40:34,638] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:34,638] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:34,639] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:34,680] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 18.198897 ms
[2024-09-03 16:40:34,683] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4500, partition values: [empty row]
[2024-09-03 16:40:34,698] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO CodeGenerator: Code generated in 12.536911 ms
[2024-09-03 16:40:34,712] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640342990648800882998463_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_202409031640342990648800882998463_0002_m_000000
[2024-09-03 16:40:34,713] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkHadoopMapRedUtil: attempt_202409031640342990648800882998463_0002_m_000000_2: Committed
[2024-09-03 16:40:34,715] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:40:34,719] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:34,721] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,116 s
[2024-09-03 16:40:34,723] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:34,725] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:40:34,726] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:40:34,727] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,121286 s
[2024-09-03 16:40:34,759] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileFormatWriter: Write Job b704fbdb-9e5d-4877-9b04-fcbf4a1d46e5 committed.
[2024-09-03 16:40:34,760] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO FileFormatWriter: Finished processing stats for write job b704fbdb-9e5d-4877-9b04-fcbf4a1d46e5.
[2024-09-03 16:40:34,793] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:40:34,801] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:40:34,812] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:40:34,821] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:40:34,821] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO BlockManager: BlockManager stopped
[2024-09-03 16:40:34,827] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:40:34,829] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:40:34,833] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:40:34,833] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:40:34,834] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-7329d6b2-2686-45ad-ae7f-952fb524803a/pyspark-32e919c7-a3ab-4bf3-ac2e-40baed50626b
[2024-09-03 16:40:34,836] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-53d55780-b5d3-4337-a093-a6691ebb4583
[2024-09-03 16:40:34,838] {spark_submit.py:495} INFO - 24/09/03 16:40:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-7329d6b2-2686-45ad-ae7f-952fb524803a
[2024-09-03 16:40:34,902] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T194026, end_date=20240903T194034
[2024-09-03 16:40:34,918] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:40:34,926] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:54:10,407] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 17:54:10,411] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 17:54:10,412] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:54:10,412] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:54:10,412] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:54:10,423] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 17:54:10,425] {standard_task_runner.py:52} INFO - Started process 324305 to run task
[2024-09-03 17:54:10,428] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '86', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmphkxewbul', '--error-file', '/tmp/tmpdjridmf6']
[2024-09-03 17:54:10,428] {standard_task_runner.py:80} INFO - Job 86: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:54:10,459] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:54:10,498] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 17:54:10,501] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:54:10,502] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 17:54:11,491] {spark_submit.py:495} INFO - 24/09/03 17:54:11 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:54:11,492] {spark_submit.py:495} INFO - 24/09/03 17:54:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:54:11,811] {spark_submit.py:495} INFO - 24/09/03 17:54:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:54:12,361] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:54:12,369] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:54:12,412] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO ResourceUtils: ==============================================================
[2024-09-03 17:54:12,412] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:54:12,413] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO ResourceUtils: ==============================================================
[2024-09-03 17:54:12,414] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:54:12,434] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:54:12,448] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:54:12,449] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:54:12,490] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:54:12,490] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:54:12,491] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:54:12,491] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:54:12,491] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:54:12,670] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO Utils: Successfully started service 'sparkDriver' on port 40475.
[2024-09-03 17:54:12,697] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:54:12,728] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:54:12,744] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:54:12,745] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:54:12,749] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:54:12,761] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a41e696a-826c-4a5d-b180-c1e4980e5165
[2024-09-03 17:54:12,783] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:54:12,800] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:54:12,988] {spark_submit.py:495} INFO - 24/09/03 17:54:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:54:13,041] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:54:13,241] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:54:13,270] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44873.
[2024-09-03 17:54:13,270] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO NettyBlockTransferService: Server created on 192.168.2.128:44873
[2024-09-03 17:54:13,272] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:54:13,277] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 44873, None)
[2024-09-03 17:54:13,281] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:44873 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 44873, None)
[2024-09-03 17:54:13,283] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 44873, None)
[2024-09-03 17:54:13,284] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 44873, None)
[2024-09-03 17:54:13,745] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:54:13,747] {spark_submit.py:495} INFO - 24/09/03 17:54:13 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:54:14,546] {spark_submit.py:495} INFO - 24/09/03 17:54:14 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.
[2024-09-03 17:54:14,607] {spark_submit.py:495} INFO - 24/09/03 17:54:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:54:16,219] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:54:16,220] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:54:16,223] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:54:16,459] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:54:16,501] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:54:16,504] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:44873 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:54:16,511] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:16,520] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649079 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:54:16,688] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:16,702] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:54:16,703] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:54:16,703] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:54:16,704] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:54:16,711] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:54:16,804] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:54:16,806] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:54:16,806] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:44873 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:54:16,807] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:54:16,817] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:54:16,818] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:54:16,864] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:54:16,886] {spark_submit.py:495} INFO - 24/09/03 17:54:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:54:17,154] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-454775, partition values: [empty row]
[2024-09-03 17:54:17,371] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO CodeGenerator: Code generated in 130.137597 ms
[2024-09-03 17:54:17,454] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 17:54:17,563] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 611 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:54:17,570] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:54:17,574] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,850 s
[2024-09-03 17:54:17,577] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:54:17,578] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:54:17,581] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,892716 s
[2024-09-03 17:54:17,950] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:54:17,951] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:54:17,951] {spark_submit.py:495} INFO - 24/09/03 17:54:17 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:54:18,024] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:18,024] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:18,025] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:18,100] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 25.506988 ms
[2024-09-03 17:54:18,140] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 26.437201 ms
[2024-09-03 17:54:18,146] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:54:18,153] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:54:18,154] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:44873 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:54:18,155] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:18,157] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649079 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:54:18,211] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:18,213] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:54:18,213] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:54:18,213] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:54:18,214] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:54:18,218] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:54:18,253] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:54:18,255] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:54:18,256] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:44873 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:54:18,256] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:54:18,257] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:54:18,257] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:54:18,261] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:54:18,262] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:54:18,316] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:18,316] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:18,316] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:18,374] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 20.214948 ms
[2024-09-03 17:54:18,378] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-454775, partition values: [empty row]
[2024-09-03 17:54:18,400] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 19.0149 ms
[2024-09-03 17:54:18,425] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 4.698905 ms
[2024-09-03 17:54:18,527] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409031754186655133737714219415_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_202409031754186655133737714219415_0001_m_000000
[2024-09-03 17:54:18,527] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkHadoopMapRedUtil: attempt_202409031754186655133737714219415_0001_m_000000_1: Committed
[2024-09-03 17:54:18,533] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:54:18,541] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 282 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:54:18,542] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,325 s
[2024-09-03 17:54:18,543] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:54:18,545] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:54:18,548] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:54:18,549] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,337134 s
[2024-09-03 17:54:18,565] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileFormatWriter: Write Job e7cac90b-de31-4c8e-a1af-e469d455e6f1 committed.
[2024-09-03 17:54:18,568] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileFormatWriter: Finished processing stats for write job e7cac90b-de31-4c8e-a1af-e469d455e6f1.
[2024-09-03 17:54:18,600] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:54:18,600] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:54:18,600] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:54:18,608] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:18,608] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:18,609] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:18,633] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 10.578322 ms
[2024-09-03 17:54:18,637] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:54:18,643] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:54:18,644] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:44873 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:54:18,645] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:18,646] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649079 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:54:18,660] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:54:18,662] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:54:18,662] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:54:18,663] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:54:18,663] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:54:18,665] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:54:18,681] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:54:18,692] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:54:18,694] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:44873 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:54:18,695] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:54:18,697] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:54:18,697] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:54:18,698] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:54:18,699] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:54:18,709] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:54:18,710] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:54:18,711] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:54:18,736] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 7.956525 ms
[2024-09-03 17:54:18,738] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-454775, partition values: [empty row]
[2024-09-03 17:54:18,761] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO CodeGenerator: Code generated in 13.170209 ms
[2024-09-03 17:54:18,790] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileOutputCommitter: Saved output of task 'attempt_20240903175418317311637104168760_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_20240903175418317311637104168760_0002_m_000000
[2024-09-03 17:54:18,791] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkHadoopMapRedUtil: attempt_20240903175418317311637104168760_0002_m_000000_2: Committed
[2024-09-03 17:54:18,791] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:54:18,792] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 95 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:54:18,795] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:54:18,796] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,126 s
[2024-09-03 17:54:18,799] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:54:18,799] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:54:18,799] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,134804 s
[2024-09-03 17:54:18,830] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileFormatWriter: Write Job 04fe940c-9510-46a7-9601-c78f46f8f84c committed.
[2024-09-03 17:54:18,830] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO FileFormatWriter: Finished processing stats for write job 04fe940c-9510-46a7-9601-c78f46f8f84c.
[2024-09-03 17:54:18,868] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:54:18,878] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:54:18,889] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:54:18,900] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:54:18,901] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO BlockManager: BlockManager stopped
[2024-09-03 17:54:18,908] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:54:18,910] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:54:18,917] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:54:18,918] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:54:18,919] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-24a4c808-6d50-42fa-be97-db4dccdfd331
[2024-09-03 17:54:18,921] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-24a4c808-6d50-42fa-be97-db4dccdfd331/pyspark-1f217fc0-0fd8-43a2-a0ea-e4acb975a85b
[2024-09-03 17:54:18,923] {spark_submit.py:495} INFO - 24/09/03 17:54:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-24c1b8cc-4fa4-4101-bab1-2f0d2b933490
[2024-09-03 17:54:18,987] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T205410, end_date=20240903T205418
[2024-09-03 17:54:19,005] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:54:19,029] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:11:04,789] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 18:11:04,797] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 18:11:04,797] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:11:04,797] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:11:04,797] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:11:04,808] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 18:11:04,811] {standard_task_runner.py:52} INFO - Started process 332841 to run task
[2024-09-03 18:11:04,814] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp505a6vuz', '--error-file', '/tmp/tmpfxedyt41']
[2024-09-03 18:11:04,814] {standard_task_runner.py:80} INFO - Job 87: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:11:04,848] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:11:04,888] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 18:11:04,893] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:11:04,894] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 18:11:06,034] {spark_submit.py:495} INFO - 24/09/03 18:11:06 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:11:06,035] {spark_submit.py:495} INFO - 24/09/03 18:11:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:11:06,369] {spark_submit.py:495} INFO - 24/09/03 18:11:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:11:06,880] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:11:06,891] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:11:06,942] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO ResourceUtils: ==============================================================
[2024-09-03 18:11:06,944] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:11:06,945] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO ResourceUtils: ==============================================================
[2024-09-03 18:11:06,945] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:11:06,963] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:11:06,976] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:11:06,977] {spark_submit.py:495} INFO - 24/09/03 18:11:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:11:07,012] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:11:07,013] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:11:07,014] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:11:07,014] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:11:07,014] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:11:07,161] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO Utils: Successfully started service 'sparkDriver' on port 39729.
[2024-09-03 18:11:07,185] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:11:07,209] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:11:07,224] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:11:07,225] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:11:07,228] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:11:07,240] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f51e9f90-c5ee-46e9-9ae2-af212b0b052b
[2024-09-03 18:11:07,258] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:11:07,272] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:11:07,447] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:11:07,493] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:11:07,699] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:11:07,721] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37101.
[2024-09-03 18:11:07,721] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO NettyBlockTransferService: Server created on 192.168.2.128:37101
[2024-09-03 18:11:07,723] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:11:07,728] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 37101, None)
[2024-09-03 18:11:07,731] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:37101 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 37101, None)
[2024-09-03 18:11:07,733] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 37101, None)
[2024-09-03 18:11:07,734] {spark_submit.py:495} INFO - 24/09/03 18:11:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 37101, None)
[2024-09-03 18:11:08,192] {spark_submit.py:495} INFO - 24/09/03 18:11:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:11:08,193] {spark_submit.py:495} INFO - 24/09/03 18:11:08 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:11:08,950] {spark_submit.py:495} INFO - 24/09/03 18:11:08 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 18:11:09,000] {spark_submit.py:495} INFO - 24/09/03 18:11:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:11:10,750] {spark_submit.py:495} INFO - 24/09/03 18:11:10 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:11:10,750] {spark_submit.py:495} INFO - 24/09/03 18:11:10 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:11:10,754] {spark_submit.py:495} INFO - 24/09/03 18:11:10 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:11:11,017] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:11:11,063] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:11:11,065] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:37101 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:11:11,069] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:11,086] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650469 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:11:11,233] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:11,249] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:11:11,250] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:11:11,250] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:11:11,251] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:11:11,260] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:11:11,362] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:11:11,365] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:11:11,365] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:37101 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:11:11,366] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:11:11,375] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:11:11,376] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:11:11,423] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:11:11,439] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:11:11,767] {spark_submit.py:495} INFO - 24/09/03 18:11:11 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-456165, partition values: [empty row]
[2024-09-03 18:11:12,000] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO CodeGenerator: Code generated in 147.525319 ms
[2024-09-03 18:11:12,098] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:11:12,215] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 696 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:11:12,217] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:11:12,224] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,933 s
[2024-09-03 18:11:12,228] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:11:12,228] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:11:12,230] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,996101 s
[2024-09-03 18:11:12,652] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:11:12,653] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:11:12,654] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:11:12,744] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:12,745] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:12,745] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:12,816] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO CodeGenerator: Code generated in 26.107761 ms
[2024-09-03 18:11:12,852] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO CodeGenerator: Code generated in 22.445557 ms
[2024-09-03 18:11:12,858] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:11:12,865] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:11:12,865] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:37101 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:11:12,867] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:12,869] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650469 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:11:12,921] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:12,924] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:11:12,924] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:11:12,924] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:11:12,925] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:11:12,925] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:11:12,975] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:11:12,977] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:11:12,978] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:37101 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:11:12,979] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:11:12,980] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:11:12,980] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:11:12,984] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:11:12,985] {spark_submit.py:495} INFO - 24/09/03 18:11:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:11:13,043] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:13,044] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:13,044] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:13,095] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO CodeGenerator: Code generated in 18.722752 ms
[2024-09-03 18:11:13,098] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-456165, partition values: [empty row]
[2024-09-03 18:11:13,116] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO CodeGenerator: Code generated in 15.266033 ms
[2024-09-03 18:11:13,136] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO CodeGenerator: Code generated in 4.444527 ms
[2024-09-03 18:11:13,261] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: Saved output of task 'attempt_202409031811121823122589849650472_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_202409031811121823122589849650472_0001_m_000000
[2024-09-03 18:11:13,262] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkHadoopMapRedUtil: attempt_202409031811121823122589849650472_0001_m_000000_1: Committed
[2024-09-03 18:11:13,266] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:11:13,272] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 290 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:11:13,272] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:11:13,274] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,347 s
[2024-09-03 18:11:13,274] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:11:13,274] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:11:13,276] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,354943 s
[2024-09-03 18:11:13,314] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileFormatWriter: Write Job e253bb31-010e-430d-8578-4c5f958225e0 committed.
[2024-09-03 18:11:13,317] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileFormatWriter: Finished processing stats for write job e253bb31-010e-430d-8578-4c5f958225e0.
[2024-09-03 18:11:13,360] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:11:13,361] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:11:13,362] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:11:13,372] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:13,373] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:13,373] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:13,413] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO CodeGenerator: Code generated in 11.546754 ms
[2024-09-03 18:11:13,417] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:11:13,428] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:11:13,428] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:37101 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:11:13,429] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:13,430] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650469 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:11:13,452] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:13,453] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:11:13,455] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:11:13,455] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:11:13,455] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:11:13,462] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:11:13,480] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:11:13,482] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:11:13,483] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:37101 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:11:13,484] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:11:13,485] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:11:13,485] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:11:13,486] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:11:13,486] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:11:13,498] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:13,499] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:13,499] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:13,555] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO CodeGenerator: Code generated in 11.720766 ms
[2024-09-03 18:11:13,563] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-456165, partition values: [empty row]
[2024-09-03 18:11:13,579] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO CodeGenerator: Code generated in 13.107578 ms
[2024-09-03 18:11:13,618] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileOutputCommitter: Saved output of task 'attempt_202409031811139108288717082097797_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_202409031811139108288717082097797_0002_m_000000
[2024-09-03 18:11:13,619] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkHadoopMapRedUtil: attempt_202409031811139108288717082097797_0002_m_000000_2: Committed
[2024-09-03 18:11:13,619] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:11:13,624] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 138 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:11:13,624] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:11:13,626] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,162 s
[2024-09-03 18:11:13,627] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:11:13,627] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:11:13,627] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,173944 s
[2024-09-03 18:11:13,642] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileFormatWriter: Write Job c8147606-13fd-4734-bcba-8ad2956ed36d committed.
[2024-09-03 18:11:13,642] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO FileFormatWriter: Finished processing stats for write job c8147606-13fd-4734-bcba-8ad2956ed36d.
[2024-09-03 18:11:13,695] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:11:13,705] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:11:13,719] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:11:13,738] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:11:13,738] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO BlockManager: BlockManager stopped
[2024-09-03 18:11:13,747] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:11:13,749] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:11:13,756] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:11:13,759] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:11:13,759] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-dd353e71-21e3-4e4e-a27f-d2c31e334282/pyspark-c629026f-d4e1-4f16-a53b-4ceebe5b1078
[2024-09-03 18:11:13,763] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-b62e0028-b121-4d25-8410-dfab1cd679ab
[2024-09-03 18:11:13,766] {spark_submit.py:495} INFO - 24/09/03 18:11:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-dd353e71-21e3-4e4e-a27f-d2c31e334282
[2024-09-03 18:11:13,893] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T211104, end_date=20240903T211113
[2024-09-03 18:11:13,905] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:11:13,922] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:15:08,092] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 18:15:08,101] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 18:15:08,101] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:15:08,101] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:15:08,101] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:15:08,111] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 18:15:08,114] {standard_task_runner.py:52} INFO - Started process 336791 to run task
[2024-09-03 18:15:08,117] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpegyncaod', '--error-file', '/tmp/tmpxau77n5w']
[2024-09-03 18:15:08,117] {standard_task_runner.py:80} INFO - Job 87: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:15:08,160] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:15:08,214] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 18:15:08,219] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:15:08,220] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 18:15:09,685] {spark_submit.py:495} INFO - 24/09/03 18:15:09 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:15:09,686] {spark_submit.py:495} INFO - 24/09/03 18:15:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:15:10,137] {spark_submit.py:495} INFO - 24/09/03 18:15:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:15:10,764] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:15:10,776] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:15:10,825] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO ResourceUtils: ==============================================================
[2024-09-03 18:15:10,826] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:15:10,827] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO ResourceUtils: ==============================================================
[2024-09-03 18:15:10,828] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:15:10,852] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:15:10,866] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:15:10,867] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:15:10,914] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:15:10,916] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:15:10,917] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:15:10,917] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:15:10,917] {spark_submit.py:495} INFO - 24/09/03 18:15:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:15:11,100] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO Utils: Successfully started service 'sparkDriver' on port 43609.
[2024-09-03 18:15:11,126] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:15:11,158] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:15:11,173] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:15:11,174] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:15:11,177] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:15:11,190] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-665eeda3-b1cb-4760-8731-86ea8bd6bffc
[2024-09-03 18:15:11,214] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:15:11,233] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:15:11,486] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:15:11,545] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:15:11,870] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:15:11,907] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38241.
[2024-09-03 18:15:11,907] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO NettyBlockTransferService: Server created on 192.168.2.128:38241
[2024-09-03 18:15:11,910] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:15:11,924] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38241, None)
[2024-09-03 18:15:11,927] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38241 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38241, None)
[2024-09-03 18:15:11,938] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38241, None)
[2024-09-03 18:15:11,940] {spark_submit.py:495} INFO - 24/09/03 18:15:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38241, None)
[2024-09-03 18:15:12,605] {spark_submit.py:495} INFO - 24/09/03 18:15:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:15:12,605] {spark_submit.py:495} INFO - 24/09/03 18:15:12 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:15:13,456] {spark_submit.py:495} INFO - 24/09/03 18:15:13 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
[2024-09-03 18:15:13,554] {spark_submit.py:495} INFO - 24/09/03 18:15:13 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2024-09-03 18:15:15,676] {spark_submit.py:495} INFO - 24/09/03 18:15:15 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:15:15,677] {spark_submit.py:495} INFO - 24/09/03 18:15:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:15:15,682] {spark_submit.py:495} INFO - 24/09/03 18:15:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:15:15,948] {spark_submit.py:495} INFO - 24/09/03 18:15:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:15:15,991] {spark_submit.py:495} INFO - 24/09/03 18:15:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:15:15,995] {spark_submit.py:495} INFO - 24/09/03 18:15:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38241 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:15:16,007] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:15:16,016] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198799 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:15:16,177] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:15:16,200] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:15:16,201] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:15:16,201] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:15:16,202] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:15:16,208] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:15:16,306] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:15:16,309] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:15:16,311] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38241 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:15:16,314] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:15:16,326] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:15:16,327] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:15:16,386] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:15:16,402] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:15:16,815] {spark_submit.py:495} INFO - 24/09/03 18:15:16 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4495, partition values: [empty row]
[2024-09-03 18:15:17,093] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO CodeGenerator: Code generated in 171.295102 ms
[2024-09-03 18:15:17,160] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:15:17,179] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 804 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:15:17,182] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:15:17,301] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,075 s
[2024-09-03 18:15:17,303] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:15:17,304] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:15:17,307] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,129087 s
[2024-09-03 18:15:17,784] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:15:17,786] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:15:17,786] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:15:17,866] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:15:17,866] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:15:17,867] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:15:17,990] {spark_submit.py:495} INFO - 24/09/03 18:15:17 INFO CodeGenerator: Code generated in 40.934576 ms
[2024-09-03 18:15:18,052] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 43.607715 ms
[2024-09-03 18:15:18,058] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:15:18,068] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:15:18,069] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38241 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:15:18,070] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:15:18,075] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198799 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:15:18,151] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:15:18,153] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:15:18,153] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:15:18,153] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:15:18,154] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:15:18,168] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:15:18,224] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:15:18,228] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:15:18,229] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38241 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:15:18,231] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:15:18,232] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:15:18,232] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:15:18,236] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:15:18,236] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:15:18,307] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:15:18,307] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:15:18,311] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:15:18,384] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 30.895971 ms
[2024-09-03 18:15:18,389] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4495, partition values: [empty row]
[2024-09-03 18:15:18,422] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 29.769379 ms
[2024-09-03 18:15:18,454] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 4.842932 ms
[2024-09-03 18:15:18,501] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409031815181547710366763847124_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_202409031815181547710366763847124_0001_m_000000
[2024-09-03 18:15:18,502] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkHadoopMapRedUtil: attempt_202409031815181547710366763847124_0001_m_000000_1: Committed
[2024-09-03 18:15:18,507] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:15:18,515] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 282 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:15:18,517] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:15:18,520] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,350 s
[2024-09-03 18:15:18,521] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:15:18,521] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:15:18,521] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,370033 s
[2024-09-03 18:15:18,541] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileFormatWriter: Write Job 20e0633f-5e47-4533-9337-775aa2e6eeaf committed.
[2024-09-03 18:15:18,546] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileFormatWriter: Finished processing stats for write job 20e0633f-5e47-4533-9337-775aa2e6eeaf.
[2024-09-03 18:15:18,605] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:15:18,605] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:15:18,605] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:15:18,619] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:15:18,619] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:15:18,619] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:15:18,649] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 10.724225 ms
[2024-09-03 18:15:18,653] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:15:18,662] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:15:18,663] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38241 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:15:18,666] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:15:18,668] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198799 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:15:18,687] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:15:18,688] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:15:18,688] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:15:18,688] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:15:18,688] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:15:18,689] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:15:18,709] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:15:18,711] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:15:18,714] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38241 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:15:18,719] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:15:18,722] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:15:18,722] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:15:18,724] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:15:18,729] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:15:18,739] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:15:18,740] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:15:18,741] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:15:18,771] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 7.877399 ms
[2024-09-03 18:15:18,774] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4495, partition values: [empty row]
[2024-09-03 18:15:18,790] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO CodeGenerator: Code generated in 12.162433 ms
[2024-09-03 18:15:18,802] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409031815186240185247172753217_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_202409031815186240185247172753217_0002_m_000000
[2024-09-03 18:15:18,802] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkHadoopMapRedUtil: attempt_202409031815186240185247172753217_0002_m_000000_2: Committed
[2024-09-03 18:15:18,804] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:15:18,805] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 82 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:15:18,806] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:15:18,808] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,116 s
[2024-09-03 18:15:18,808] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:15:18,808] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:15:18,808] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,120045 s
[2024-09-03 18:15:18,830] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileFormatWriter: Write Job 6e95638a-253f-4eb7-9168-874056b7e726 committed.
[2024-09-03 18:15:18,830] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO FileFormatWriter: Finished processing stats for write job 6e95638a-253f-4eb7-9168-874056b7e726.
[2024-09-03 18:15:18,861] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:15:18,873] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:15:18,887] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:15:18,898] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:15:18,898] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO BlockManager: BlockManager stopped
[2024-09-03 18:15:18,904] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:15:18,906] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:15:18,915] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:15:18,915] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:15:18,916] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab3672ea-545e-415d-94df-b752ed2a9b3f
[2024-09-03 18:15:18,919] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-7b9a12eb-b4d7-4ed7-89f8-54a792edf8fa/pyspark-33bf63f2-9245-457c-9c48-494e6c25b2b3
[2024-09-03 18:15:18,922] {spark_submit.py:495} INFO - 24/09/03 18:15:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-7b9a12eb-b4d7-4ed7-89f8-54a792edf8fa
[2024-09-03 18:15:19,010] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T211508, end_date=20240903T211519
[2024-09-03 18:15:19,056] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:15:19,073] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:21:18,416] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 18:21:18,424] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 18:21:18,424] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:21:18,425] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:21:18,425] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:21:18,437] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 18:21:18,440] {standard_task_runner.py:52} INFO - Started process 342970 to run task
[2024-09-03 18:21:18,443] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpi6f82c5r', '--error-file', '/tmp/tmpgqevvb15']
[2024-09-03 18:21:18,443] {standard_task_runner.py:80} INFO - Job 87: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:21:18,478] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:21:18,527] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 18:21:18,533] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:21:18,536] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 18:21:19,705] {spark_submit.py:495} INFO - 24/09/03 18:21:19 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:21:19,705] {spark_submit.py:495} INFO - 24/09/03 18:21:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:21:20,140] {spark_submit.py:495} INFO - 24/09/03 18:21:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:21:20,765] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:21:20,774] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:21:20,823] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO ResourceUtils: ==============================================================
[2024-09-03 18:21:20,824] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:21:20,824] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO ResourceUtils: ==============================================================
[2024-09-03 18:21:20,825] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:21:20,855] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:21:20,870] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:21:20,871] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:21:20,923] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:21:20,924] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:21:20,925] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:21:20,926] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:21:20,926] {spark_submit.py:495} INFO - 24/09/03 18:21:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:21:21,124] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO Utils: Successfully started service 'sparkDriver' on port 38187.
[2024-09-03 18:21:21,155] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:21:21,199] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:21:21,224] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:21:21,224] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:21:21,230] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:21:21,242] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-120fe77b-dbb7-458f-ac56-0d93e1735b6e
[2024-09-03 18:21:21,264] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:21:21,282] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:21:21,513] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:21:21,571] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:21:21,830] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:21:21,878] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36443.
[2024-09-03 18:21:21,878] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO NettyBlockTransferService: Server created on 192.168.2.128:36443
[2024-09-03 18:21:21,880] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:21:21,887] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36443, None)
[2024-09-03 18:21:21,891] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36443 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36443, None)
[2024-09-03 18:21:21,902] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36443, None)
[2024-09-03 18:21:21,904] {spark_submit.py:495} INFO - 24/09/03 18:21:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36443, None)
[2024-09-03 18:21:22,550] {spark_submit.py:495} INFO - 24/09/03 18:21:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:21:22,551] {spark_submit.py:495} INFO - 24/09/03 18:21:22 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:21:23,453] {spark_submit.py:495} INFO - 24/09/03 18:21:23 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2024-09-03 18:21:23,518] {spark_submit.py:495} INFO - 24/09/03 18:21:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:21:25,656] {spark_submit.py:495} INFO - 24/09/03 18:21:25 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:21:25,657] {spark_submit.py:495} INFO - 24/09/03 18:21:25 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:21:25,662] {spark_submit.py:495} INFO - 24/09/03 18:21:25 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:21:26,039] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:21:26,105] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:21:26,113] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36443 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:21:26,124] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:21:26,137] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649687 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:21:26,341] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:21:26,370] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:21:26,371] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:21:26,371] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:21:26,372] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:21:26,382] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:21:26,495] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:21:26,499] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:21:26,503] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36443 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:21:26,505] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:21:26,521] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:21:26,524] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:21:26,586] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:21:26,606] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:21:26,988] {spark_submit.py:495} INFO - 24/09/03 18:21:26 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455383, partition values: [empty row]
[2024-09-03 18:21:27,356] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO CodeGenerator: Code generated in 251.333059 ms
[2024-09-03 18:21:27,545] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:21:27,731] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 993 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:21:27,739] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:21:27,752] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,343 s
[2024-09-03 18:21:27,759] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:21:27,760] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:21:27,776] {spark_submit.py:495} INFO - 24/09/03 18:21:27 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,424850 s
[2024-09-03 18:21:28,775] {spark_submit.py:495} INFO - 24/09/03 18:21:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:21:28,779] {spark_submit.py:495} INFO - 24/09/03 18:21:28 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:21:28,780] {spark_submit.py:495} INFO - 24/09/03 18:21:28 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:21:28,925] {spark_submit.py:495} INFO - 24/09/03 18:21:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:21:28,926] {spark_submit.py:495} INFO - 24/09/03 18:21:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:21:28,930] {spark_submit.py:495} INFO - 24/09/03 18:21:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:21:29,097] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO CodeGenerator: Code generated in 65.927537 ms
[2024-09-03 18:21:29,171] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO CodeGenerator: Code generated in 49.481368 ms
[2024-09-03 18:21:29,183] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:21:29,196] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:21:29,196] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36443 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:21:29,198] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:21:29,202] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649687 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:21:29,279] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:21:29,283] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:21:29,283] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:21:29,283] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:21:29,284] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:21:29,286] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:21:29,358] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:21:29,364] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:21:29,367] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36443 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:21:29,367] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:21:29,368] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:21:29,368] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:21:29,375] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:21:29,376] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:21:29,453] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:21:29,453] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:21:29,454] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:21:29,548] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO CodeGenerator: Code generated in 27.591114 ms
[2024-09-03 18:21:29,553] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455383, partition values: [empty row]
[2024-09-03 18:21:29,591] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO CodeGenerator: Code generated in 33.508717 ms
[2024-09-03 18:21:29,629] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO CodeGenerator: Code generated in 7.314366 ms
[2024-09-03 18:21:29,801] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409031821294203603742670345320_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_202409031821294203603742670345320_0001_m_000000
[2024-09-03 18:21:29,801] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SparkHadoopMapRedUtil: attempt_202409031821294203603742670345320_0001_m_000000_1: Committed
[2024-09-03 18:21:29,805] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:21:29,810] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 440 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:21:29,811] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:21:29,812] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,524 s
[2024-09-03 18:21:29,812] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:21:29,812] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:21:29,815] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,535545 s
[2024-09-03 18:21:29,847] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileFormatWriter: Write Job e2d7a66d-7165-4553-adb8-0947362f6885 committed.
[2024-09-03 18:21:29,851] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileFormatWriter: Finished processing stats for write job e2d7a66d-7165-4553-adb8-0947362f6885.
[2024-09-03 18:21:29,899] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:21:29,899] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:21:29,899] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:21:29,911] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:21:29,912] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:21:29,912] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:21:29,964] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO CodeGenerator: Code generated in 13.854995 ms
[2024-09-03 18:21:29,970] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:21:29,981] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:21:29,982] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36443 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:21:29,983] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:21:29,983] {spark_submit.py:495} INFO - 24/09/03 18:21:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649687 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:21:30,001] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:21:30,002] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:21:30,002] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:21:30,002] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:21:30,002] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:21:30,004] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:21:30,036] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:21:30,038] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:21:30,039] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36443 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:21:30,040] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:21:30,040] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:21:30,040] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:21:30,042] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:21:30,047] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:21:30,059] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:21:30,060] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:21:30,060] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:21:30,096] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO CodeGenerator: Code generated in 12.32603 ms
[2024-09-03 18:21:30,100] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455383, partition values: [empty row]
[2024-09-03 18:21:30,115] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO CodeGenerator: Code generated in 12.426669 ms
[2024-09-03 18:21:30,154] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO FileOutputCommitter: Saved output of task 'attempt_202409031821293006200794510350865_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_202409031821293006200794510350865_0002_m_000000
[2024-09-03 18:21:30,154] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SparkHadoopMapRedUtil: attempt_202409031821293006200794510350865_0002_m_000000_2: Committed
[2024-09-03 18:21:30,156] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:21:30,159] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 117 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:21:30,159] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,154 s
[2024-09-03 18:21:30,159] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:21:30,160] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:21:30,160] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:21:30,160] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,159326 s
[2024-09-03 18:21:30,188] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO FileFormatWriter: Write Job 841cc082-7f3d-439c-b687-faf37b424c83 committed.
[2024-09-03 18:21:30,189] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO FileFormatWriter: Finished processing stats for write job 841cc082-7f3d-439c-b687-faf37b424c83.
[2024-09-03 18:21:30,235] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:21:30,251] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:21:30,272] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:21:30,300] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:21:30,301] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO BlockManager: BlockManager stopped
[2024-09-03 18:21:30,314] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:21:30,317] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:21:30,326] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:21:30,326] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:21:30,326] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-92ebec25-9282-4fc7-97d0-a40e5db2b8d0/pyspark-82d69476-4393-41a4-a8a9-8b07420e4624
[2024-09-03 18:21:30,332] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-92ebec25-9282-4fc7-97d0-a40e5db2b8d0
[2024-09-03 18:21:30,335] {spark_submit.py:495} INFO - 24/09/03 18:21:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-ba4416d2-be8f-4778-a306-d6e078058e16
[2024-09-03 18:21:30,454] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240903T212118, end_date=20240903T212130
[2024-09-03 18:21:30,488] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:21:30,505] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:42:51,546] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 21:42:51,553] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 21:42:51,553] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:42:51,554] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:42:51,554] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:42:51,563] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 21:42:51,566] {standard_task_runner.py:52} INFO - Started process 416988 to run task
[2024-09-03 21:42:51,570] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp0ey6guvv', '--error-file', '/tmp/tmprcu2v60i']
[2024-09-03 21:42:51,570] {standard_task_runner.py:80} INFO - Job 84: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:42:51,606] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:42:51,648] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 21:42:51,651] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:42:51,652] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 21:42:52,772] {spark_submit.py:495} INFO - 24/09/03 21:42:52 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:42:52,772] {spark_submit.py:495} INFO - 24/09/03 21:42:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:42:53,102] {spark_submit.py:495} INFO - 24/09/03 21:42:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:42:53,668] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:42:53,678] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:42:53,721] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO ResourceUtils: ==============================================================
[2024-09-03 21:42:53,721] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:42:53,721] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO ResourceUtils: ==============================================================
[2024-09-03 21:42:53,721] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:42:53,739] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:42:53,751] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:42:53,752] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:42:53,796] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:42:53,796] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:42:53,796] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:42:53,796] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:42:53,797] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:42:53,946] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO Utils: Successfully started service 'sparkDriver' on port 35543.
[2024-09-03 21:42:53,970] {spark_submit.py:495} INFO - 24/09/03 21:42:53 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:42:54,000] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:42:54,016] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:42:54,017] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:42:54,020] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:42:54,031] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-055e226a-c0d9-4e82-a718-1152aa4de7bf
[2024-09-03 21:42:54,049] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:42:54,070] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:42:54,254] {spark_submit.py:495} INFO - 24/09/03 21:42:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:42:54,262] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:42:54,310] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:42:54,508] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:42:54,529] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46025.
[2024-09-03 21:42:54,530] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO NettyBlockTransferService: Server created on 192.168.2.128:46025
[2024-09-03 21:42:54,531] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:42:54,536] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46025, None)
[2024-09-03 21:42:54,540] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46025 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46025, None)
[2024-09-03 21:42:54,541] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46025, None)
[2024-09-03 21:42:54,541] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46025, None)
[2024-09-03 21:42:54,986] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:42:54,987] {spark_submit.py:495} INFO - 24/09/03 21:42:54 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:42:55,859] {spark_submit.py:495} INFO - 24/09/03 21:42:55 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 21:42:55,919] {spark_submit.py:495} INFO - 24/09/03 21:42:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2024-09-03 21:42:57,717] {spark_submit.py:495} INFO - 24/09/03 21:42:57 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:42:57,719] {spark_submit.py:495} INFO - 24/09/03 21:42:57 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:42:57,723] {spark_submit.py:495} INFO - 24/09/03 21:42:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:42:58,009] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:42:58,074] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:42:58,077] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46025 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:42:58,090] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:58,100] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649376 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:42:58,255] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:58,272] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:42:58,273] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:42:58,274] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:42:58,275] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:42:58,280] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:42:58,362] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:42:58,365] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:42:58,368] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46025 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:42:58,369] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:42:58,379] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:42:58,380] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:42:58,446] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:42:58,462] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:42:58,724] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455072, partition values: [empty row]
[2024-09-03 21:42:58,961] {spark_submit.py:495} INFO - 24/09/03 21:42:58 INFO CodeGenerator: Code generated in 136.230657 ms
[2024-09-03 21:42:59,047] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 21:42:59,171] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 626 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:42:59,173] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:42:59,179] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,880 s
[2024-09-03 21:42:59,182] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:42:59,182] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:42:59,186] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,929194 s
[2024-09-03 21:42:59,589] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:42:59,590] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:42:59,591] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:42:59,671] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:42:59,671] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:42:59,672] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:42:59,750] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO CodeGenerator: Code generated in 26.684653 ms
[2024-09-03 21:42:59,787] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO CodeGenerator: Code generated in 24.12484 ms
[2024-09-03 21:42:59,794] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:42:59,803] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:42:59,804] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46025 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:42:59,805] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:59,808] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649376 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:42:59,869] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:42:59,871] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:42:59,871] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:42:59,871] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:42:59,871] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:42:59,872] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:42:59,919] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:42:59,921] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:42:59,922] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46025 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:42:59,923] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:42:59,923] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:42:59,923] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:42:59,928] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:42:59,928] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:42:59,996] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:42:59,997] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:42:59,997] {spark_submit.py:495} INFO - 24/09/03 21:42:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:00,056] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO CodeGenerator: Code generated in 22.071296 ms
[2024-09-03 21:43:00,061] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455072, partition values: [empty row]
[2024-09-03 21:43:00,083] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO CodeGenerator: Code generated in 19.146862 ms
[2024-09-03 21:43:00,107] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO CodeGenerator: Code generated in 6.224918 ms
[2024-09-03 21:43:00,237] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileOutputCommitter: Saved output of task 'attempt_20240903214259151334737020049616_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_20240903214259151334737020049616_0001_m_000000
[2024-09-03 21:43:00,238] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkHadoopMapRedUtil: attempt_20240903214259151334737020049616_0001_m_000000_1: Committed
[2024-09-03 21:43:00,243] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:43:00,247] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 322 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:43:00,252] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,376 s
[2024-09-03 21:43:00,253] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:43:00,253] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:43:00,254] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:43:00,254] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,384479 s
[2024-09-03 21:43:00,276] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileFormatWriter: Write Job 2f76b741-aeed-4bd9-9426-9a53d8f0d735 committed.
[2024-09-03 21:43:00,279] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileFormatWriter: Finished processing stats for write job 2f76b741-aeed-4bd9-9426-9a53d8f0d735.
[2024-09-03 21:43:00,319] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:43:00,319] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:43:00,320] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:43:00,329] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:43:00,329] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:43:00,329] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:00,363] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO CodeGenerator: Code generated in 10.0117 ms
[2024-09-03 21:43:00,369] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:43:00,377] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:43:00,377] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46025 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:43:00,378] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:00,379] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649376 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:43:00,396] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:00,398] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:43:00,398] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:43:00,399] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:43:00,400] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:43:00,400] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:43:00,426] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:43:00,428] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:43:00,429] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46025 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:43:00,430] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:43:00,430] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:43:00,432] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:43:00,435] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:43:00,438] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:43:00,453] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:43:00,453] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:43:00,454] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:00,499] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO CodeGenerator: Code generated in 20.847059 ms
[2024-09-03 21:43:00,502] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-455072, partition values: [empty row]
[2024-09-03 21:43:00,514] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO CodeGenerator: Code generated in 11.023002 ms
[2024-09-03 21:43:00,544] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileOutputCommitter: Saved output of task 'attempt_202409032143007190508200567525087_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_202409032143007190508200567525087_0002_m_000000
[2024-09-03 21:43:00,544] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkHadoopMapRedUtil: attempt_202409032143007190508200567525087_0002_m_000000_2: Committed
[2024-09-03 21:43:00,544] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:43:00,545] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 114 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:43:00,545] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:43:00,546] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,144 s
[2024-09-03 21:43:00,549] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:43:00,549] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:43:00,549] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,149589 s
[2024-09-03 21:43:00,570] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileFormatWriter: Write Job f0c164e8-f47b-4601-9fbf-76fa794ceda3 committed.
[2024-09-03 21:43:00,571] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO FileFormatWriter: Finished processing stats for write job f0c164e8-f47b-4601-9fbf-76fa794ceda3.
[2024-09-03 21:43:00,607] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:43:00,619] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:43:00,641] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:43:00,653] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:43:00,654] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO BlockManager: BlockManager stopped
[2024-09-03 21:43:00,662] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:43:00,666] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:43:00,673] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:43:00,673] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:43:00,674] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e59525d-bbef-4662-a5a5-bbb30e22a9ee
[2024-09-03 21:43:00,676] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-be85e2f8-d0a5-4962-a276-4e9d00218836
[2024-09-03 21:43:00,678] {spark_submit.py:495} INFO - 24/09/03 21:43:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e59525d-bbef-4662-a5a5-bbb30e22a9ee/pyspark-dda6ca36-aa7d-4884-adbe-363c9dc78785
[2024-09-03 21:43:00,776] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240904T004251, end_date=20240904T004300
[2024-09-03 21:43:00,819] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:43:00,833] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:50:55,677] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 22:50:55,685] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [queued]>
[2024-09-03 22:50:55,685] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:50:55,685] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:50:55,685] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:50:55,696] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-23 00:00:00+00:00
[2024-09-03 22:50:55,699] {standard_task_runner.py:52} INFO - Started process 449740 to run task
[2024-09-03 22:50:55,703] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-23T00:00:00+00:00', '--job-id', '132', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmph3p3tomg', '--error-file', '/tmp/tmpwayms02s']
[2024-09-03 22:50:55,704] {standard_task_runner.py:80} INFO - Job 132: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:50:55,745] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-23T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:50:55,788] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-23T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-23T00:00:00+00:00
[2024-09-03 22:50:55,792] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:50:55,793] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240823
[2024-09-03 22:50:56,824] {spark_submit.py:495} INFO - 24/09/03 22:50:56 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:50:56,824] {spark_submit.py:495} INFO - 24/09/03 22:50:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:50:57,166] {spark_submit.py:495} INFO - 24/09/03 22:50:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:50:57,714] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:50:57,724] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:50:57,758] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO ResourceUtils: ==============================================================
[2024-09-03 22:50:57,759] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:50:57,759] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO ResourceUtils: ==============================================================
[2024-09-03 22:50:57,759] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:50:57,778] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:50:57,792] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:50:57,793] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:50:57,835] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:50:57,836] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:50:57,836] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:50:57,837] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:50:57,837] {spark_submit.py:495} INFO - 24/09/03 22:50:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:50:58,003] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO Utils: Successfully started service 'sparkDriver' on port 32915.
[2024-09-03 22:50:58,030] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:50:58,072] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:50:58,089] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:50:58,089] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:50:58,094] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:50:58,108] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-467eb34e-b41a-47ec-8753-28bd19215a11
[2024-09-03 22:50:58,129] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:50:58,144] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:50:58,334] {spark_submit.py:495} INFO - 24/09/03 22:50:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:50:58,346] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:50:58,405] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:50:58,618] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:50:58,645] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35773.
[2024-09-03 22:50:58,645] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO NettyBlockTransferService: Server created on 192.168.2.128:35773
[2024-09-03 22:50:58,647] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:50:58,654] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 35773, None)
[2024-09-03 22:50:58,657] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:35773 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 35773, None)
[2024-09-03 22:50:58,661] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 35773, None)
[2024-09-03 22:50:58,663] {spark_submit.py:495} INFO - 24/09/03 22:50:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 35773, None)
[2024-09-03 22:50:59,172] {spark_submit.py:495} INFO - 24/09/03 22:50:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:50:59,174] {spark_submit.py:495} INFO - 24/09/03 22:50:59 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:50:59,983] {spark_submit.py:495} INFO - 24/09/03 22:50:59 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
[2024-09-03 22:51:00,044] {spark_submit.py:495} INFO - 24/09/03 22:51:00 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:51:01,849] {spark_submit.py:495} INFO - 24/09/03 22:51:01 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:51:01,850] {spark_submit.py:495} INFO - 24/09/03 22:51:01 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:51:01,854] {spark_submit.py:495} INFO - 24/09/03 22:51:01 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:51:02,164] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:51:02,224] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:51:02,226] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:35773 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:51:02,233] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:51:02,242] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198894 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:51:02,424] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:51:02,440] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:51:02,440] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:51:02,440] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:51:02,441] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:51:02,458] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:51:02,560] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:51:02,565] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:51:02,566] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:35773 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:51:02,567] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:51:02,581] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:51:02,583] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:51:02,647] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:51:02,670] {spark_submit.py:495} INFO - 24/09/03 22:51:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:51:03,012] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4590, partition values: [empty row]
[2024-09-03 22:51:03,253] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO CodeGenerator: Code generated in 131.277162 ms
[2024-09-03 22:51:03,298] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 22:51:03,308] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 678 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:51:03,408] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:51:03,409] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,930 s
[2024-09-03 22:51:03,415] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:51:03,416] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:51:03,422] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,996771 s
[2024-09-03 22:51:03,825] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:51:03,827] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:51:03,827] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:51:03,901] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:51:03,901] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:51:03,902] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:51:03,980] {spark_submit.py:495} INFO - 24/09/03 22:51:03 INFO CodeGenerator: Code generated in 25.667033 ms
[2024-09-03 22:51:04,020] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 25.102157 ms
[2024-09-03 22:51:04,026] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:51:04,036] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:51:04,038] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:35773 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:51:04,039] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:51:04,042] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198894 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:51:04,108] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:51:04,110] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:51:04,110] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:51:04,111] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:51:04,111] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:51:04,118] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:51:04,198] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:51:04,201] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:51:04,202] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:35773 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:51:04,203] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:51:04,204] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:51:04,204] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:51:04,209] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:51:04,210] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:51:04,275] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:51:04,275] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:51:04,277] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:51:04,348] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 26.807762 ms
[2024-09-03 22:51:04,351] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4590, partition values: [empty row]
[2024-09-03 22:51:04,375] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 21.572531 ms
[2024-09-03 22:51:04,399] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 6.675166 ms
[2024-09-03 22:51:04,440] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_20240903225104352388106886704896_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240823/_temporary/0/task_20240903225104352388106886704896_0001_m_000000
[2024-09-03 22:51:04,441] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkHadoopMapRedUtil: attempt_20240903225104352388106886704896_0001_m_000000_1: Committed
[2024-09-03 22:51:04,447] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:51:04,450] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 245 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:51:04,451] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:51:04,453] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,330 s
[2024-09-03 22:51:04,453] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:51:04,453] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:51:04,454] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,346399 s
[2024-09-03 22:51:04,484] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileFormatWriter: Write Job 64dac4bc-d55a-43f7-8276-37f3c0b51d79 committed.
[2024-09-03 22:51:04,490] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileFormatWriter: Finished processing stats for write job 64dac4bc-d55a-43f7-8276-37f3c0b51d79.
[2024-09-03 22:51:04,568] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:51:04,569] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:51:04,569] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:51:04,584] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:51:04,584] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:51:04,585] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:51:04,621] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 12.04606 ms
[2024-09-03 22:51:04,626] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:51:04,635] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:51:04,636] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:35773 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:51:04,637] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:51:04,639] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198894 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:51:04,657] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:51:04,660] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:51:04,660] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:51:04,661] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:51:04,661] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:51:04,671] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:51:04,715] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:51:04,719] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:51:04,720] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:35773 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:51:04,721] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:51:04,722] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:51:04,722] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:51:04,723] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:51:04,724] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:51:04,736] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:51:04,736] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:51:04,736] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:51:04,774] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 9.320275 ms
[2024-09-03 22:51:04,776] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-23T00:00:00.00Z/data_engineer_20240823.json, range: 0-4590, partition values: [empty row]
[2024-09-03 22:51:04,791] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO CodeGenerator: Code generated in 11.754375 ms
[2024-09-03 22:51:04,800] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileOutputCommitter: Saved output of task 'attempt_202409032251042078840701065192019_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240823/_temporary/0/task_202409032251042078840701065192019_0002_m_000000
[2024-09-03 22:51:04,800] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkHadoopMapRedUtil: attempt_202409032251042078840701065192019_0002_m_000000_2: Committed
[2024-09-03 22:51:04,801] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:51:04,802] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 79 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:51:04,802] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:51:04,807] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,132 s
[2024-09-03 22:51:04,807] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:51:04,807] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:51:04,807] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,148549 s
[2024-09-03 22:51:04,826] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileFormatWriter: Write Job 08cb26be-761d-40a2-a849-8027d4e2e107 committed.
[2024-09-03 22:51:04,826] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO FileFormatWriter: Finished processing stats for write job 08cb26be-761d-40a2-a849-8027d4e2e107.
[2024-09-03 22:51:04,872] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:51:04,876] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:51:04,895] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:51:04,914] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:51:04,915] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO BlockManager: BlockManager stopped
[2024-09-03 22:51:04,929] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:51:04,936] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:51:04,943] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:51:04,943] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:51:04,945] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5f9d47d-63c0-43cc-a15a-727f5e893054
[2024-09-03 22:51:04,951] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5f9d47d-63c0-43cc-a15a-727f5e893054/pyspark-c443b484-bc36-4d51-a50b-ec0018acbe84
[2024-09-03 22:51:04,955] {spark_submit.py:495} INFO - 24/09/03 22:51:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbb1c5c2-40b8-4a57-a2cc-d4301a2e5033
[2024-09-03 22:51:05,012] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240823T000000, start_date=20240904T015055, end_date=20240904T015105
[2024-09-03 22:51:05,035] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:51:05,082] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
