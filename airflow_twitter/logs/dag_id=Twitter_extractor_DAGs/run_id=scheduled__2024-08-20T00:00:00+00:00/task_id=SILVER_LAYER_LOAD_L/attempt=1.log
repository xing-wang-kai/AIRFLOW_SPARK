[2024-09-03 16:17:33,284] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 16:17:33,291] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 16:17:33,291] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:17:33,291] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:17:33,291] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:17:33,300] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 16:17:33,303] {standard_task_runner.py:52} INFO - Started process 280029 to run task
[2024-09-03 16:17:33,306] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2fjd16lu', '--error-file', '/tmp/tmpcixo62gb']
[2024-09-03 16:17:33,306] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:17:33,340] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:17:33,382] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 16:17:33,385] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:17:33,386] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-20T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240820
[2024-09-03 16:17:34,546] {spark_submit.py:495} INFO - 24/09/03 16:17:34 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:17:34,547] {spark_submit.py:495} INFO - 24/09/03 16:17:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:17:34,863] {spark_submit.py:495} INFO - 24/09/03 16:17:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:17:35,422] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:17:35,433] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:17:35,485] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO ResourceUtils: ==============================================================
[2024-09-03 16:17:35,493] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:17:35,495] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO ResourceUtils: ==============================================================
[2024-09-03 16:17:35,495] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:17:35,520] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:17:35,536] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:17:35,536] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:17:35,580] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:17:35,580] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:17:35,580] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:17:35,580] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:17:35,581] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:17:35,734] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO Utils: Successfully started service 'sparkDriver' on port 41685.
[2024-09-03 16:17:35,756] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:17:35,785] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:17:35,806] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:17:35,806] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:17:35,810] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:17:35,821] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-97eb908d-70be-4e69-bd4d-1bdc0f6f7b31
[2024-09-03 16:17:35,841] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:17:35,855] {spark_submit.py:495} INFO - 24/09/03 16:17:35 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:17:36,064] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:17:36,122] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:17:36,316] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:17:36,338] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39701.
[2024-09-03 16:17:36,338] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO NettyBlockTransferService: Server created on 192.168.2.128:39701
[2024-09-03 16:17:36,339] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:17:36,348] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 39701, None)
[2024-09-03 16:17:36,351] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:39701 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 39701, None)
[2024-09-03 16:17:36,355] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 39701, None)
[2024-09-03 16:17:36,359] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 39701, None)
[2024-09-03 16:17:36,855] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:17:36,856] {spark_submit.py:495} INFO - 24/09/03 16:17:36 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 76, in <module>
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - define_extration(spark,
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 48, in define_extration
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - df = spark.read.json(src)
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 372, in json
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2024-09-03 16:17:37,602] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2024-09-03 16:17:37,608] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-20T00:00:00.00Z
[2024-09-03 16:17:37,637] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:17:37,647] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:17:37,659] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:17:37,667] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:17:37,668] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO BlockManager: BlockManager stopped
[2024-09-03 16:17:37,674] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:17:37,680] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:17:37,684] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:17:37,685] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:17:37,685] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d36f7ce-d76e-443b-9109-8e2f2d285990
[2024-09-03 16:17:37,687] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d65b89e9-aec2-408f-87a3-5f95d11ff0d7
[2024-09-03 16:17:37,689] {spark_submit.py:495} INFO - 24/09/03 16:17:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d65b89e9-aec2-408f-87a3-5f95d11ff0d7/pyspark-2789a680-fd42-4d12-823c-9f9a98b6439c
[2024-09-03 16:17:37,724] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-20T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240820. Error code is: 1.
[2024-09-03 16:17:37,727] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T191733, end_date=20240903T191737
[2024-09-03 16:17:37,738] {standard_task_runner.py:92} ERROR - Failed to execute job 78 for task SILVER_LAYER_LOAD_L (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src /twitter_extractor_airflow/datalake/bronze/twitter_dataengineer/extract_data=2024-08-20T00:00:00.00Z --dest /twitter_extractor_airflow/datalake/silver/twitter_dataengineer/ --process-data 20240820. Error code is: 1.; 280029)
[2024-09-03 16:17:37,757] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-09-03 16:17:37,770] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:21:45,406] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 16:21:45,416] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 16:21:45,416] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:21:45,416] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:21:45,416] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:21:45,427] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 16:21:45,430] {standard_task_runner.py:52} INFO - Started process 282601 to run task
[2024-09-03 16:21:45,433] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpy931k_u7', '--error-file', '/tmp/tmp4czyw29k']
[2024-09-03 16:21:45,436] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:21:45,478] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:21:45,523] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 16:21:45,527] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:21:45,527] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/ --process-data 20240820
[2024-09-03 16:21:46,521] {spark_submit.py:495} INFO - 24/09/03 16:21:46 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:21:46,522] {spark_submit.py:495} INFO - 24/09/03 16:21:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:21:46,831] {spark_submit.py:495} INFO - 24/09/03 16:21:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:21:47,348] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:21:47,356] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:21:47,396] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO ResourceUtils: ==============================================================
[2024-09-03 16:21:47,396] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:21:47,397] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO ResourceUtils: ==============================================================
[2024-09-03 16:21:47,398] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:21:47,421] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:21:47,435] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:21:47,435] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:21:47,474] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:21:47,474] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:21:47,474] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:21:47,474] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:21:47,475] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:21:47,629] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO Utils: Successfully started service 'sparkDriver' on port 45317.
[2024-09-03 16:21:47,654] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:21:47,681] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:21:47,699] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:21:47,699] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:21:47,703] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:21:47,716] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0464a0b8-c600-4f28-8874-862d5a5afa53
[2024-09-03 16:21:47,737] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:21:47,751] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:21:47,939] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:21:47,989] {spark_submit.py:495} INFO - 24/09/03 16:21:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:21:48,170] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:21:48,192] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40069.
[2024-09-03 16:21:48,192] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO NettyBlockTransferService: Server created on 192.168.2.128:40069
[2024-09-03 16:21:48,193] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:21:48,199] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40069, None)
[2024-09-03 16:21:48,201] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40069 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40069, None)
[2024-09-03 16:21:48,203] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40069, None)
[2024-09-03 16:21:48,204] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40069, None)
[2024-09-03 16:21:48,667] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:21:48,668] {spark_submit.py:495} INFO - 24/09/03 16:21:48 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:21:49,439] {spark_submit.py:495} INFO - 24/09/03 16:21:49 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 16:21:49,491] {spark_submit.py:495} INFO - 24/09/03 16:21:49 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:21:51,161] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:21:51,163] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:21:51,166] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:21:51,412] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:21:51,455] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:21:51,457] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40069 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:21:51,462] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:21:51,470] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198760 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:21:51,661] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:21:51,685] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:21:51,685] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:21:51,685] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:21:51,685] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:21:51,694] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:21:51,768] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:21:51,771] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:21:51,772] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40069 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:21:51,772] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:21:51,783] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:21:51,784] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:21:51,826] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4967 bytes) taskResourceAssignments Map()
[2024-09-03 16:21:51,845] {spark_submit.py:495} INFO - 24/09/03 16:21:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:21:52,144] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4456, partition values: [empty row]
[2024-09-03 16:21:52,397] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO CodeGenerator: Code generated in 153.559433 ms
[2024-09-03 16:21:52,439] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:21:52,448] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 629 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:21:52,455] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:21:52,456] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,748 s
[2024-09-03 16:21:52,568] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:21:52,569] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:21:52,572] {spark_submit.py:495} INFO - 24/09/03 16:21:52 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,910034 s
[2024-09-03 16:21:53,028] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:21:53,032] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:21:53,032] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:21:53,106] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:21:53,107] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:21:53,109] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:21:53,206] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 33.649396 ms
[2024-09-03 16:21:53,255] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 31.664702 ms
[2024-09-03 16:21:53,265] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:21:53,274] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:21:53,275] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40069 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:21:53,276] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:21:53,279] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198760 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:21:53,344] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:21:53,346] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:21:53,347] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:21:53,348] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:21:53,350] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:21:53,350] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:21:53,396] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:21:53,399] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:21:53,399] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40069 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:21:53,400] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:21:53,400] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:21:53,401] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:21:53,404] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:21:53,405] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:21:53,458] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:21:53,458] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:21:53,459] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:21:53,539] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 33.447928 ms
[2024-09-03 16:21:53,542] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4456, partition values: [empty row]
[2024-09-03 16:21:53,561] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 16.22242 ms
[2024-09-03 16:21:53,622] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 7.965052 ms
[2024-09-03 16:21:53,655] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: Saved output of task 'attempt_202409031621532183509182096661057_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/twitter_df/process_data=20240820/_temporary/0/task_202409031621532183509182096661057_0001_m_000000
[2024-09-03 16:21:53,655] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkHadoopMapRedUtil: attempt_202409031621532183509182096661057_0001_m_000000_1: Committed
[2024-09-03 16:21:53,660] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:21:53,665] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 264 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:21:53,665] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:21:53,666] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,311 s
[2024-09-03 16:21:53,667] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:21:53,668] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:21:53,668] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,323296 s
[2024-09-03 16:21:53,683] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileFormatWriter: Write Job 2305b898-b225-4be5-add5-2e655758be9a committed.
[2024-09-03 16:21:53,686] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileFormatWriter: Finished processing stats for write job 2305b898-b225-4be5-add5-2e655758be9a.
[2024-09-03 16:21:53,731] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:21:53,731] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:21:53,731] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:21:53,743] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:21:53,743] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:21:53,744] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:21:53,814] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 38.431113 ms
[2024-09-03 16:21:53,817] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:21:53,823] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:21:53,824] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40069 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:21:53,825] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:21:53,833] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198760 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:21:53,851] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:21:53,863] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:21:53,863] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:21:53,863] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:21:53,864] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:21:53,864] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:21:53,882] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:21:53,884] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.9 KiB, free 364.8 MiB)
[2024-09-03 16:21:53,885] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40069 (size: 63.9 KiB, free: 366.1 MiB)
[2024-09-03 16:21:53,886] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:21:53,888] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:21:53,888] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:21:53,890] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:21:53,890] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:21:53,902] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:21:53,902] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:21:53,903] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:21:53,929] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 8.055959 ms
[2024-09-03 16:21:53,933] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4456, partition values: [empty row]
[2024-09-03 16:21:53,952] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO CodeGenerator: Code generated in 17.570297 ms
[2024-09-03 16:21:53,960] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileOutputCommitter: Saved output of task 'attempt_202409031621536763534080998654151_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/user_df/process_data=20240820/_temporary/0/task_202409031621536763534080998654151_0002_m_000000
[2024-09-03 16:21:53,960] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO SparkHadoopMapRedUtil: attempt_202409031621536763534080998654151_0002_m_000000_2: Committed
[2024-09-03 16:21:53,961] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:21:53,964] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 75 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:21:53,965] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:21:53,965] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,100 s
[2024-09-03 16:21:53,965] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:21:53,966] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:21:53,966] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,103980 s
[2024-09-03 16:21:53,983] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileFormatWriter: Write Job 99f9a46d-a388-4520-95bc-7bed9bbb120e committed.
[2024-09-03 16:21:53,983] {spark_submit.py:495} INFO - 24/09/03 16:21:53 INFO FileFormatWriter: Finished processing stats for write job 99f9a46d-a388-4520-95bc-7bed9bbb120e.
[2024-09-03 16:21:54,013] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:21:54,021] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:21:54,043] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:21:54,053] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:21:54,053] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO BlockManager: BlockManager stopped
[2024-09-03 16:21:54,059] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:21:54,061] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:21:54,067] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:21:54,068] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:21:54,069] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-4197f506-316a-46e4-b274-ffd67cdde246/pyspark-6f1b3660-582b-4e4c-bc77-d082499a5cbd
[2024-09-03 16:21:54,072] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-2586e95d-c1d2-4246-b6a1-0b1e244d66fd
[2024-09-03 16:21:54,074] {spark_submit.py:495} INFO - 24/09/03 16:21:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-4197f506-316a-46e4-b274-ffd67cdde246
[2024-09-03 16:21:54,184] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T192145, end_date=20240903T192154
[2024-09-03 16:21:54,242] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:21:54,253] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:39:53,635] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 16:39:53,642] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 16:39:53,642] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:39:53,642] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:39:53,642] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:39:53,653] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 16:39:53,655] {standard_task_runner.py:52} INFO - Started process 293468 to run task
[2024-09-03 16:39:53,659] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmplfhf5g81', '--error-file', '/tmp/tmphe6a5sfq']
[2024-09-03 16:39:53,659] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:39:53,692] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:39:53,735] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 16:39:53,739] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:39:53,741] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 16:39:54,867] {spark_submit.py:495} INFO - 24/09/03 16:39:54 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:39:54,868] {spark_submit.py:495} INFO - 24/09/03 16:39:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:39:55,202] {spark_submit.py:495} INFO - 24/09/03 16:39:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:39:55,756] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:39:55,768] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:39:55,817] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO ResourceUtils: ==============================================================
[2024-09-03 16:39:55,818] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:39:55,819] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO ResourceUtils: ==============================================================
[2024-09-03 16:39:55,820] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:39:55,840] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:39:55,853] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:39:55,854] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:39:55,897] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:39:55,898] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:39:55,899] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:39:55,899] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:39:55,899] {spark_submit.py:495} INFO - 24/09/03 16:39:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:39:56,058] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO Utils: Successfully started service 'sparkDriver' on port 39563.
[2024-09-03 16:39:56,083] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:39:56,111] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:39:56,129] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:39:56,131] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:39:56,134] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:39:56,146] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-67540b5e-d859-440a-aeea-777be2e90cb5
[2024-09-03 16:39:56,166] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:39:56,182] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:39:56,411] {spark_submit.py:495} INFO - 24/09/03 16:39:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:39:56,428] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:39:56,486] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:39:56,681] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:39:56,702] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38287.
[2024-09-03 16:39:56,703] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO NettyBlockTransferService: Server created on 192.168.2.128:38287
[2024-09-03 16:39:56,705] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:39:56,710] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38287, None)
[2024-09-03 16:39:56,714] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38287 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38287, None)
[2024-09-03 16:39:56,716] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38287, None)
[2024-09-03 16:39:56,717] {spark_submit.py:495} INFO - 24/09/03 16:39:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38287, None)
[2024-09-03 16:39:57,169] {spark_submit.py:495} INFO - 24/09/03 16:39:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:39:57,169] {spark_submit.py:495} INFO - 24/09/03 16:39:57 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:39:58,023] {spark_submit.py:495} INFO - 24/09/03 16:39:58 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
[2024-09-03 16:39:58,077] {spark_submit.py:495} INFO - 24/09/03 16:39:58 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:39:59,902] {spark_submit.py:495} INFO - 24/09/03 16:39:59 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:39:59,903] {spark_submit.py:495} INFO - 24/09/03 16:39:59 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:39:59,906] {spark_submit.py:495} INFO - 24/09/03 16:39:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:40:00,166] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:00,216] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:40:00,219] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38287 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:40:00,223] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:00,231] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649369 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:00,388] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:00,403] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:00,404] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:00,405] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:00,406] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:00,414] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:00,502] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:40:00,506] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:40:00,507] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38287 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:40:00,507] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:00,519] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:00,520] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:40:00,570] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:00,590] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:40:00,853] {spark_submit.py:495} INFO - 24/09/03 16:40:00 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455065, partition values: [empty row]
[2024-09-03 16:40:01,102] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO CodeGenerator: Code generated in 145.987712 ms
[2024-09-03 16:40:01,208] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 16:40:01,340] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 671 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:01,341] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:40:01,358] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,921 s
[2024-09-03 16:40:01,362] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:01,364] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:40:01,369] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,978822 s
[2024-09-03 16:40:01,762] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:40:01,763] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:40:01,763] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:40:01,824] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:01,824] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:01,825] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:01,910] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO CodeGenerator: Code generated in 32.67837 ms
[2024-09-03 16:40:01,955] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO CodeGenerator: Code generated in 28.691317 ms
[2024-09-03 16:40:01,960] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:40:01,967] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:40:01,968] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38287 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:40:01,969] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:01,973] {spark_submit.py:495} INFO - 24/09/03 16:40:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649369 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:02,043] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:02,043] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:02,043] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:02,044] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:02,044] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:02,064] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:02,111] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:40:02,115] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:40:02,115] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38287 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:40:02,116] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:02,117] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:02,118] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:40:02,122] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:02,123] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:40:02,168] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:02,168] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:02,169] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:02,229] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO CodeGenerator: Code generated in 18.457937 ms
[2024-09-03 16:40:02,232] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455065, partition values: [empty row]
[2024-09-03 16:40:02,255] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO CodeGenerator: Code generated in 20.782475 ms
[2024-09-03 16:40:02,279] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO CodeGenerator: Code generated in 5.423436 ms
[2024-09-03 16:40:02,399] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640025025402205335615659_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409031640025025402205335615659_0001_m_000000
[2024-09-03 16:40:02,400] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkHadoopMapRedUtil: attempt_202409031640025025402205335615659_0001_m_000000_1: Committed
[2024-09-03 16:40:02,405] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:40:02,412] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 293 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:02,414] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,349 s
[2024-09-03 16:40:02,414] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:02,416] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:40:02,418] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:40:02,420] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,380778 s
[2024-09-03 16:40:02,443] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileFormatWriter: Write Job 1587e134-f525-48cb-a62a-2b17d061c128 committed.
[2024-09-03 16:40:02,446] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileFormatWriter: Finished processing stats for write job 1587e134-f525-48cb-a62a-2b17d061c128.
[2024-09-03 16:40:02,490] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:40:02,491] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:40:02,491] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:40:02,500] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:02,500] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:02,501] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:02,546] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO CodeGenerator: Code generated in 18.106891 ms
[2024-09-03 16:40:02,552] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:40:02,563] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:40:02,563] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38287 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:40:02,565] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:02,566] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649369 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:40:02,585] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:40:02,586] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:40:02,586] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:40:02,586] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:40:02,586] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:40:02,590] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:40:02,628] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:40:02,631] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:40:02,632] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38287 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:40:02,633] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:40:02,634] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:40:02,634] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:40:02,636] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:40:02,638] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:40:02,654] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:40:02,654] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:40:02,654] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:40:02,687] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO CodeGenerator: Code generated in 12.464159 ms
[2024-09-03 16:40:02,689] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455065, partition values: [empty row]
[2024-09-03 16:40:02,702] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO CodeGenerator: Code generated in 10.546387 ms
[2024-09-03 16:40:02,728] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileOutputCommitter: Saved output of task 'attempt_202409031640023340344657636375844_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_202409031640023340344657636375844_0002_m_000000
[2024-09-03 16:40:02,728] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkHadoopMapRedUtil: attempt_202409031640023340344657636375844_0002_m_000000_2: Committed
[2024-09-03 16:40:02,729] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:40:02,730] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 95 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:40:02,730] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:40:02,734] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,141 s
[2024-09-03 16:40:02,734] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:40:02,735] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:40:02,735] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,146775 s
[2024-09-03 16:40:02,765] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileFormatWriter: Write Job 3d7f6df0-6621-4c1c-9c8f-ffa57bec35e3 committed.
[2024-09-03 16:40:02,766] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO FileFormatWriter: Finished processing stats for write job 3d7f6df0-6621-4c1c-9c8f-ffa57bec35e3.
[2024-09-03 16:40:02,798] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:40:02,804] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:40:02,814] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:40:02,823] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:40:02,824] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO BlockManager: BlockManager stopped
[2024-09-03 16:40:02,829] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:40:02,831] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:40:02,844] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:40:02,845] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:40:02,846] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-e9668f01-25fc-475e-8aa3-c628b7bdf726/pyspark-101d64c4-e4b5-4cd5-887e-11b491b1350f
[2024-09-03 16:40:02,849] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-e9668f01-25fc-475e-8aa3-c628b7bdf726
[2024-09-03 16:40:02,854] {spark_submit.py:495} INFO - 24/09/03 16:40:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-0415af00-b721-44a0-8aaf-8a120af0347d
[2024-09-03 16:40:02,923] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T193953, end_date=20240903T194002
[2024-09-03 16:40:02,955] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:40:02,962] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:53:00,487] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 17:53:00,493] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 17:53:00,493] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:53:00,493] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:53:00,494] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:53:00,509] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 17:53:00,512] {standard_task_runner.py:52} INFO - Started process 322934 to run task
[2024-09-03 17:53:00,515] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp4thkw1pa', '--error-file', '/tmp/tmpmfwnodsr']
[2024-09-03 17:53:00,515] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:53:00,547] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:53:00,590] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 17:53:00,594] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:53:00,595] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 17:53:01,716] {spark_submit.py:495} INFO - 24/09/03 17:53:01 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:53:01,716] {spark_submit.py:495} INFO - 24/09/03 17:53:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:53:02,030] {spark_submit.py:495} INFO - 24/09/03 17:53:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:53:02,617] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:53:02,624] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:53:02,663] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO ResourceUtils: ==============================================================
[2024-09-03 17:53:02,663] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:53:02,664] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO ResourceUtils: ==============================================================
[2024-09-03 17:53:02,664] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:53:02,683] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:53:02,694] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:53:02,695] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:53:02,733] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:53:02,734] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:53:02,734] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:53:02,734] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:53:02,735] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:53:02,896] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO Utils: Successfully started service 'sparkDriver' on port 41135.
[2024-09-03 17:53:02,921] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:53:02,949] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:53:02,964] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:53:02,965] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:53:02,968] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:53:02,978] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d31d282b-7710-411d-9015-a25e08b9d4bb
[2024-09-03 17:53:02,997] {spark_submit.py:495} INFO - 24/09/03 17:53:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:53:03,030] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:53:03,233] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:53:03,294] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:53:03,467] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:53:03,488] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36047.
[2024-09-03 17:53:03,489] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO NettyBlockTransferService: Server created on 192.168.2.128:36047
[2024-09-03 17:53:03,490] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:53:03,494] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36047, None)
[2024-09-03 17:53:03,499] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36047 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36047, None)
[2024-09-03 17:53:03,503] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36047, None)
[2024-09-03 17:53:03,505] {spark_submit.py:495} INFO - 24/09/03 17:53:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36047, None)
[2024-09-03 17:53:04,053] {spark_submit.py:495} INFO - 24/09/03 17:53:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:53:04,054] {spark_submit.py:495} INFO - 24/09/03 17:53:04 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:53:04,843] {spark_submit.py:495} INFO - 24/09/03 17:53:04 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 17:53:04,902] {spark_submit.py:495} INFO - 24/09/03 17:53:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:53:06,512] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:53:06,514] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:53:06,518] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:53:06,764] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:53:06,837] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:53:06,843] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36047 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:53:06,855] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:06,865] {spark_submit.py:495} INFO - 24/09/03 17:53:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648815 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:53:07,017] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:07,034] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:53:07,034] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:53:07,035] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:53:07,036] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:53:07,045] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:53:07,136] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:53:07,138] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:53:07,139] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36047 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:53:07,139] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:53:07,151] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:53:07,152] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:53:07,197] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:53:07,216] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:53:07,461] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-454511, partition values: [empty row]
[2024-09-03 17:53:07,728] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO CodeGenerator: Code generated in 160.085523 ms
[2024-09-03 17:53:07,818] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 17:53:07,931] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 641 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:53:07,940] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:53:07,950] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,889 s
[2024-09-03 17:53:07,954] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:53:07,961] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:53:07,964] {spark_submit.py:495} INFO - 24/09/03 17:53:07 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,946027 s
[2024-09-03 17:53:08,332] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:53:08,334] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:53:08,334] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:53:08,419] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:08,420] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:08,420] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:08,503] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO CodeGenerator: Code generated in 24.99173 ms
[2024-09-03 17:53:08,550] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO CodeGenerator: Code generated in 30.650387 ms
[2024-09-03 17:53:08,556] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:53:08,569] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:53:08,570] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36047 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:53:08,570] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:08,573] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648815 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:53:08,624] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:08,626] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:53:08,626] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:53:08,627] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:53:08,627] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:53:08,630] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:53:08,671] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:53:08,674] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:53:08,675] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36047 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:53:08,676] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:53:08,676] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:53:08,677] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:53:08,679] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:53:08,680] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:53:08,730] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:08,731] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:08,732] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:08,789] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO CodeGenerator: Code generated in 19.607698 ms
[2024-09-03 17:53:08,794] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-454511, partition values: [empty row]
[2024-09-03 17:53:08,814] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO CodeGenerator: Code generated in 17.34299 ms
[2024-09-03 17:53:08,839] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO CodeGenerator: Code generated in 5.357403 ms
[2024-09-03 17:53:08,951] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO FileOutputCommitter: Saved output of task 'attempt_202409031753083149459590692857863_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409031753083149459590692857863_0001_m_000000
[2024-09-03 17:53:08,952] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO SparkHadoopMapRedUtil: attempt_202409031753083149459590692857863_0001_m_000000_1: Committed
[2024-09-03 17:53:08,958] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:53:08,967] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 288 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:53:08,969] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:53:08,970] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,337 s
[2024-09-03 17:53:08,970] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:53:08,970] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:53:08,972] {spark_submit.py:495} INFO - 24/09/03 17:53:08 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,347874 s
[2024-09-03 17:53:09,003] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileFormatWriter: Write Job 1e4026f3-9fb5-4447-987c-35b3bfc201e4 committed.
[2024-09-03 17:53:09,006] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileFormatWriter: Finished processing stats for write job 1e4026f3-9fb5-4447-987c-35b3bfc201e4.
[2024-09-03 17:53:09,055] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:53:09,055] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:53:09,056] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:53:09,065] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:09,066] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:09,066] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:09,090] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO CodeGenerator: Code generated in 9.17484 ms
[2024-09-03 17:53:09,094] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:53:09,100] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:53:09,101] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36047 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:53:09,101] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:09,102] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648815 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:53:09,118] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:53:09,119] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:53:09,119] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:53:09,119] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:53:09,120] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:53:09,120] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:53:09,142] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:53:09,146] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:53:09,146] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36047 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:53:09,147] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:53:09,147] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:53:09,148] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:53:09,150] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:53:09,150] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:53:09,164] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:53:09,164] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:53:09,165] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:53:09,195] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO CodeGenerator: Code generated in 11.990431 ms
[2024-09-03 17:53:09,197] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-454511, partition values: [empty row]
[2024-09-03 17:53:09,247] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO CodeGenerator: Code generated in 48.561087 ms
[2024-09-03 17:53:09,264] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.2.128:36047 in memory (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:53:09,288] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileOutputCommitter: Saved output of task 'attempt_202409031753097658432408011938798_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_202409031753097658432408011938798_0002_m_000000
[2024-09-03 17:53:09,289] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkHadoopMapRedUtil: attempt_202409031753097658432408011938798_0002_m_000000_2: Committed
[2024-09-03 17:53:09,289] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2777 bytes result sent to driver
[2024-09-03 17:53:09,294] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.128:36047 in memory (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:53:09,295] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 146 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:53:09,295] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:53:09,296] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,176 s
[2024-09-03 17:53:09,296] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:53:09,296] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:53:09,297] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,179358 s
[2024-09-03 17:53:09,326] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileFormatWriter: Write Job 44123827-578f-4758-8830-d694f5ab11d7 committed.
[2024-09-03 17:53:09,327] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO FileFormatWriter: Finished processing stats for write job 44123827-578f-4758-8830-d694f5ab11d7.
[2024-09-03 17:53:09,374] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:53:09,385] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:53:09,402] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:53:09,415] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:53:09,415] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO BlockManager: BlockManager stopped
[2024-09-03 17:53:09,421] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:53:09,425] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:53:09,432] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:53:09,433] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:53:09,434] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b891246-a02c-45c5-a132-d1a09e8f59cd/pyspark-408dac90-9076-470c-8d99-2839ecfd0117
[2024-09-03 17:53:09,437] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-6aa3e6b4-3d10-439e-b035-3b01de0fda11
[2024-09-03 17:53:09,439] {spark_submit.py:495} INFO - 24/09/03 17:53:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b891246-a02c-45c5-a132-d1a09e8f59cd
[2024-09-03 17:53:09,490] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T205300, end_date=20240903T205309
[2024-09-03 17:53:09,523] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:53:09,539] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:09:43,983] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 18:09:43,989] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 18:09:43,989] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:09:43,989] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:09:43,990] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:09:43,998] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 18:09:44,000] {standard_task_runner.py:52} INFO - Started process 331297 to run task
[2024-09-03 18:09:44,003] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp7rl2r5eh', '--error-file', '/tmp/tmpm9j812r4']
[2024-09-03 18:09:44,004] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:09:44,034] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:09:44,077] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 18:09:44,082] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:09:44,083] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 18:09:45,186] {spark_submit.py:495} INFO - 24/09/03 18:09:45 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:09:45,187] {spark_submit.py:495} INFO - 24/09/03 18:09:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:09:45,534] {spark_submit.py:495} INFO - 24/09/03 18:09:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:09:46,067] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:09:46,075] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:09:46,115] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO ResourceUtils: ==============================================================
[2024-09-03 18:09:46,116] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:09:46,117] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO ResourceUtils: ==============================================================
[2024-09-03 18:09:46,118] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:09:46,140] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:09:46,154] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:09:46,155] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:09:46,195] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:09:46,196] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:09:46,196] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:09:46,196] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:09:46,197] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:09:46,354] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO Utils: Successfully started service 'sparkDriver' on port 43109.
[2024-09-03 18:09:46,376] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:09:46,404] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:09:46,421] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:09:46,422] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:09:46,426] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:09:46,439] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-12bdb67c-5a57-42fa-bfc0-42f1920595f4
[2024-09-03 18:09:46,458] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:09:46,473] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:09:46,672] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:09:46,720] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:09:46,907] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:09:46,929] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41423.
[2024-09-03 18:09:46,930] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO NettyBlockTransferService: Server created on 192.168.2.128:41423
[2024-09-03 18:09:46,930] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:09:46,936] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 41423, None)
[2024-09-03 18:09:46,939] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:41423 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 41423, None)
[2024-09-03 18:09:46,941] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 41423, None)
[2024-09-03 18:09:46,943] {spark_submit.py:495} INFO - 24/09/03 18:09:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 41423, None)
[2024-09-03 18:09:47,382] {spark_submit.py:495} INFO - 24/09/03 18:09:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:09:47,383] {spark_submit.py:495} INFO - 24/09/03 18:09:47 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:09:48,188] {spark_submit.py:495} INFO - 24/09/03 18:09:48 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
[2024-09-03 18:09:48,248] {spark_submit.py:495} INFO - 24/09/03 18:09:48 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:09:49,873] {spark_submit.py:495} INFO - 24/09/03 18:09:49 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:09:49,874] {spark_submit.py:495} INFO - 24/09/03 18:09:49 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:09:49,877] {spark_submit.py:495} INFO - 24/09/03 18:09:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:09:50,113] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:09:50,156] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:09:50,158] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:41423 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:09:50,164] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:09:50,171] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649459 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:09:50,306] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:09:50,326] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:09:50,326] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:09:50,327] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:09:50,328] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:09:50,341] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:09:50,412] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:09:50,414] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:09:50,415] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:41423 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:09:50,415] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:09:50,427] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:09:50,428] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:09:50,471] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:09:50,486] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:09:50,762] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455155, partition values: [empty row]
[2024-09-03 18:09:50,990] {spark_submit.py:495} INFO - 24/09/03 18:09:50 INFO CodeGenerator: Code generated in 134.54257 ms
[2024-09-03 18:09:51,068] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:09:51,197] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 615 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:09:51,203] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:09:51,207] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,851 s
[2024-09-03 18:09:51,219] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:09:51,220] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:09:51,222] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,915867 s
[2024-09-03 18:09:51,592] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:09:51,594] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:09:51,594] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:09:51,656] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:09:51,656] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:09:51,657] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:09:51,743] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO CodeGenerator: Code generated in 31.884384 ms
[2024-09-03 18:09:51,784] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO CodeGenerator: Code generated in 26.303479 ms
[2024-09-03 18:09:51,790] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:09:51,800] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:09:51,801] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:41423 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:09:51,802] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:09:51,804] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649459 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:09:51,860] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:09:51,862] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:09:51,862] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:09:51,862] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:09:51,862] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:09:51,869] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:09:51,928] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:09:51,931] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:09:51,933] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:41423 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:09:51,933] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:09:51,933] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:09:51,933] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:09:51,936] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:09:51,936] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:09:51,990] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:09:51,990] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:09:51,990] {spark_submit.py:495} INFO - 24/09/03 18:09:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:09:52,041] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO CodeGenerator: Code generated in 19.028646 ms
[2024-09-03 18:09:52,047] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455155, partition values: [empty row]
[2024-09-03 18:09:52,070] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO CodeGenerator: Code generated in 20.567937 ms
[2024-09-03 18:09:52,095] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO CodeGenerator: Code generated in 6.600761 ms
[2024-09-03 18:09:52,201] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409031809514527681841314318066_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409031809514527681841314318066_0001_m_000000
[2024-09-03 18:09:52,202] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkHadoopMapRedUtil: attempt_202409031809514527681841314318066_0001_m_000000_1: Committed
[2024-09-03 18:09:52,206] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:09:52,212] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 279 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:09:52,212] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:09:52,216] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,344 s
[2024-09-03 18:09:52,217] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:09:52,217] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:09:52,217] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,355356 s
[2024-09-03 18:09:52,236] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileFormatWriter: Write Job 38bfdbde-a5ef-488c-a5c7-6b300084b8a3 committed.
[2024-09-03 18:09:52,239] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileFormatWriter: Finished processing stats for write job 38bfdbde-a5ef-488c-a5c7-6b300084b8a3.
[2024-09-03 18:09:52,275] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:09:52,275] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:09:52,276] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:09:52,287] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:09:52,288] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:09:52,289] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:09:52,318] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO CodeGenerator: Code generated in 11.346158 ms
[2024-09-03 18:09:52,322] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:09:52,329] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:09:52,330] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:41423 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:09:52,332] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:09:52,333] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649459 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:09:52,349] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:09:52,350] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:09:52,351] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:09:52,352] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:09:52,353] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:09:52,353] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:09:52,372] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:09:52,374] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:09:52,375] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:41423 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:09:52,376] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:09:52,380] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:09:52,380] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:09:52,380] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:09:52,380] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:09:52,390] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:09:52,390] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:09:52,390] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:09:52,434] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO CodeGenerator: Code generated in 13.087597 ms
[2024-09-03 18:09:52,443] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455155, partition values: [empty row]
[2024-09-03 18:09:52,455] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO CodeGenerator: Code generated in 9.783074 ms
[2024-09-03 18:09:52,482] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409031809522549131050351058035_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_202409031809522549131050351058035_0002_m_000000
[2024-09-03 18:09:52,483] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkHadoopMapRedUtil: attempt_202409031809522549131050351058035_0002_m_000000_2: Committed
[2024-09-03 18:09:52,483] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:09:52,485] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 107 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:09:52,487] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,131 s
[2024-09-03 18:09:52,487] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:09:52,487] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:09:52,487] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:09:52,487] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,137216 s
[2024-09-03 18:09:52,504] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileFormatWriter: Write Job 47c7e905-fd2a-4473-ba24-0f2c1404b0c9 committed.
[2024-09-03 18:09:52,504] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO FileFormatWriter: Finished processing stats for write job 47c7e905-fd2a-4473-ba24-0f2c1404b0c9.
[2024-09-03 18:09:52,538] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:09:52,553] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:09:52,568] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:09:52,575] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:09:52,576] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO BlockManager: BlockManager stopped
[2024-09-03 18:09:52,584] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:09:52,586] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:09:52,607] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:09:52,607] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:09:52,607] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-c40e184a-5610-4c27-96f2-78acc0d7cc83
[2024-09-03 18:09:52,610] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-4bcfa27a-da77-4797-bef9-f15fbf933144
[2024-09-03 18:09:52,613] {spark_submit.py:495} INFO - 24/09/03 18:09:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-c40e184a-5610-4c27-96f2-78acc0d7cc83/pyspark-f652b382-61b3-4b2e-8028-e9ad9bf86d62
[2024-09-03 18:09:52,668] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T210943, end_date=20240903T210952
[2024-09-03 18:09:52,701] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:09:52,747] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:14:16,185] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 18:14:16,190] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 18:14:16,190] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:14:16,190] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:14:16,190] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:14:16,198] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 18:14:16,202] {standard_task_runner.py:52} INFO - Started process 335532 to run task
[2024-09-03 18:14:16,205] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp4ybloh0b', '--error-file', '/tmp/tmpt40xdfrj']
[2024-09-03 18:14:16,205] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:14:16,237] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:14:16,278] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 18:14:16,281] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:14:16,282] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 18:14:17,295] {spark_submit.py:495} INFO - 24/09/03 18:14:17 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:14:17,295] {spark_submit.py:495} INFO - 24/09/03 18:14:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:14:17,667] {spark_submit.py:495} INFO - 24/09/03 18:14:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:14:18,221] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:14:18,229] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:14:18,268] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO ResourceUtils: ==============================================================
[2024-09-03 18:14:18,269] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:14:18,269] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO ResourceUtils: ==============================================================
[2024-09-03 18:14:18,269] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:14:18,293] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:14:18,306] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:14:18,306] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:14:18,347] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:14:18,347] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:14:18,347] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:14:18,347] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:14:18,348] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:14:18,504] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO Utils: Successfully started service 'sparkDriver' on port 33071.
[2024-09-03 18:14:18,528] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:14:18,559] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:14:18,576] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:14:18,577] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:14:18,581] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:14:18,593] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e0de05fa-9efe-43e9-a092-07c3dd49b4df
[2024-09-03 18:14:18,613] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:14:18,629] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:14:18,829] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:14:18,874] {spark_submit.py:495} INFO - 24/09/03 18:14:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:14:19,063] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:14:19,088] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46233.
[2024-09-03 18:14:19,089] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO NettyBlockTransferService: Server created on 192.168.2.128:46233
[2024-09-03 18:14:19,090] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:14:19,095] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46233, None)
[2024-09-03 18:14:19,097] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46233 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46233, None)
[2024-09-03 18:14:19,099] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46233, None)
[2024-09-03 18:14:19,102] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46233, None)
[2024-09-03 18:14:19,577] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:14:19,578] {spark_submit.py:495} INFO - 24/09/03 18:14:19 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:14:20,416] {spark_submit.py:495} INFO - 24/09/03 18:14:20 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 18:14:20,469] {spark_submit.py:495} INFO - 24/09/03 18:14:20 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:14:22,290] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:14:22,291] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:14:22,295] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:14:22,544] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:14:22,602] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:14:22,606] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46233 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:14:22,613] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:22,620] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649042 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:22,753] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:22,771] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:22,771] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:22,772] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:22,776] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:22,782] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:22,863] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:14:22,866] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:14:22,868] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46233 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:14:22,868] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:22,881] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:22,882] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:14:22,950] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:22,966] {spark_submit.py:495} INFO - 24/09/03 18:14:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:14:23,183] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-454738, partition values: [empty row]
[2024-09-03 18:14:23,447] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO CodeGenerator: Code generated in 178.195324 ms
[2024-09-03 18:14:23,554] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 18:14:23,721] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 642 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:23,726] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:14:23,735] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,933 s
[2024-09-03 18:14:23,735] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:23,735] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:14:23,741] {spark_submit.py:495} INFO - 24/09/03 18:14:23 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,985349 s
[2024-09-03 18:14:24,106] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:14:24,107] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:14:24,108] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:14:24,169] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:24,169] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:24,170] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:24,268] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 33.944468 ms
[2024-09-03 18:14:24,310] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 27.211134 ms
[2024-09-03 18:14:24,315] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:14:24,323] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:14:24,324] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46233 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:14:24,325] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:24,327] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649042 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:24,379] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:24,381] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:24,381] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:24,381] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:24,381] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:24,382] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:24,426] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:14:24,428] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:14:24,429] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46233 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:14:24,430] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:24,430] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:24,430] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:14:24,435] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:24,436] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:14:24,483] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:24,483] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:24,484] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:24,545] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 20.17443 ms
[2024-09-03 18:14:24,549] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-454738, partition values: [empty row]
[2024-09-03 18:14:24,573] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 20.442107 ms
[2024-09-03 18:14:24,595] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 5.224243 ms
[2024-09-03 18:14:24,703] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: Saved output of task 'attempt_202409031814244525440804842819631_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409031814244525440804842819631_0001_m_000000
[2024-09-03 18:14:24,704] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkHadoopMapRedUtil: attempt_202409031814244525440804842819631_0001_m_000000_1: Committed
[2024-09-03 18:14:24,714] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:14:24,720] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 287 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:24,721] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,336 s
[2024-09-03 18:14:24,722] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:24,726] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:14:24,726] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:14:24,727] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,347973 s
[2024-09-03 18:14:24,753] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileFormatWriter: Write Job 85c0c750-afc2-477c-9fe7-178bb515872f committed.
[2024-09-03 18:14:24,757] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileFormatWriter: Finished processing stats for write job 85c0c750-afc2-477c-9fe7-178bb515872f.
[2024-09-03 18:14:24,805] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:14:24,805] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:14:24,805] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:14:24,816] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:24,816] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:24,817] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:24,841] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 8.458263 ms
[2024-09-03 18:14:24,846] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:14:24,853] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:14:24,854] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46233 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:14:24,855] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:24,855] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649042 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:14:24,872] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:14:24,873] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:14:24,873] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:14:24,873] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:14:24,874] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:14:24,876] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:14:24,894] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:14:24,897] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:14:24,899] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46233 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:14:24,903] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:14:24,904] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:14:24,904] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:14:24,906] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:14:24,906] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:14:24,917] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:14:24,917] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:14:24,918] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:14:24,946] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 8.671872 ms
[2024-09-03 18:14:24,949] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-454738, partition values: [empty row]
[2024-09-03 18:14:24,963] {spark_submit.py:495} INFO - 24/09/03 18:14:24 INFO CodeGenerator: Code generated in 10.820033 ms
[2024-09-03 18:14:25,000] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO FileOutputCommitter: Saved output of task 'attempt_20240903181424531889502581827083_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_20240903181424531889502581827083_0002_m_000000
[2024-09-03 18:14:25,000] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO SparkHadoopMapRedUtil: attempt_20240903181424531889502581827083_0002_m_000000_2: Committed
[2024-09-03 18:14:25,001] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:14:25,003] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 98 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:14:25,004] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,127 s
[2024-09-03 18:14:25,005] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:14:25,006] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:14:25,006] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:14:25,007] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,134771 s
[2024-09-03 18:14:25,025] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO FileFormatWriter: Write Job 0e7097ca-e500-4834-805c-fd53d0b4b4e6 committed.
[2024-09-03 18:14:25,025] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO FileFormatWriter: Finished processing stats for write job 0e7097ca-e500-4834-805c-fd53d0b4b4e6.
[2024-09-03 18:14:25,057] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:14:25,067] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:14:25,079] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:14:25,088] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:14:25,089] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO BlockManager: BlockManager stopped
[2024-09-03 18:14:25,096] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:14:25,098] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:14:25,104] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:14:25,105] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:14:25,106] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d52f51a-1541-4a20-96b4-bf76528f9a99
[2024-09-03 18:14:25,109] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-7412622d-5608-479f-b197-1c425bc3f610
[2024-09-03 18:14:25,112] {spark_submit.py:495} INFO - 24/09/03 18:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d52f51a-1541-4a20-96b4-bf76528f9a99/pyspark-0f6b3aba-7e11-4678-ae66-dc22672d2c94
[2024-09-03 18:14:25,202] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T211416, end_date=20240903T211425
[2024-09-03 18:14:25,240] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:14:25,293] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:19:45,867] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 18:19:45,872] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 18:19:45,872] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:19:45,872] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:19:45,872] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:19:45,880] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 18:19:45,882] {standard_task_runner.py:52} INFO - Started process 341423 to run task
[2024-09-03 18:19:45,884] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpu3ledcys', '--error-file', '/tmp/tmpedcvpb0r']
[2024-09-03 18:19:45,885] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:19:45,915] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:19:45,954] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 18:19:45,957] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:19:45,958] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 18:19:47,063] {spark_submit.py:495} INFO - 24/09/03 18:19:47 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:19:47,063] {spark_submit.py:495} INFO - 24/09/03 18:19:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:19:47,412] {spark_submit.py:495} INFO - 24/09/03 18:19:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:19:47,939] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:19:47,947] {spark_submit.py:495} INFO - 24/09/03 18:19:47 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:19:47,986] {spark_submit.py:495} INFO - 24/09/03 18:19:47 INFO ResourceUtils: ==============================================================
[2024-09-03 18:19:47,986] {spark_submit.py:495} INFO - 24/09/03 18:19:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:19:47,986] {spark_submit.py:495} INFO - 24/09/03 18:19:47 INFO ResourceUtils: ==============================================================
[2024-09-03 18:19:47,986] {spark_submit.py:495} INFO - 24/09/03 18:19:47 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:19:48,004] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:19:48,018] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:19:48,018] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:19:48,061] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:19:48,061] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:19:48,061] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:19:48,061] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:19:48,061] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:19:48,231] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO Utils: Successfully started service 'sparkDriver' on port 46411.
[2024-09-03 18:19:48,256] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:19:48,284] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:19:48,300] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:19:48,300] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:19:48,303] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:19:48,316] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9fb67695-9d03-453b-a19f-6946ee09fc01
[2024-09-03 18:19:48,334] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:19:48,350] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:19:48,541] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:19:48,590] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:19:48,785] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:19:48,812] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46797.
[2024-09-03 18:19:48,812] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO NettyBlockTransferService: Server created on 192.168.2.128:46797
[2024-09-03 18:19:48,814] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:19:48,819] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46797, None)
[2024-09-03 18:19:48,821] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46797 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46797, None)
[2024-09-03 18:19:48,824] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46797, None)
[2024-09-03 18:19:48,825] {spark_submit.py:495} INFO - 24/09/03 18:19:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46797, None)
[2024-09-03 18:19:49,267] {spark_submit.py:495} INFO - 24/09/03 18:19:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:19:49,267] {spark_submit.py:495} INFO - 24/09/03 18:19:49 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:19:50,108] {spark_submit.py:495} INFO - 24/09/03 18:19:50 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2024-09-03 18:19:50,164] {spark_submit.py:495} INFO - 24/09/03 18:19:50 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:19:51,889] {spark_submit.py:495} INFO - 24/09/03 18:19:51 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:19:51,890] {spark_submit.py:495} INFO - 24/09/03 18:19:51 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:19:51,894] {spark_submit.py:495} INFO - 24/09/03 18:19:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:19:52,153] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:19:52,197] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:19:52,200] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46797 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:19:52,204] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:52,212] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650126 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:19:52,368] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:52,389] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:19:52,389] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:19:52,390] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:19:52,391] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:19:52,397] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:19:52,471] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:19:52,473] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:19:52,474] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46797 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:19:52,476] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:19:52,486] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:19:52,487] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:19:52,533] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:19:52,550] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:19:52,819] {spark_submit.py:495} INFO - 24/09/03 18:19:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455822, partition values: [empty row]
[2024-09-03 18:19:53,041] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO CodeGenerator: Code generated in 135.26783 ms
[2024-09-03 18:19:53,134] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 18:19:53,245] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 627 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:19:53,248] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:19:53,253] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,841 s
[2024-09-03 18:19:53,256] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:19:53,264] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:19:53,273] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,898324 s
[2024-09-03 18:19:53,677] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:19:53,680] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:19:53,680] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:19:53,754] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:53,754] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:53,756] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:53,840] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO CodeGenerator: Code generated in 28.540939 ms
[2024-09-03 18:19:53,889] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO CodeGenerator: Code generated in 33.302497 ms
[2024-09-03 18:19:53,898] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:19:53,906] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:19:53,907] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46797 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:19:53,908] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:53,911] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650126 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:19:53,966] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:53,967] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:19:53,968] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:19:53,968] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:19:53,968] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:19:53,968] {spark_submit.py:495} INFO - 24/09/03 18:19:53 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:19:54,030] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:19:54,032] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:19:54,035] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46797 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:19:54,036] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:19:54,036] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:19:54,037] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:19:54,043] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:19:54,044] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:19:54,100] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:54,101] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:54,101] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:54,165] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO CodeGenerator: Code generated in 24.433465 ms
[2024-09-03 18:19:54,169] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455822, partition values: [empty row]
[2024-09-03 18:19:54,194] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO CodeGenerator: Code generated in 20.647211 ms
[2024-09-03 18:19:54,220] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO CodeGenerator: Code generated in 4.601591 ms
[2024-09-03 18:19:54,345] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: Saved output of task 'attempt_202409031819536489065366850430849_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409031819536489065366850430849_0001_m_000000
[2024-09-03 18:19:54,345] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkHadoopMapRedUtil: attempt_202409031819536489065366850430849_0001_m_000000_1: Committed
[2024-09-03 18:19:54,349] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:19:54,351] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 313 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:19:54,353] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:19:54,354] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,383 s
[2024-09-03 18:19:54,354] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:19:54,354] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:19:54,354] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,387358 s
[2024-09-03 18:19:54,380] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileFormatWriter: Write Job 7b5c67a8-6312-4ab0-ae7a-569f62b7d617 committed.
[2024-09-03 18:19:54,383] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileFormatWriter: Finished processing stats for write job 7b5c67a8-6312-4ab0-ae7a-569f62b7d617.
[2024-09-03 18:19:54,417] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:19:54,417] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:19:54,418] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:19:54,429] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:54,429] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:54,430] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:54,452] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO CodeGenerator: Code generated in 8.690584 ms
[2024-09-03 18:19:54,458] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:19:54,465] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:19:54,466] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46797 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:19:54,466] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:54,469] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650126 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:19:54,489] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:54,490] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:19:54,491] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:19:54,491] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:19:54,491] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:19:54,493] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:19:54,506] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:19:54,514] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:19:54,515] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46797 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:19:54,516] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:19:54,517] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:19:54,518] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:19:54,520] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:19:54,520] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:19:54,534] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:54,535] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:54,535] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:54,565] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO CodeGenerator: Code generated in 11.173973 ms
[2024-09-03 18:19:54,569] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-455822, partition values: [empty row]
[2024-09-03 18:19:54,587] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO CodeGenerator: Code generated in 14.82125 ms
[2024-09-03 18:19:54,636] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileOutputCommitter: Saved output of task 'attempt_202409031819546497851035337276102_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_202409031819546497851035337276102_0002_m_000000
[2024-09-03 18:19:54,636] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkHadoopMapRedUtil: attempt_202409031819546497851035337276102_0002_m_000000_2: Committed
[2024-09-03 18:19:54,637] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:19:54,642] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 122 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:19:54,645] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,152 s
[2024-09-03 18:19:54,645] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:19:54,646] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:19:54,647] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:19:54,647] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,158305 s
[2024-09-03 18:19:54,663] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileFormatWriter: Write Job 07ce85e8-fc3c-472a-897b-9c6b07134c97 committed.
[2024-09-03 18:19:54,663] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO FileFormatWriter: Finished processing stats for write job 07ce85e8-fc3c-472a-897b-9c6b07134c97.
[2024-09-03 18:19:54,702] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:19:54,716] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:19:54,728] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:19:54,752] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:19:54,753] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO BlockManager: BlockManager stopped
[2024-09-03 18:19:54,759] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:19:54,763] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:19:54,767] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:19:54,767] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:19:54,768] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-5029c780-7fa6-4c76-b1dc-9f65187cae7c
[2024-09-03 18:19:54,770] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-57820b94-3729-499e-9f7c-b91bd58a6fed
[2024-09-03 18:19:54,773] {spark_submit.py:495} INFO - 24/09/03 18:19:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-57820b94-3729-499e-9f7c-b91bd58a6fed/pyspark-03e5b07d-e125-4f38-bddf-8792d0ea894f
[2024-09-03 18:19:54,826] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240903T211945, end_date=20240903T211954
[2024-09-03 18:19:54,872] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:19:54,888] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:41:17,413] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 21:41:17,421] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 21:41:17,421] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:41:17,421] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:41:17,421] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:41:17,436] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 21:41:17,440] {standard_task_runner.py:52} INFO - Started process 415379 to run task
[2024-09-03 21:41:17,445] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpftw1vkl2', '--error-file', '/tmp/tmppuj7_q43']
[2024-09-03 21:41:17,445] {standard_task_runner.py:80} INFO - Job 76: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:41:17,487] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:41:17,530] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 21:41:17,533] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:41:17,534] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 21:41:18,655] {spark_submit.py:495} INFO - 24/09/03 21:41:18 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:41:18,656] {spark_submit.py:495} INFO - 24/09/03 21:41:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:41:19,095] {spark_submit.py:495} INFO - 24/09/03 21:41:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:41:19,732] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:41:19,743] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:41:19,820] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO ResourceUtils: ==============================================================
[2024-09-03 21:41:19,821] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:41:19,821] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO ResourceUtils: ==============================================================
[2024-09-03 21:41:19,821] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:41:19,848] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:41:19,861] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:41:19,863] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:41:19,914] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:41:19,915] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:41:19,915] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:41:19,915] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:41:19,915] {spark_submit.py:495} INFO - 24/09/03 21:41:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:41:20,107] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO Utils: Successfully started service 'sparkDriver' on port 41103.
[2024-09-03 21:41:20,145] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:41:20,185] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:41:20,201] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:41:20,202] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:41:20,206] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:41:20,221] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b0006bc6-c7e5-4f3b-806d-659f9bf0430a
[2024-09-03 21:41:20,244] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:41:20,270] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:41:20,513] {spark_submit.py:495} INFO - 24/09/03 21:41:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:41:20,529] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:41:20,679] {spark_submit.py:495} INFO - 24/09/03 21:41:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:41:21,517] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:41:21,549] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34887.
[2024-09-03 21:41:21,549] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO NettyBlockTransferService: Server created on 192.168.2.128:34887
[2024-09-03 21:41:21,552] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:41:21,557] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34887, None)
[2024-09-03 21:41:21,562] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34887 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34887, None)
[2024-09-03 21:41:21,564] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34887, None)
[2024-09-03 21:41:21,567] {spark_submit.py:495} INFO - 24/09/03 21:41:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34887, None)
[2024-09-03 21:41:22,978] {spark_submit.py:495} INFO - 24/09/03 21:41:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:41:22,979] {spark_submit.py:495} INFO - 24/09/03 21:41:22 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:41:24,189] {spark_submit.py:495} INFO - 24/09/03 21:41:24 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
[2024-09-03 21:41:24,312] {spark_submit.py:495} INFO - 24/09/03 21:41:24 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:41:28,347] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:41:28,348] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:41:28,354] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:41:28,765] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:41:28,825] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:41:28,827] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34887 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:41:28,832] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:28,842] {spark_submit.py:495} INFO - 24/09/03 21:41:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198829 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:41:29,026] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:29,049] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:41:29,050] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:41:29,050] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:41:29,052] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:41:29,059] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:41:29,176] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:41:29,179] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:41:29,179] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34887 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:41:29,180] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:41:29,194] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:41:29,196] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:41:29,274] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:41:29,295] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:41:29,597] {spark_submit.py:495} INFO - 24/09/03 21:41:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4525, partition values: [empty row]
[2024-09-03 21:41:30,240] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO CodeGenerator: Code generated in 478.852605 ms
[2024-09-03 21:41:30,335] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 21:41:30,348] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1093 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:41:30,632] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:41:30,642] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,562 s
[2024-09-03 21:41:30,645] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:41:30,646] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:41:30,649] {spark_submit.py:495} INFO - 24/09/03 21:41:30 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,622424 s
[2024-09-03 21:41:31,152] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:41:31,154] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:41:31,154] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:41:31,226] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:31,226] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:31,226] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:31,329] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO CodeGenerator: Code generated in 32.401146 ms
[2024-09-03 21:41:31,367] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO CodeGenerator: Code generated in 23.229232 ms
[2024-09-03 21:41:31,372] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:41:31,400] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:41:31,401] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34887 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:41:31,407] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:31,407] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198829 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:41:31,481] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:31,483] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:41:31,483] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:41:31,483] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:41:31,484] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:41:31,486] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:41:31,550] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:41:31,554] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:41:31,554] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34887 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:41:31,555] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:41:31,556] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:41:31,556] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:41:31,561] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:41:31,562] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:41:31,621] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:31,621] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:31,622] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:31,686] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO CodeGenerator: Code generated in 24.971236 ms
[2024-09-03 21:41:31,691] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4525, partition values: [empty row]
[2024-09-03 21:41:31,716] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO CodeGenerator: Code generated in 22.881123 ms
[2024-09-03 21:41:31,737] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO CodeGenerator: Code generated in 6.886965 ms
[2024-09-03 21:41:31,771] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: Saved output of task 'attempt_202409032141311541174440613533250_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409032141311541174440613533250_0001_m_000000
[2024-09-03 21:41:31,772] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SparkHadoopMapRedUtil: attempt_202409032141311541174440613533250_0001_m_000000_1: Committed
[2024-09-03 21:41:31,787] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:41:31,800] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 232 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:41:31,800] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,303 s
[2024-09-03 21:41:31,800] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:41:31,800] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:41:31,801] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:41:31,805] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,318554 s
[2024-09-03 21:41:31,885] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileFormatWriter: Write Job 9ed2e47c-d408-4401-8456-e12e971c7186 committed.
[2024-09-03 21:41:31,893] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileFormatWriter: Finished processing stats for write job 9ed2e47c-d408-4401-8456-e12e971c7186.
[2024-09-03 21:41:31,941] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:41:31,942] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:41:31,942] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:41:31,959] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:31,960] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:31,960] {spark_submit.py:495} INFO - 24/09/03 21:41:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:32,005] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO CodeGenerator: Code generated in 13.501727 ms
[2024-09-03 21:41:32,010] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:41:32,022] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:41:32,023] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34887 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:41:32,024] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:32,025] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198829 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:41:32,043] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:41:32,044] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:41:32,045] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:41:32,045] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:41:32,045] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:41:32,046] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:41:32,067] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:41:32,070] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:41:32,071] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34887 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:41:32,071] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:41:32,072] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:41:32,072] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:41:32,073] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:41:32,076] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:41:32,092] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:41:32,092] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:41:32,093] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:41:32,128] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO CodeGenerator: Code generated in 8.575181 ms
[2024-09-03 21:41:32,131] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4525, partition values: [empty row]
[2024-09-03 21:41:32,147] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO CodeGenerator: Code generated in 12.788951 ms
[2024-09-03 21:41:32,156] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileOutputCommitter: Saved output of task 'attempt_202409032141324380168967713901848_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_202409032141324380168967713901848_0002_m_000000
[2024-09-03 21:41:32,156] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkHadoopMapRedUtil: attempt_202409032141324380168967713901848_0002_m_000000_2: Committed
[2024-09-03 21:41:32,157] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:41:32,159] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 86 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:41:32,159] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:41:32,161] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,113 s
[2024-09-03 21:41:32,161] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:41:32,162] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:41:32,162] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,117586 s
[2024-09-03 21:41:32,186] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileFormatWriter: Write Job 45a78afb-4ec1-4350-bb49-4788de4197b4 committed.
[2024-09-03 21:41:32,186] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO FileFormatWriter: Finished processing stats for write job 45a78afb-4ec1-4350-bb49-4788de4197b4.
[2024-09-03 21:41:32,218] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:41:32,228] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:41:32,242] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:41:32,275] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:41:32,276] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO BlockManager: BlockManager stopped
[2024-09-03 21:41:32,282] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:41:32,284] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:41:32,290] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:41:32,291] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:41:32,291] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-f144c689-b81f-494b-b62a-01be426c0bef
[2024-09-03 21:41:32,294] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-1a704378-34a7-48b9-b1d4-f5a7643b0634
[2024-09-03 21:41:32,297] {spark_submit.py:495} INFO - 24/09/03 21:41:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-1a704378-34a7-48b9-b1d4-f5a7643b0634/pyspark-c9682f45-3b8f-4f04-8509-64c8c00c60c1
[2024-09-03 21:41:32,389] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240904T004117, end_date=20240904T004132
[2024-09-03 21:41:32,426] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:41:32,445] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:49:17,337] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 22:49:17,344] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [queued]>
[2024-09-03 22:49:17,344] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:49:17,344] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:49:17,344] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:49:17,354] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-20 00:00:00+00:00
[2024-09-03 22:49:17,357] {standard_task_runner.py:52} INFO - Started process 447656 to run task
[2024-09-03 22:49:17,360] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-20T00:00:00+00:00', '--job-id', '121', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnsu14uaz', '--error-file', '/tmp/tmpiz_s_koa']
[2024-09-03 22:49:17,361] {standard_task_runner.py:80} INFO - Job 121: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:49:17,396] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-20T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:49:17,442] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-20T00:00:00+00:00
[2024-09-03 22:49:17,445] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:49:17,446] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240820
[2024-09-03 22:49:18,552] {spark_submit.py:495} INFO - 24/09/03 22:49:18 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:49:18,553] {spark_submit.py:495} INFO - 24/09/03 22:49:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:49:18,885] {spark_submit.py:495} INFO - 24/09/03 22:49:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:49:19,399] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:49:19,407] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:49:19,445] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO ResourceUtils: ==============================================================
[2024-09-03 22:49:19,445] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:49:19,445] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO ResourceUtils: ==============================================================
[2024-09-03 22:49:19,446] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:49:19,467] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:49:19,482] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:49:19,483] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:49:19,536] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:49:19,536] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:49:19,536] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:49:19,536] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:49:19,537] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:49:19,736] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO Utils: Successfully started service 'sparkDriver' on port 45525.
[2024-09-03 22:49:19,764] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:49:19,793] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:49:19,809] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:49:19,809] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:49:19,813] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:49:19,825] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c75f471d-e84c-4d89-833f-d7f55a3ee185
[2024-09-03 22:49:19,844] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:49:19,864] {spark_submit.py:495} INFO - 24/09/03 22:49:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:49:20,058] {spark_submit.py:495} INFO - 24/09/03 22:49:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:49:20,065] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:49:20,111] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:49:20,304] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:49:20,326] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38303.
[2024-09-03 22:49:20,327] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO NettyBlockTransferService: Server created on 192.168.2.128:38303
[2024-09-03 22:49:20,328] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:49:20,333] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38303, None)
[2024-09-03 22:49:20,338] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38303 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38303, None)
[2024-09-03 22:49:20,338] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38303, None)
[2024-09-03 22:49:20,338] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38303, None)
[2024-09-03 22:49:20,760] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:49:20,760] {spark_submit.py:495} INFO - 24/09/03 22:49:20 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:49:21,576] {spark_submit.py:495} INFO - 24/09/03 22:49:21 INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.
[2024-09-03 22:49:21,649] {spark_submit.py:495} INFO - 24/09/03 22:49:21 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2024-09-03 22:49:23,445] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:49:23,446] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:49:23,450] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:49:23,725] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:49:23,783] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:49:23,786] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38303 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:49:23,792] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:49:23,803] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198896 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:49:23,953] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:49:23,969] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:49:23,970] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:49:23,971] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:49:23,972] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:49:23,982] {spark_submit.py:495} INFO - 24/09/03 22:49:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:49:24,099] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:49:24,102] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:49:24,102] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38303 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:49:24,103] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:49:24,116] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:49:24,117] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:49:24,175] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:49:24,195] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:49:24,418] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4592, partition values: [empty row]
[2024-09-03 22:49:24,690] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO CodeGenerator: Code generated in 169.4959 ms
[2024-09-03 22:49:24,727] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 22:49:24,743] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 581 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:49:24,747] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:49:24,752] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,750 s
[2024-09-03 22:49:24,870] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:49:24,871] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:49:24,875] {spark_submit.py:495} INFO - 24/09/03 22:49:24 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,922213 s
[2024-09-03 22:49:25,306] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:49:25,307] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:49:25,308] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:49:25,392] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:49:25,393] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:49:25,395] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:49:25,478] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO CodeGenerator: Code generated in 24.919883 ms
[2024-09-03 22:49:25,525] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO CodeGenerator: Code generated in 28.027829 ms
[2024-09-03 22:49:25,531] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:49:25,539] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:49:25,542] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38303 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:49:25,543] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:49:25,546] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198896 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:49:25,627] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:49:25,628] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:49:25,631] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:49:25,631] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:49:25,631] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:49:25,631] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:49:25,721] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:49:25,724] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:49:25,725] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38303 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:49:25,726] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:49:25,726] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:49:25,726] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:49:25,733] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:49:25,733] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:49:25,799] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:49:25,799] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:49:25,800] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:49:25,881] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO CodeGenerator: Code generated in 35.399948 ms
[2024-09-03 22:49:25,884] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4592, partition values: [empty row]
[2024-09-03 22:49:25,905] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO CodeGenerator: Code generated in 18.607661 ms
[2024-09-03 22:49:25,947] {spark_submit.py:495} INFO - 24/09/03 22:49:25 INFO CodeGenerator: Code generated in 14.129421 ms
[2024-09-03 22:49:26,005] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileOutputCommitter: Saved output of task 'attempt_202409032249251829049276617896562_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240820/_temporary/0/task_202409032249251829049276617896562_0001_m_000000
[2024-09-03 22:49:26,006] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkHadoopMapRedUtil: attempt_202409032249251829049276617896562_0001_m_000000_1: Committed
[2024-09-03 22:49:26,013] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:49:26,020] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 292 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:49:26,020] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:49:26,021] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,391 s
[2024-09-03 22:49:26,022] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:49:26,022] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:49:26,024] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,399020 s
[2024-09-03 22:49:26,042] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileFormatWriter: Write Job 01a0d438-a131-428f-909b-5fda72a2c8a8 committed.
[2024-09-03 22:49:26,048] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileFormatWriter: Finished processing stats for write job 01a0d438-a131-428f-909b-5fda72a2c8a8.
[2024-09-03 22:49:26,109] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:49:26,109] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:49:26,110] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:49:26,145] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:49:26,146] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:49:26,146] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:49:26,183] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO CodeGenerator: Code generated in 11.581282 ms
[2024-09-03 22:49:26,187] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:49:26,196] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:49:26,198] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38303 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:49:26,199] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:49:26,201] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198896 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:49:26,220] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:49:26,224] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:49:26,224] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:49:26,224] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:49:26,225] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:49:26,225] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:49:26,250] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:49:26,252] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:49:26,253] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38303 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:49:26,256] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:49:26,257] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:49:26,257] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:49:26,258] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:49:26,267] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:49:26,271] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:49:26,271] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:49:26,274] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:49:26,312] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO CodeGenerator: Code generated in 12.007174 ms
[2024-09-03 22:49:26,318] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-20T00:00:00.00Z/data_engineer_20240820.json, range: 0-4592, partition values: [empty row]
[2024-09-03 22:49:26,341] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO CodeGenerator: Code generated in 20.533182 ms
[2024-09-03 22:49:26,355] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileOutputCommitter: Saved output of task 'attempt_202409032249263182896240905404722_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240820/_temporary/0/task_202409032249263182896240905404722_0002_m_000000
[2024-09-03 22:49:26,355] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkHadoopMapRedUtil: attempt_202409032249263182896240905404722_0002_m_000000_2: Committed
[2024-09-03 22:49:26,358] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:49:26,361] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 102 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:49:26,364] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,136 s
[2024-09-03 22:49:26,365] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:49:26,368] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:49:26,368] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:49:26,369] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,147871 s
[2024-09-03 22:49:26,396] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileFormatWriter: Write Job d91bd9e1-7adc-4131-87e3-220cc6939f46 committed.
[2024-09-03 22:49:26,397] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO FileFormatWriter: Finished processing stats for write job d91bd9e1-7adc-4131-87e3-220cc6939f46.
[2024-09-03 22:49:26,441] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:49:26,449] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:49:26,467] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:49:26,491] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:49:26,492] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO BlockManager: BlockManager stopped
[2024-09-03 22:49:26,500] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:49:26,505] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:49:26,518] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:49:26,518] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:49:26,519] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-f120ed46-c934-4597-a2d4-7a6ef5bcc583/pyspark-80608721-a60e-453c-930f-aa9ffd1646a6
[2024-09-03 22:49:26,524] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-f120ed46-c934-4597-a2d4-7a6ef5bcc583
[2024-09-03 22:49:26,527] {spark_submit.py:495} INFO - 24/09/03 22:49:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-fadf637a-bbb7-4fef-b159-9017e40b7c5c
[2024-09-03 22:49:26,635] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240820T000000, start_date=20240904T014917, end_date=20240904T014926
[2024-09-03 22:49:26,682] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:49:26,697] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
