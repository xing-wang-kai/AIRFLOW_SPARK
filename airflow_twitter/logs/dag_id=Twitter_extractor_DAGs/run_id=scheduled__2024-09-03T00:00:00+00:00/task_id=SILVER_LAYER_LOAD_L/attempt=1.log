[2024-09-03 21:00:18,392] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:00:18,396] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:00:18,396] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:00:18,396] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:00:18,396] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:00:18,405] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 21:00:18,407] {standard_task_runner.py:52} INFO - Started process 401178 to run task
[2024-09-03 21:00:18,409] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '120', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1ayxgekl', '--error-file', '/tmp/tmpawrqpho9']
[2024-09-03 21:00:18,410] {standard_task_runner.py:80} INFO - Job 120: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:00:18,436] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:00:18,471] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 21:00:18,475] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:00:18,475] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 21:00:19,449] {spark_submit.py:495} INFO - 24/09/03 21:00:19 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:00:19,449] {spark_submit.py:495} INFO - 24/09/03 21:00:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:00:19,730] {spark_submit.py:495} INFO - 24/09/03 21:00:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:00:20,214] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:00:20,218] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:00:20,251] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO ResourceUtils: ==============================================================
[2024-09-03 21:00:20,252] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:00:20,252] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO ResourceUtils: ==============================================================
[2024-09-03 21:00:20,253] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:00:20,278] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:00:20,289] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:00:20,291] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:00:20,327] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:00:20,328] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:00:20,328] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:00:20,329] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:00:20,329] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:00:20,470] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO Utils: Successfully started service 'sparkDriver' on port 44129.
[2024-09-03 21:00:20,493] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:00:20,523] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:00:20,540] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:00:20,541] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:00:20,544] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:00:20,557] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ba6f32c4-af4f-47d8-8e78-489ef617fe4c
[2024-09-03 21:00:20,575] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:00:20,610] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:00:20,797] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 21:00:20,841] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 21:00:20,996] {spark_submit.py:495} INFO - 24/09/03 21:00:20 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:00:21,021] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41057.
[2024-09-03 21:00:21,021] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO NettyBlockTransferService: Server created on 192.168.2.128:41057
[2024-09-03 21:00:21,023] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:00:21,028] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 41057, None)
[2024-09-03 21:00:21,031] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:41057 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 41057, None)
[2024-09-03 21:00:21,032] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 41057, None)
[2024-09-03 21:00:21,033] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 41057, None)
[2024-09-03 21:00:21,457] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:00:21,457] {spark_submit.py:495} INFO - 24/09/03 21:00:21 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:00:22,110] {spark_submit.py:495} INFO - 24/09/03 21:00:22 INFO InMemoryFileIndex: It took 29 ms to list leaf files for 1 paths.
[2024-09-03 21:00:22,157] {spark_submit.py:495} INFO - 24/09/03 21:00:22 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:00:23,697] {spark_submit.py:495} INFO - 24/09/03 21:00:23 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:00:23,698] {spark_submit.py:495} INFO - 24/09/03 21:00:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:00:23,700] {spark_submit.py:495} INFO - 24/09/03 21:00:23 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:00:24,014] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:00:24,064] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:00:24,066] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:41057 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:00:24,073] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:00:24,081] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648519 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:00:24,218] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:00:24,230] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:00:24,231] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:00:24,231] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:00:24,232] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:00:24,238] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:00:24,309] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:00:24,313] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:00:24,319] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:41057 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:00:24,319] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:00:24,331] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:00:24,332] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:00:24,378] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:00:24,392] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:00:24,677] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454215, partition values: [empty row]
[2024-09-03 21:00:24,898] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO CodeGenerator: Code generated in 117.621468 ms
[2024-09-03 21:00:24,974] {spark_submit.py:495} INFO - 24/09/03 21:00:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 21:00:25,075] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 615 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:00:25,080] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:00:25,090] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,840 s
[2024-09-03 21:00:25,093] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:00:25,094] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:00:25,104] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,877268 s
[2024-09-03 21:00:25,456] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:00:25,457] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:00:25,457] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:00:25,518] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:00:25,519] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:00:25,520] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:00:25,598] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO CodeGenerator: Code generated in 22.200075 ms
[2024-09-03 21:00:25,632] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO CodeGenerator: Code generated in 22.780878 ms
[2024-09-03 21:00:25,639] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:00:25,646] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:00:25,647] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:41057 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:00:25,648] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:00:25,650] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648519 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:00:25,700] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:00:25,701] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:00:25,702] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:00:25,702] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:00:25,702] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:00:25,705] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:00:25,742] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:00:25,744] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:00:25,745] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:41057 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:00:25,746] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:00:25,746] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:00:25,747] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:00:25,750] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:00:25,751] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:00:25,804] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:00:25,804] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:00:25,805] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:00:25,857] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO CodeGenerator: Code generated in 17.039648 ms
[2024-09-03 21:00:25,860] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454215, partition values: [empty row]
[2024-09-03 21:00:25,880] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO CodeGenerator: Code generated in 17.165362 ms
[2024-09-03 21:00:25,900] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO CodeGenerator: Code generated in 4.676747 ms
[2024-09-03 21:00:25,992] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO FileOutputCommitter: Saved output of task 'attempt_202409032100253532238069630707580_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032100253532238069630707580_0001_m_000000
[2024-09-03 21:00:25,993] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO SparkHadoopMapRedUtil: attempt_202409032100253532238069630707580_0001_m_000000_1: Committed
[2024-09-03 21:00:25,997] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:00:25,999] {spark_submit.py:495} INFO - 24/09/03 21:00:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 251 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:00:26,002] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,297 s
[2024-09-03 21:00:26,002] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:00:26,003] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:00:26,005] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:00:26,005] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,304088 s
[2024-09-03 21:00:26,029] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileFormatWriter: Write Job a6849398-8bca-4fd1-a356-1edd475465ce committed.
[2024-09-03 21:00:26,030] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileFormatWriter: Finished processing stats for write job a6849398-8bca-4fd1-a356-1edd475465ce.
[2024-09-03 21:00:26,070] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:00:26,071] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:00:26,071] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:00:26,080] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:00:26,080] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:00:26,080] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:00:26,103] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO CodeGenerator: Code generated in 9.010859 ms
[2024-09-03 21:00:26,106] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:00:26,112] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:00:26,113] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:41057 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:00:26,114] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:00:26,115] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648519 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:00:26,128] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:00:26,130] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:00:26,130] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:00:26,130] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:00:26,130] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:00:26,132] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:00:26,157] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:00:26,159] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:00:26,160] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:41057 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:00:26,161] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:00:26,162] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:00:26,162] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:00:26,164] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:00:26,164] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:00:26,175] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:00:26,176] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:00:26,176] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:00:26,202] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO CodeGenerator: Code generated in 9.133642 ms
[2024-09-03 21:00:26,207] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454215, partition values: [empty row]
[2024-09-03 21:00:26,223] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO CodeGenerator: Code generated in 13.329373 ms
[2024-09-03 21:00:26,247] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileOutputCommitter: Saved output of task 'attempt_202409032100266183967809467486117_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032100266183967809467486117_0002_m_000000
[2024-09-03 21:00:26,248] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkHadoopMapRedUtil: attempt_202409032100266183967809467486117_0002_m_000000_2: Committed
[2024-09-03 21:00:26,248] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:00:26,249] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 86 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:00:26,250] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:00:26,253] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,119 s
[2024-09-03 21:00:26,253] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:00:26,253] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:00:26,254] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,125115 s
[2024-09-03 21:00:26,281] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileFormatWriter: Write Job 6a6a795c-ccdb-4797-8aa2-55b8a331a5d8 committed.
[2024-09-03 21:00:26,281] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO FileFormatWriter: Finished processing stats for write job 6a6a795c-ccdb-4797-8aa2-55b8a331a5d8.
[2024-09-03 21:00:26,308] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:00:26,318] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 21:00:26,335] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:00:26,343] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:00:26,344] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO BlockManager: BlockManager stopped
[2024-09-03 21:00:26,356] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:00:26,359] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:00:26,363] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:00:26,364] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:00:26,364] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-a48275ca-3bc1-485f-9fda-3b3a28f68a25
[2024-09-03 21:00:26,367] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c8a3e3f-e941-4cc1-b230-c61b9691bf8d
[2024-09-03 21:00:26,370] {spark_submit.py:495} INFO - 24/09/03 21:00:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-a48275ca-3bc1-485f-9fda-3b3a28f68a25/pyspark-088a2bd3-b89b-4332-8df4-36a12cab5789
[2024-09-03 21:00:26,428] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T000018, end_date=20240904T000026
[2024-09-03 21:00:26,475] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:00:26,498] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:45:14,272] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:45:14,278] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:45:14,278] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:45:14,278] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:45:14,278] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:45:14,288] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 21:45:14,291] {standard_task_runner.py:52} INFO - Started process 419653 to run task
[2024-09-03 21:45:14,295] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '77', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpvjixssho', '--error-file', '/tmp/tmp9xbgx8mi']
[2024-09-03 21:45:14,296] {standard_task_runner.py:80} INFO - Job 77: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:45:14,330] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:45:14,368] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 21:45:14,373] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:45:14,373] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 21:45:15,446] {spark_submit.py:495} INFO - 24/09/03 21:45:15 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:45:15,447] {spark_submit.py:495} INFO - 24/09/03 21:45:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:45:15,779] {spark_submit.py:495} INFO - 24/09/03 21:45:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:45:16,265] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:45:16,266] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:45:16,304] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO ResourceUtils: ==============================================================
[2024-09-03 21:45:16,304] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:45:16,305] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO ResourceUtils: ==============================================================
[2024-09-03 21:45:16,305] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:45:16,324] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:45:16,339] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:45:16,340] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:45:16,377] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:45:16,377] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:45:16,378] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:45:16,378] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:45:16,378] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:45:16,524] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO Utils: Successfully started service 'sparkDriver' on port 42047.
[2024-09-03 21:45:16,546] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:45:16,571] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:45:16,587] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:45:16,588] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:45:16,591] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:45:16,602] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b728d297-295e-49a8-a7cd-fe9d4407d56f
[2024-09-03 21:45:16,620] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:45:16,635] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:45:16,836] {spark_submit.py:495} INFO - 24/09/03 21:45:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:45:16,843] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:45:16,889] {spark_submit.py:495} INFO - 24/09/03 21:45:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:45:17,068] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:45:17,092] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34977.
[2024-09-03 21:45:17,093] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO NettyBlockTransferService: Server created on 192.168.2.128:34977
[2024-09-03 21:45:17,094] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:45:17,101] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34977, None)
[2024-09-03 21:45:17,105] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34977 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34977, None)
[2024-09-03 21:45:17,109] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34977, None)
[2024-09-03 21:45:17,110] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34977, None)
[2024-09-03 21:45:17,560] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:45:17,561] {spark_submit.py:495} INFO - 24/09/03 21:45:17 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:45:18,301] {spark_submit.py:495} INFO - 24/09/03 21:45:18 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2024-09-03 21:45:18,353] {spark_submit.py:495} INFO - 24/09/03 21:45:18 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:45:20,039] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:45:20,040] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:45:20,042] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:45:20,283] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:45:20,323] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:45:20,326] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34977 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:45:20,332] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:20,341] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198745 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:45:20,493] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:20,509] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:45:20,509] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:45:20,510] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:45:20,511] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:45:20,525] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:45:20,623] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:45:20,625] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:45:20,625] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34977 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:45:20,626] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:45:20,636] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:45:20,638] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:45:20,685] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:45:20,698] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:45:20,953] {spark_submit.py:495} INFO - 24/09/03 21:45:20 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4441, partition values: [empty row]
[2024-09-03 21:45:21,200] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO CodeGenerator: Code generated in 144.047274 ms
[2024-09-03 21:45:21,246] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 21:45:21,259] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 583 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:45:21,261] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:45:21,265] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,726 s
[2024-09-03 21:45:21,365] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:45:21,366] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:45:21,367] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,873691 s
[2024-09-03 21:45:21,773] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:45:21,776] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:45:21,776] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:45:21,846] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:21,846] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:21,847] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:21,940] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO CodeGenerator: Code generated in 37.297046 ms
[2024-09-03 21:45:21,989] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO CodeGenerator: Code generated in 30.854429 ms
[2024-09-03 21:45:21,996] {spark_submit.py:495} INFO - 24/09/03 21:45:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:45:22,003] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:45:22,005] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34977 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:45:22,005] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:22,009] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198745 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:45:22,071] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:22,073] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:45:22,073] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:45:22,073] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:45:22,073] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:45:22,076] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:45:22,122] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 21:45:22,125] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:45:22,125] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34977 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:45:22,126] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:45:22,127] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:45:22,127] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:45:22,131] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:45:22,133] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:45:22,192] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:22,192] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:22,193] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:22,259] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO CodeGenerator: Code generated in 25.19728 ms
[2024-09-03 21:45:22,262] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4441, partition values: [empty row]
[2024-09-03 21:45:22,282] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO CodeGenerator: Code generated in 16.251059 ms
[2024-09-03 21:45:22,305] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO CodeGenerator: Code generated in 4.862642 ms
[2024-09-03 21:45:22,339] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_20240903214522766186752647104711_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_20240903214522766186752647104711_0001_m_000000
[2024-09-03 21:45:22,340] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkHadoopMapRedUtil: attempt_20240903214522766186752647104711_0001_m_000000_1: Committed
[2024-09-03 21:45:22,344] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:45:22,353] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 224 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:45:22,353] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,277 s
[2024-09-03 21:45:22,353] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:45:22,354] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:45:22,354] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:45:22,360] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,282956 s
[2024-09-03 21:45:22,377] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileFormatWriter: Write Job dfbba12d-0530-441d-98c4-4e9f7d42b569 committed.
[2024-09-03 21:45:22,383] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileFormatWriter: Finished processing stats for write job dfbba12d-0530-441d-98c4-4e9f7d42b569.
[2024-09-03 21:45:22,441] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:45:22,441] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:45:22,441] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:45:22,457] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:22,458] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:22,458] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:22,523] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO CodeGenerator: Code generated in 24.420403 ms
[2024-09-03 21:45:22,530] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:45:22,542] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:45:22,548] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34977 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:45:22,549] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:22,551] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198745 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:45:22,569] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:22,570] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:45:22,570] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:45:22,570] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:45:22,570] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:45:22,574] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:45:22,595] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:45:22,601] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:45:22,601] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34977 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:45:22,603] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:45:22,605] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:45:22,605] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:45:22,607] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:45:22,614] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:45:22,624] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:22,625] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:22,625] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:22,664] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO CodeGenerator: Code generated in 9.489859 ms
[2024-09-03 21:45:22,680] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4441, partition values: [empty row]
[2024-09-03 21:45:22,696] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO CodeGenerator: Code generated in 12.157782 ms
[2024-09-03 21:45:22,707] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_202409032145225133291548629122767_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032145225133291548629122767_0002_m_000000
[2024-09-03 21:45:22,708] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkHadoopMapRedUtil: attempt_202409032145225133291548629122767_0002_m_000000_2: Committed
[2024-09-03 21:45:22,709] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:45:22,714] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 106 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:45:22,716] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:45:22,716] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,139 s
[2024-09-03 21:45:22,716] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:45:22,717] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:45:22,717] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,146224 s
[2024-09-03 21:45:22,749] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileFormatWriter: Write Job 2f282156-0771-43a8-8f5d-dd99b1781581 committed.
[2024-09-03 21:45:22,750] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO FileFormatWriter: Finished processing stats for write job 2f282156-0771-43a8-8f5d-dd99b1781581.
[2024-09-03 21:45:22,779] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:45:22,789] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:45:22,811] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:45:22,823] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:45:22,824] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO BlockManager: BlockManager stopped
[2024-09-03 21:45:22,831] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:45:22,834] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:45:22,840] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:45:22,841] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:45:22,842] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-35240d59-31bf-4c07-bdfd-7865535d1e2f
[2024-09-03 21:45:22,845] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7576a2a5-c46e-4586-b918-9c8e9e83c732/pyspark-5375c05a-b8dd-4937-9a4c-c7ce70286c8d
[2024-09-03 21:45:22,849] {spark_submit.py:495} INFO - 24/09/03 21:45:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-7576a2a5-c46e-4586-b918-9c8e9e83c732
[2024-09-03 21:45:22,978] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T004514, end_date=20240904T004522
[2024-09-03 21:45:23,002] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:45:23,021] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:50:57,405] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:50:57,412] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:50:57,412] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:50:57,412] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:50:57,412] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:50:57,421] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 21:50:57,426] {standard_task_runner.py:52} INFO - Started process 421166 to run task
[2024-09-03 21:50:57,429] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '77', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_piddyl8', '--error-file', '/tmp/tmpdy26lk0a']
[2024-09-03 21:50:57,431] {standard_task_runner.py:80} INFO - Job 77: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:50:57,462] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:50:57,507] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 21:50:57,511] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:50:57,512] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 21:50:58,577] {spark_submit.py:495} INFO - 24/09/03 21:50:58 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:50:58,577] {spark_submit.py:495} INFO - 24/09/03 21:50:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:50:58,894] {spark_submit.py:495} INFO - 24/09/03 21:50:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:50:59,417] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:50:59,427] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:50:59,470] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO ResourceUtils: ==============================================================
[2024-09-03 21:50:59,471] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:50:59,471] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO ResourceUtils: ==============================================================
[2024-09-03 21:50:59,472] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:50:59,489] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:50:59,503] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:50:59,504] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:50:59,543] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:50:59,544] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:50:59,544] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:50:59,544] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:50:59,545] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:50:59,698] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO Utils: Successfully started service 'sparkDriver' on port 36651.
[2024-09-03 21:50:59,720] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:50:59,745] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:50:59,759] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:50:59,760] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:50:59,763] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:50:59,772] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2be71d5b-5f40-403a-b096-eb7a2197b0dd
[2024-09-03 21:50:59,791] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:50:59,807] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:50:59,980] {spark_submit.py:495} INFO - 24/09/03 21:50:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:50:59,997] {spark_submit.py:495} INFO - 24/09/03 21:50:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:51:00,067] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:51:00,249] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:51:00,273] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42487.
[2024-09-03 21:51:00,274] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO NettyBlockTransferService: Server created on 192.168.2.128:42487
[2024-09-03 21:51:00,275] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:51:00,282] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 42487, None)
[2024-09-03 21:51:00,285] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:42487 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 42487, None)
[2024-09-03 21:51:00,287] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 42487, None)
[2024-09-03 21:51:00,288] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 42487, None)
[2024-09-03 21:51:00,723] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:51:00,724] {spark_submit.py:495} INFO - 24/09/03 21:51:00 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:51:01,461] {spark_submit.py:495} INFO - 24/09/03 21:51:01 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 21:51:01,513] {spark_submit.py:495} INFO - 24/09/03 21:51:01 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:51:03,184] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:51:03,184] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:51:03,187] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:51:03,419] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:51:03,467] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:51:03,469] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:42487 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:51:03,474] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:51:03,480] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198797 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:51:03,612] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:51:03,627] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:51:03,627] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:51:03,627] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:51:03,628] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:51:03,635] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:51:03,757] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:51:03,762] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:51:03,764] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:42487 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:51:03,764] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:51:03,779] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:51:03,780] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:51:03,831] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:51:03,849] {spark_submit.py:495} INFO - 24/09/03 21:51:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:51:04,100] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4493, partition values: [empty row]
[2024-09-03 21:51:04,339] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO CodeGenerator: Code generated in 150.898488 ms
[2024-09-03 21:51:04,385] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 21:51:04,395] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 573 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:51:04,493] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:51:04,494] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,847 s
[2024-09-03 21:51:04,496] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:51:04,497] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:51:04,513] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,888790 s
[2024-09-03 21:51:04,884] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:51:04,886] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:51:04,888] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:51:04,964] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:51:04,964] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:51:04,966] {spark_submit.py:495} INFO - 24/09/03 21:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:51:05,036] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 25.081235 ms
[2024-09-03 21:51:05,080] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 31.016293 ms
[2024-09-03 21:51:05,087] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:51:05,096] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:51:05,098] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:42487 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:51:05,099] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:51:05,102] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198797 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:51:05,161] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:51:05,163] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:51:05,164] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:51:05,164] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:51:05,164] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:51:05,165] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:51:05,210] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:51:05,212] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:51:05,213] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:42487 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:51:05,214] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:51:05,216] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:51:05,216] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:51:05,220] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:51:05,221] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:51:05,273] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:51:05,273] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:51:05,273] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:51:05,340] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 24.791814 ms
[2024-09-03 21:51:05,352] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4493, partition values: [empty row]
[2024-09-03 21:51:05,385] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 29.671847 ms
[2024-09-03 21:51:05,427] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 7.326777 ms
[2024-09-03 21:51:05,478] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409032151052442841177426789466_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032151052442841177426789466_0001_m_000000
[2024-09-03 21:51:05,479] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkHadoopMapRedUtil: attempt_202409032151052442841177426789466_0001_m_000000_1: Committed
[2024-09-03 21:51:05,484] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:51:05,490] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 273 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:51:05,491] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:51:05,493] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,325 s
[2024-09-03 21:51:05,494] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:51:05,494] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:51:05,497] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,333636 s
[2024-09-03 21:51:05,524] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileFormatWriter: Write Job 33ad0e55-dadd-40c9-9269-0b1a4ef5ee63 committed.
[2024-09-03 21:51:05,528] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileFormatWriter: Finished processing stats for write job 33ad0e55-dadd-40c9-9269-0b1a4ef5ee63.
[2024-09-03 21:51:05,593] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:51:05,594] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:51:05,594] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:51:05,617] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:51:05,618] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:51:05,624] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:51:05,674] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 19.34529 ms
[2024-09-03 21:51:05,678] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:51:05,687] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:51:05,688] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:42487 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:51:05,689] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:51:05,689] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198797 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:51:05,705] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:51:05,706] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:51:05,706] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:51:05,706] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:51:05,706] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:51:05,707] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:51:05,734] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:51:05,737] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:51:05,738] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:42487 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:51:05,739] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:51:05,739] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:51:05,739] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:51:05,740] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:51:05,741] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:51:05,754] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:51:05,754] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:51:05,754] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:51:05,789] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 12.060063 ms
[2024-09-03 21:51:05,793] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4493, partition values: [empty row]
[2024-09-03 21:51:05,821] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO CodeGenerator: Code generated in 24.774927 ms
[2024-09-03 21:51:05,836] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409032151054886686101637067592_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032151054886686101637067592_0002_m_000000
[2024-09-03 21:51:05,836] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkHadoopMapRedUtil: attempt_202409032151054886686101637067592_0002_m_000000_2: Committed
[2024-09-03 21:51:05,837] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2777 bytes result sent to driver
[2024-09-03 21:51:05,850] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 110 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:51:05,851] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:51:05,852] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,144 s
[2024-09-03 21:51:05,852] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:51:05,852] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:51:05,853] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,147167 s
[2024-09-03 21:51:05,878] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.2.128:42487 in memory (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:51:05,915] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileFormatWriter: Write Job 9326eac9-f50e-419d-a253-e52fcede9d20 committed.
[2024-09-03 21:51:05,915] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO FileFormatWriter: Finished processing stats for write job 9326eac9-f50e-419d-a253-e52fcede9d20.
[2024-09-03 21:51:05,929] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.128:42487 in memory (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:51:05,953] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:51:05,962] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:51:05,977] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:51:05,990] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:51:05,991] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManager: BlockManager stopped
[2024-09-03 21:51:05,995] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:51:05,999] {spark_submit.py:495} INFO - 24/09/03 21:51:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:51:06,004] {spark_submit.py:495} INFO - 24/09/03 21:51:06 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:51:06,004] {spark_submit.py:495} INFO - 24/09/03 21:51:06 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:51:06,005] {spark_submit.py:495} INFO - 24/09/03 21:51:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-091c9f29-8266-4368-b754-baea088c15a3
[2024-09-03 21:51:06,008] {spark_submit.py:495} INFO - 24/09/03 21:51:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-091c9f29-8266-4368-b754-baea088c15a3/pyspark-1df5c327-e176-4918-b61a-fe197c63952e
[2024-09-03 21:51:06,011] {spark_submit.py:495} INFO - 24/09/03 21:51:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-b089a436-4981-49cf-8bcb-0164372b20f3
[2024-09-03 21:51:06,052] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T005057, end_date=20240904T005106
[2024-09-03 21:51:06,094] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:51:06,110] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:57:32,688] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:57:32,696] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 21:57:32,696] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:57:32,696] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:57:32,696] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:57:32,706] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 21:57:32,708] {standard_task_runner.py:52} INFO - Started process 423292 to run task
[2024-09-03 21:57:32,712] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '77', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpgyp6_bkt', '--error-file', '/tmp/tmpdipg1ryc']
[2024-09-03 21:57:32,712] {standard_task_runner.py:80} INFO - Job 77: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:57:32,746] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:57:32,785] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 21:57:32,790] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:57:32,790] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 21:57:33,866] {spark_submit.py:495} INFO - 24/09/03 21:57:33 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:57:33,866] {spark_submit.py:495} INFO - 24/09/03 21:57:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:57:34,174] {spark_submit.py:495} INFO - 24/09/03 21:57:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:57:34,733] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:57:34,747] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:57:34,792] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO ResourceUtils: ==============================================================
[2024-09-03 21:57:34,792] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:57:34,792] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO ResourceUtils: ==============================================================
[2024-09-03 21:57:34,793] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:57:34,816] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:57:34,829] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:57:34,830] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:57:34,871] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:57:34,872] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:57:34,872] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:57:34,872] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:57:34,873] {spark_submit.py:495} INFO - 24/09/03 21:57:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:57:35,033] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO Utils: Successfully started service 'sparkDriver' on port 39391.
[2024-09-03 21:57:35,057] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:57:35,082] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:57:35,097] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:57:35,097] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:57:35,100] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:57:35,112] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f3549df3-18bb-415d-882f-00e413e5afb7
[2024-09-03 21:57:35,130] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:57:35,144] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:57:35,329] {spark_submit.py:495} INFO - 24/09/03 21:57:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:57:35,337] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:57:35,381] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:57:35,592] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:57:35,612] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38411.
[2024-09-03 21:57:35,612] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO NettyBlockTransferService: Server created on 192.168.2.128:38411
[2024-09-03 21:57:35,614] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:57:35,622] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38411, None)
[2024-09-03 21:57:35,625] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38411 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38411, None)
[2024-09-03 21:57:35,626] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38411, None)
[2024-09-03 21:57:35,627] {spark_submit.py:495} INFO - 24/09/03 21:57:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38411, None)
[2024-09-03 21:57:36,086] {spark_submit.py:495} INFO - 24/09/03 21:57:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:57:36,086] {spark_submit.py:495} INFO - 24/09/03 21:57:36 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:57:36,853] {spark_submit.py:495} INFO - 24/09/03 21:57:36 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 21:57:36,910] {spark_submit.py:495} INFO - 24/09/03 21:57:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:57:38,701] {spark_submit.py:495} INFO - 24/09/03 21:57:38 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:57:38,702] {spark_submit.py:495} INFO - 24/09/03 21:57:38 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:57:38,706] {spark_submit.py:495} INFO - 24/09/03 21:57:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:57:38,960] {spark_submit.py:495} INFO - 24/09/03 21:57:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:57:39,008] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:57:39,011] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38411 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:57:39,018] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:39,026] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198843 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:57:39,175] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:39,191] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:57:39,192] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:57:39,193] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:57:39,194] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:57:39,200] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:57:39,287] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:57:39,291] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:57:39,291] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38411 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:57:39,292] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:57:39,303] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:57:39,304] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:57:39,345] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:57:39,367] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:57:39,630] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4539, partition values: [empty row]
[2024-09-03 21:57:39,852] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO CodeGenerator: Code generated in 138.253334 ms
[2024-09-03 21:57:39,896] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 21:57:39,910] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 572 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:57:39,912] {spark_submit.py:495} INFO - 24/09/03 21:57:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:57:40,001] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,782 s
[2024-09-03 21:57:40,005] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:57:40,006] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:57:40,011] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,832291 s
[2024-09-03 21:57:40,379] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:57:40,380] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:57:40,381] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:57:40,473] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:40,474] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:40,474] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:40,551] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO CodeGenerator: Code generated in 25.486344 ms
[2024-09-03 21:57:40,596] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO CodeGenerator: Code generated in 32.368867 ms
[2024-09-03 21:57:40,603] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:57:40,609] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:57:40,609] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38411 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:57:40,611] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:40,613] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198843 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:57:40,681] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:40,682] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:57:40,682] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:57:40,682] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:57:40,683] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:57:40,687] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:57:40,746] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:57:40,749] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:57:40,751] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38411 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:57:40,752] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:57:40,753] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:57:40,753] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:57:40,757] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:57:40,759] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:57:40,812] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:40,813] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:40,813] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:40,872] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO CodeGenerator: Code generated in 18.216873 ms
[2024-09-03 21:57:40,876] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4539, partition values: [empty row]
[2024-09-03 21:57:40,899] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO CodeGenerator: Code generated in 19.447344 ms
[2024-09-03 21:57:40,921] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO CodeGenerator: Code generated in 5.283063 ms
[2024-09-03 21:57:40,957] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileOutputCommitter: Saved output of task 'attempt_202409032157407977812956661012688_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032157407977812956661012688_0001_m_000000
[2024-09-03 21:57:40,957] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO SparkHadoopMapRedUtil: attempt_202409032157407977812956661012688_0001_m_000000_1: Committed
[2024-09-03 21:57:40,961] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:57:40,962] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 208 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:57:40,963] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:57:40,963] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,278 s
[2024-09-03 21:57:40,963] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:57:40,963] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:57:40,964] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,283129 s
[2024-09-03 21:57:40,985] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileFormatWriter: Write Job a48c627b-81a6-4b59-b206-94ac252ac4ad committed.
[2024-09-03 21:57:40,988] {spark_submit.py:495} INFO - 24/09/03 21:57:40 INFO FileFormatWriter: Finished processing stats for write job a48c627b-81a6-4b59-b206-94ac252ac4ad.
[2024-09-03 21:57:41,025] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:57:41,025] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:57:41,026] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:57:41,034] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:41,034] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:41,035] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:41,060] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO CodeGenerator: Code generated in 10.163323 ms
[2024-09-03 21:57:41,064] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:57:41,071] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:57:41,071] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38411 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:57:41,072] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:41,073] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198843 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:57:41,087] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:41,088] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:57:41,089] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:57:41,089] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:57:41,089] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:57:41,089] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:57:41,107] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:57:41,109] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:57:41,110] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38411 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:57:41,110] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:57:41,111] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:57:41,111] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:57:41,112] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:57:41,113] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:57:41,125] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:41,125] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:41,125] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:41,168] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO CodeGenerator: Code generated in 22.83434 ms
[2024-09-03 21:57:41,171] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4539, partition values: [empty row]
[2024-09-03 21:57:41,184] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO CodeGenerator: Code generated in 10.191852 ms
[2024-09-03 21:57:41,194] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileOutputCommitter: Saved output of task 'attempt_202409032157413965582577251642042_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032157413965582577251642042_0002_m_000000
[2024-09-03 21:57:41,194] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkHadoopMapRedUtil: attempt_202409032157413965582577251642042_0002_m_000000_2: Committed
[2024-09-03 21:57:41,194] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:57:41,196] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 83 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:57:41,196] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,106 s
[2024-09-03 21:57:41,197] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:57:41,197] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:57:41,197] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:57:41,197] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,109548 s
[2024-09-03 21:57:41,217] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileFormatWriter: Write Job 74ebb557-93aa-4408-953d-e25c4077ab70 committed.
[2024-09-03 21:57:41,217] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO FileFormatWriter: Finished processing stats for write job 74ebb557-93aa-4408-953d-e25c4077ab70.
[2024-09-03 21:57:41,256] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:57:41,270] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:57:41,284] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:57:41,296] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:57:41,296] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO BlockManager: BlockManager stopped
[2024-09-03 21:57:41,304] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:57:41,306] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:57:41,314] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:57:41,314] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:57:41,314] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-d52825ea-d3de-4d10-a69f-9d09eafcbd80
[2024-09-03 21:57:41,318] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c9d5194-3466-406c-a1c7-6793d7d4dcbd/pyspark-4b9107dc-ffe9-414f-aa32-928c7093d125
[2024-09-03 21:57:41,321] {spark_submit.py:495} INFO - 24/09/03 21:57:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c9d5194-3466-406c-a1c7-6793d7d4dcbd
[2024-09-03 21:57:41,404] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T005732, end_date=20240904T005741
[2024-09-03 21:57:41,434] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:57:41,448] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:01:22,567] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:01:22,574] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:01:22,574] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:01:22,575] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:01:22,575] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:01:22,588] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 22:01:22,590] {standard_task_runner.py:52} INFO - Started process 425304 to run task
[2024-09-03 22:01:22,593] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpuiprk22y', '--error-file', '/tmp/tmpe24r63m_']
[2024-09-03 22:01:22,594] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:01:22,639] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:01:22,704] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 22:01:22,708] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:01:22,709] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 22:01:23,740] {spark_submit.py:495} INFO - 24/09/03 22:01:23 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:01:23,741] {spark_submit.py:495} INFO - 24/09/03 22:01:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:01:24,050] {spark_submit.py:495} INFO - 24/09/03 22:01:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:01:24,588] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:01:24,597] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:01:24,631] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO ResourceUtils: ==============================================================
[2024-09-03 22:01:24,632] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:01:24,632] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO ResourceUtils: ==============================================================
[2024-09-03 22:01:24,633] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:01:24,650] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:01:24,663] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:01:24,663] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:01:24,705] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:01:24,706] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:01:24,706] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:01:24,706] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:01:24,706] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:01:24,852] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO Utils: Successfully started service 'sparkDriver' on port 36225.
[2024-09-03 22:01:24,874] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:01:24,903] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:01:24,920] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:01:24,921] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:01:24,924] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:01:24,936] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e4fc8a9f-57e0-4e8e-878e-f10f0b8c8384
[2024-09-03 22:01:24,955] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:01:24,982] {spark_submit.py:495} INFO - 24/09/03 22:01:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:01:25,170] {spark_submit.py:495} INFO - 24/09/03 22:01:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:01:25,180] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:01:25,226] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:01:25,415] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:01:25,440] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34819.
[2024-09-03 22:01:25,440] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO NettyBlockTransferService: Server created on 192.168.2.128:34819
[2024-09-03 22:01:25,442] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:01:25,449] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34819, None)
[2024-09-03 22:01:25,453] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34819 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34819, None)
[2024-09-03 22:01:25,458] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34819, None)
[2024-09-03 22:01:25,458] {spark_submit.py:495} INFO - 24/09/03 22:01:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34819, None)
[2024-09-03 22:01:26,075] {spark_submit.py:495} INFO - 24/09/03 22:01:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:01:26,075] {spark_submit.py:495} INFO - 24/09/03 22:01:26 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:01:26,959] {spark_submit.py:495} INFO - 24/09/03 22:01:26 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2024-09-03 22:01:27,012] {spark_submit.py:495} INFO - 24/09/03 22:01:27 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:01:28,818] {spark_submit.py:495} INFO - 24/09/03 22:01:28 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:01:28,819] {spark_submit.py:495} INFO - 24/09/03 22:01:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:01:28,830] {spark_submit.py:495} INFO - 24/09/03 22:01:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:01:29,098] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:01:29,155] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:01:29,159] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34819 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:01:29,165] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:29,174] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198847 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:01:29,318] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:29,335] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:01:29,336] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:01:29,336] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:01:29,338] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:01:29,343] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:01:29,463] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:01:29,466] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:01:29,467] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34819 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:01:29,468] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:01:29,478] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:01:29,479] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:01:29,526] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:01:29,544] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:01:29,783] {spark_submit.py:495} INFO - 24/09/03 22:01:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4543, partition values: [empty row]
[2024-09-03 22:01:30,031] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO CodeGenerator: Code generated in 147.487483 ms
[2024-09-03 22:01:30,076] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 22:01:30,090] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 572 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:01:30,203] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:01:30,206] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,838 s
[2024-09-03 22:01:30,208] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:01:30,209] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:01:30,214] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,891498 s
[2024-09-03 22:01:30,600] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:01:30,601] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:01:30,602] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:01:30,676] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:30,676] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:30,677] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:30,767] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO CodeGenerator: Code generated in 36.497095 ms
[2024-09-03 22:01:30,806] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO CodeGenerator: Code generated in 27.400289 ms
[2024-09-03 22:01:30,811] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:01:30,819] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:01:30,819] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34819 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:01:30,820] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:30,826] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198847 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:01:30,882] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:30,884] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:01:30,884] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:01:30,885] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:01:30,885] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:01:30,885] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:01:30,941] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 22:01:30,943] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:01:30,943] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34819 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:01:30,944] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:01:30,944] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:01:30,945] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:01:30,950] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:01:30,951] {spark_submit.py:495} INFO - 24/09/03 22:01:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:01:31,009] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:31,010] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:31,010] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:31,068] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO CodeGenerator: Code generated in 19.750861 ms
[2024-09-03 22:01:31,072] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4543, partition values: [empty row]
[2024-09-03 22:01:31,090] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO CodeGenerator: Code generated in 15.184566 ms
[2024-09-03 22:01:31,114] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO CodeGenerator: Code generated in 5.89712 ms
[2024-09-03 22:01:31,146] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: Saved output of task 'attempt_20240903220130684392475068724019_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_20240903220130684392475068724019_0001_m_000000
[2024-09-03 22:01:31,148] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkHadoopMapRedUtil: attempt_20240903220130684392475068724019_0001_m_000000_1: Committed
[2024-09-03 22:01:31,152] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:01:31,155] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 210 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:01:31,155] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:01:31,156] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,270 s
[2024-09-03 22:01:31,156] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:01:31,156] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:01:31,157] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,275167 s
[2024-09-03 22:01:31,178] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileFormatWriter: Write Job 37350c09-dcab-4baa-813e-b4e3303fadca committed.
[2024-09-03 22:01:31,181] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileFormatWriter: Finished processing stats for write job 37350c09-dcab-4baa-813e-b4e3303fadca.
[2024-09-03 22:01:31,240] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:01:31,241] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:01:31,241] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:01:31,251] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:31,251] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:31,251] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:31,293] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO CodeGenerator: Code generated in 12.797446 ms
[2024-09-03 22:01:31,296] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:01:31,306] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:01:31,307] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34819 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:01:31,309] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:31,310] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198847 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:01:31,326] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:31,327] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:01:31,327] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:01:31,327] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:01:31,327] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:01:31,328] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:01:31,348] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:01:31,353] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:01:31,354] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34819 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:01:31,356] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:01:31,357] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:01:31,358] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:01:31,359] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:01:31,359] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:01:31,371] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:31,371] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:31,381] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:31,417] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO CodeGenerator: Code generated in 10.73439 ms
[2024-09-03 22:01:31,421] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4543, partition values: [empty row]
[2024-09-03 22:01:31,436] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO CodeGenerator: Code generated in 12.437505 ms
[2024-09-03 22:01:31,452] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileOutputCommitter: Saved output of task 'attempt_202409032201318676600569056009066_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032201318676600569056009066_0002_m_000000
[2024-09-03 22:01:31,452] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkHadoopMapRedUtil: attempt_202409032201318676600569056009066_0002_m_000000_2: Committed
[2024-09-03 22:01:31,453] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:01:31,456] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 98 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:01:31,456] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:01:31,457] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,127 s
[2024-09-03 22:01:31,457] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:01:31,457] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:01:31,458] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,131896 s
[2024-09-03 22:01:31,477] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileFormatWriter: Write Job 166873d4-b58c-49e6-94e3-714d1953b566 committed.
[2024-09-03 22:01:31,478] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO FileFormatWriter: Finished processing stats for write job 166873d4-b58c-49e6-94e3-714d1953b566.
[2024-09-03 22:01:31,515] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:01:31,524] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:01:31,545] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:01:31,563] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:01:31,563] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO BlockManager: BlockManager stopped
[2024-09-03 22:01:31,570] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:01:31,575] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:01:31,583] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:01:31,584] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:01:31,584] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3d8d1ba-3339-4c2a-a9f7-e366935d0a98
[2024-09-03 22:01:31,588] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3d8d1ba-3339-4c2a-a9f7-e366935d0a98/pyspark-cac74d17-9a37-4149-9e1a-30ce58f8b41a
[2024-09-03 22:01:31,592] {spark_submit.py:495} INFO - 24/09/03 22:01:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-21f0e47a-1269-4a63-a881-701a3dd972ac
[2024-09-03 22:01:31,663] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T010122, end_date=20240904T010131
[2024-09-03 22:01:31,690] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:01:31,705] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:04:43,827] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:04:43,832] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:04:43,832] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:04:43,833] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:04:43,833] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:04:43,843] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 22:04:43,845] {standard_task_runner.py:52} INFO - Started process 427104 to run task
[2024-09-03 22:04:43,849] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmph1zq0u6h', '--error-file', '/tmp/tmp33hjin86']
[2024-09-03 22:04:43,849] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:04:43,887] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:04:43,925] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 22:04:43,928] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:04:43,930] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 22:04:45,001] {spark_submit.py:495} INFO - 24/09/03 22:04:45 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:04:45,002] {spark_submit.py:495} INFO - 24/09/03 22:04:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:04:45,330] {spark_submit.py:495} INFO - 24/09/03 22:04:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:04:45,877] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:04:45,890] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:04:45,925] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO ResourceUtils: ==============================================================
[2024-09-03 22:04:45,926] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:04:45,926] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO ResourceUtils: ==============================================================
[2024-09-03 22:04:45,927] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:04:45,952] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:04:45,964] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:04:45,964] {spark_submit.py:495} INFO - 24/09/03 22:04:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:04:46,005] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:04:46,006] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:04:46,006] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:04:46,006] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:04:46,006] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:04:46,172] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO Utils: Successfully started service 'sparkDriver' on port 38535.
[2024-09-03 22:04:46,195] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:04:46,228] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:04:46,245] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:04:46,247] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:04:46,250] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:04:46,262] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-438dc436-2c66-4a95-b6ab-77a9c97a9f04
[2024-09-03 22:04:46,281] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:04:46,293] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:04:46,488] {spark_submit.py:495} INFO - 24/09/03 22:04:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:04:46,491] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:04:46,547] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:04:46,753] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:04:46,777] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40569.
[2024-09-03 22:04:46,778] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO NettyBlockTransferService: Server created on 192.168.2.128:40569
[2024-09-03 22:04:46,780] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:04:46,785] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40569, None)
[2024-09-03 22:04:46,789] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40569 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40569, None)
[2024-09-03 22:04:46,792] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40569, None)
[2024-09-03 22:04:46,794] {spark_submit.py:495} INFO - 24/09/03 22:04:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40569, None)
[2024-09-03 22:04:47,309] {spark_submit.py:495} INFO - 24/09/03 22:04:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:04:47,310] {spark_submit.py:495} INFO - 24/09/03 22:04:47 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:04:48,149] {spark_submit.py:495} INFO - 24/09/03 22:04:48 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 22:04:48,209] {spark_submit.py:495} INFO - 24/09/03 22:04:48 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:04:49,870] {spark_submit.py:495} INFO - 24/09/03 22:04:49 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:04:49,871] {spark_submit.py:495} INFO - 24/09/03 22:04:49 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:04:49,874] {spark_submit.py:495} INFO - 24/09/03 22:04:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:04:50,122] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:04:50,173] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:04:50,175] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40569 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:04:50,184] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:50,191] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198849 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:04:50,343] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:50,358] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:04:50,358] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:04:50,359] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:04:50,360] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:04:50,366] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:04:50,437] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:04:50,441] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:04:50,442] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40569 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:04:50,443] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:04:50,456] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:04:50,457] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:04:50,514] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:04:50,533] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:04:50,885] {spark_submit.py:495} INFO - 24/09/03 22:04:50 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4545, partition values: [empty row]
[2024-09-03 22:04:51,115] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO CodeGenerator: Code generated in 134.139337 ms
[2024-09-03 22:04:51,159] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 22:04:51,180] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 681 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:04:51,182] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:04:51,280] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,900 s
[2024-09-03 22:04:51,292] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:04:51,292] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:04:51,306] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,962125 s
[2024-09-03 22:04:51,676] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:04:51,679] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:04:51,680] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:04:51,754] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:51,755] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:51,755] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:51,839] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO CodeGenerator: Code generated in 24.357741 ms
[2024-09-03 22:04:51,873] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO CodeGenerator: Code generated in 20.515568 ms
[2024-09-03 22:04:51,883] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:04:51,890] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:04:51,890] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40569 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:04:51,891] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:51,894] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198849 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:04:51,959] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:51,962] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:04:51,962] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:04:51,962] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:04:51,963] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:04:51,963] {spark_submit.py:495} INFO - 24/09/03 22:04:51 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:04:52,017] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:04:52,019] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:04:52,020] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40569 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:04:52,020] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:04:52,022] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:04:52,023] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:04:52,027] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:04:52,028] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:04:52,081] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:52,082] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:52,082] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:52,145] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO CodeGenerator: Code generated in 17.507547 ms
[2024-09-03 22:04:52,157] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4545, partition values: [empty row]
[2024-09-03 22:04:52,189] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO CodeGenerator: Code generated in 27.779515 ms
[2024-09-03 22:04:52,222] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO CodeGenerator: Code generated in 4.572703 ms
[2024-09-03 22:04:52,258] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409032204518189184090042149851_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032204518189184090042149851_0001_m_000000
[2024-09-03 22:04:52,258] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkHadoopMapRedUtil: attempt_202409032204518189184090042149851_0001_m_000000_1: Committed
[2024-09-03 22:04:52,263] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:04:52,269] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 245 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:04:52,270] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,306 s
[2024-09-03 22:04:52,270] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:04:52,271] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:04:52,272] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:04:52,272] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,312628 s
[2024-09-03 22:04:52,306] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileFormatWriter: Write Job 2f6194e0-4051-41b1-9e2c-4f4a1cb2915f committed.
[2024-09-03 22:04:52,310] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileFormatWriter: Finished processing stats for write job 2f6194e0-4051-41b1-9e2c-4f4a1cb2915f.
[2024-09-03 22:04:52,365] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:04:52,366] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:04:52,366] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:04:52,377] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:52,377] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:52,377] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:52,411] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO CodeGenerator: Code generated in 15.795758 ms
[2024-09-03 22:04:52,416] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:04:52,422] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:04:52,423] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40569 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:04:52,424] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:52,427] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198849 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:04:52,446] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:52,448] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:04:52,448] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:04:52,448] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:04:52,448] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:04:52,450] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:04:52,470] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:04:52,475] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:04:52,476] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40569 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:04:52,479] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:04:52,480] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:04:52,480] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:04:52,485] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:04:52,485] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:04:52,498] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:52,498] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:52,498] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:52,545] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO CodeGenerator: Code generated in 9.591628 ms
[2024-09-03 22:04:52,548] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4545, partition values: [empty row]
[2024-09-03 22:04:52,564] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO CodeGenerator: Code generated in 14.055043 ms
[2024-09-03 22:04:52,573] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409032204522181430822343652_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032204522181430822343652_0002_m_000000
[2024-09-03 22:04:52,573] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkHadoopMapRedUtil: attempt_202409032204522181430822343652_0002_m_000000_2: Committed
[2024-09-03 22:04:52,575] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:04:52,578] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 95 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:04:52,579] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,127 s
[2024-09-03 22:04:52,580] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:04:52,583] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:04:52,583] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:04:52,584] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,136921 s
[2024-09-03 22:04:52,594] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileFormatWriter: Write Job 6352389a-1387-42f6-919d-de31c42d483c committed.
[2024-09-03 22:04:52,596] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO FileFormatWriter: Finished processing stats for write job 6352389a-1387-42f6-919d-de31c42d483c.
[2024-09-03 22:04:52,634] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:04:52,641] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:04:52,654] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:04:52,665] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:04:52,665] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO BlockManager: BlockManager stopped
[2024-09-03 22:04:52,684] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:04:52,686] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:04:52,700] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:04:52,700] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:04:52,700] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-08d1de59-1b5c-4ad8-a07d-04e4f87d4586
[2024-09-03 22:04:52,702] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-19afb913-a25f-4df7-a019-759d97724d80/pyspark-49a37cd7-2bab-4d7d-8323-19d304c63489
[2024-09-03 22:04:52,704] {spark_submit.py:495} INFO - 24/09/03 22:04:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-19afb913-a25f-4df7-a019-759d97724d80
[2024-09-03 22:04:52,765] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T010443, end_date=20240904T010452
[2024-09-03 22:04:52,807] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:04:52,844] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:15:37,758] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:15:37,765] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:15:37,765] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:15:37,765] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:15:37,766] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:15:37,779] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 22:15:37,781] {standard_task_runner.py:52} INFO - Started process 428929 to run task
[2024-09-03 22:15:37,785] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnsm2_gie', '--error-file', '/tmp/tmp2nhiaa73']
[2024-09-03 22:15:37,785] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:15:37,817] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:15:37,859] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 22:15:37,863] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:15:37,864] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 22:15:38,934] {spark_submit.py:495} INFO - 24/09/03 22:15:38 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:15:38,935] {spark_submit.py:495} INFO - 24/09/03 22:15:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:15:39,260] {spark_submit.py:495} INFO - 24/09/03 22:15:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:15:39,786] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:15:39,798] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:15:39,835] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO ResourceUtils: ==============================================================
[2024-09-03 22:15:39,835] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:15:39,836] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO ResourceUtils: ==============================================================
[2024-09-03 22:15:39,836] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:15:39,860] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:15:39,871] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:15:39,872] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:15:39,909] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:15:39,909] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:15:39,910] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:15:39,910] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:15:39,910] {spark_submit.py:495} INFO - 24/09/03 22:15:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:15:40,061] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO Utils: Successfully started service 'sparkDriver' on port 44123.
[2024-09-03 22:15:40,085] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:15:40,113] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:15:40,130] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:15:40,131] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:15:40,134] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:15:40,145] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-79239b84-8412-4a16-a077-01e43102a16a
[2024-09-03 22:15:40,164] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:15:40,181] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:15:40,367] {spark_submit.py:495} INFO - 24/09/03 22:15:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:15:40,375] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:15:40,424] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:15:40,604] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:15:40,626] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46277.
[2024-09-03 22:15:40,626] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO NettyBlockTransferService: Server created on 192.168.2.128:46277
[2024-09-03 22:15:40,627] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:15:40,631] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46277, None)
[2024-09-03 22:15:40,635] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46277 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46277, None)
[2024-09-03 22:15:40,639] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46277, None)
[2024-09-03 22:15:40,640] {spark_submit.py:495} INFO - 24/09/03 22:15:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46277, None)
[2024-09-03 22:15:41,083] {spark_submit.py:495} INFO - 24/09/03 22:15:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:15:41,083] {spark_submit.py:495} INFO - 24/09/03 22:15:41 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:15:41,850] {spark_submit.py:495} INFO - 24/09/03 22:15:41 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 22:15:41,904] {spark_submit.py:495} INFO - 24/09/03 22:15:41 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 22:15:43,688] {spark_submit.py:495} INFO - 24/09/03 22:15:43 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:15:43,688] {spark_submit.py:495} INFO - 24/09/03 22:15:43 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:15:43,691] {spark_submit.py:495} INFO - 24/09/03 22:15:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:15:43,931] {spark_submit.py:495} INFO - 24/09/03 22:15:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:15:43,996] {spark_submit.py:495} INFO - 24/09/03 22:15:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:15:43,999] {spark_submit.py:495} INFO - 24/09/03 22:15:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46277 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:15:44,004] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:44,011] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650244 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:15:44,167] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:44,182] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:15:44,183] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:15:44,183] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:15:44,184] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:15:44,189] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:15:44,282] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:15:44,284] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:15:44,285] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46277 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:15:44,286] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:15:44,297] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:15:44,298] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:15:44,343] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:15:44,359] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:15:44,586] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-455940, partition values: [empty row]
[2024-09-03 22:15:44,799] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO CodeGenerator: Code generated in 127.586831 ms
[2024-09-03 22:15:44,882] {spark_submit.py:495} INFO - 24/09/03 22:15:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 22:15:45,018] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 562 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:15:45,027] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:15:45,050] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,838 s
[2024-09-03 22:15:45,056] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:15:45,057] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:15:45,064] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,892689 s
[2024-09-03 22:15:45,448] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:15:45,450] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:15:45,452] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:15:45,528] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:45,528] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:45,529] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:45,603] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO CodeGenerator: Code generated in 24.812409 ms
[2024-09-03 22:15:45,645] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO CodeGenerator: Code generated in 28.490451 ms
[2024-09-03 22:15:45,652] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:15:45,660] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:15:45,661] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46277 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:15:45,661] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:45,663] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650244 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:15:45,717] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:45,719] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:15:45,719] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:15:45,719] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:15:45,721] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:15:45,722] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:15:45,774] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:15:45,778] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:15:45,779] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46277 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:15:45,780] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:15:45,781] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:15:45,781] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:15:45,785] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:15:45,786] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:15:45,848] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:45,848] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:45,848] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:45,929] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO CodeGenerator: Code generated in 37.806998 ms
[2024-09-03 22:15:45,932] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-455940, partition values: [empty row]
[2024-09-03 22:15:45,952] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO CodeGenerator: Code generated in 16.955125 ms
[2024-09-03 22:15:45,973] {spark_submit.py:495} INFO - 24/09/03 22:15:45 INFO CodeGenerator: Code generated in 5.469683 ms
[2024-09-03 22:15:46,092] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileOutputCommitter: Saved output of task 'attempt_202409032215452229188343567337291_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032215452229188343567337291_0001_m_000000
[2024-09-03 22:15:46,093] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkHadoopMapRedUtil: attempt_202409032215452229188343567337291_0001_m_000000_1: Committed
[2024-09-03 22:15:46,096] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:15:46,098] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 316 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:15:46,099] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:15:46,102] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,378 s
[2024-09-03 22:15:46,104] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:15:46,105] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:15:46,108] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,390606 s
[2024-09-03 22:15:46,129] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileFormatWriter: Write Job 62e1ce27-2af8-4f18-bfd6-02dcf1de20a5 committed.
[2024-09-03 22:15:46,132] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileFormatWriter: Finished processing stats for write job 62e1ce27-2af8-4f18-bfd6-02dcf1de20a5.
[2024-09-03 22:15:46,173] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:15:46,174] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:15:46,174] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:15:46,184] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:46,184] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:46,184] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:46,210] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO CodeGenerator: Code generated in 10.912954 ms
[2024-09-03 22:15:46,215] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:15:46,222] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:15:46,223] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46277 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:15:46,224] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:46,225] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650244 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:15:46,241] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:46,242] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:15:46,242] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:15:46,242] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:15:46,243] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:15:46,244] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:15:46,269] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:15:46,274] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:15:46,276] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46277 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:15:46,277] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:15:46,279] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:15:46,279] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:15:46,280] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:15:46,281] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:15:46,291] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:46,292] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:46,292] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:46,326] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO CodeGenerator: Code generated in 11.900814 ms
[2024-09-03 22:15:46,328] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-455940, partition values: [empty row]
[2024-09-03 22:15:46,346] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO CodeGenerator: Code generated in 15.412067 ms
[2024-09-03 22:15:46,381] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileOutputCommitter: Saved output of task 'attempt_202409032215464044409723083157542_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032215464044409723083157542_0002_m_000000
[2024-09-03 22:15:46,381] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkHadoopMapRedUtil: attempt_202409032215464044409723083157542_0002_m_000000_2: Committed
[2024-09-03 22:15:46,382] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:15:46,384] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 103 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:15:46,384] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:15:46,385] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,140 s
[2024-09-03 22:15:46,388] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:15:46,388] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:15:46,389] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,147905 s
[2024-09-03 22:15:46,402] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileFormatWriter: Write Job 831aa053-8aff-46a3-b93c-24af37ef031e committed.
[2024-09-03 22:15:46,403] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO FileFormatWriter: Finished processing stats for write job 831aa053-8aff-46a3-b93c-24af37ef031e.
[2024-09-03 22:15:46,435] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:15:46,445] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:15:46,462] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:15:46,476] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:15:46,476] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO BlockManager: BlockManager stopped
[2024-09-03 22:15:46,484] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:15:46,489] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:15:46,495] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:15:46,495] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:15:46,496] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-9be9e46d-1ce0-47bc-8cbc-5d0919fb5624
[2024-09-03 22:15:46,498] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-924800cb-d2d4-4e94-a05c-815bc54db707
[2024-09-03 22:15:46,501] {spark_submit.py:495} INFO - 24/09/03 22:15:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-9be9e46d-1ce0-47bc-8cbc-5d0919fb5624/pyspark-7cec2a95-1422-4e49-8004-60d94673ac07
[2024-09-03 22:15:46,622] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T011537, end_date=20240904T011546
[2024-09-03 22:15:46,665] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:15:46,702] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:29:38,597] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:29:38,608] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:29:38,609] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:29:38,609] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:29:38,609] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:29:38,617] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 22:29:38,619] {standard_task_runner.py:52} INFO - Started process 432714 to run task
[2024-09-03 22:29:38,624] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpdf9f1snq', '--error-file', '/tmp/tmp836vlc5j']
[2024-09-03 22:29:38,624] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:29:39,974] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:29:40,096] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 22:29:40,101] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:29:40,102] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 22:29:41,292] {spark_submit.py:495} INFO - 24/09/03 22:29:41 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:29:41,293] {spark_submit.py:495} INFO - 24/09/03 22:29:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:29:41,710] {spark_submit.py:495} INFO - 24/09/03 22:29:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:29:42,251] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:29:42,261] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:29:42,299] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO ResourceUtils: ==============================================================
[2024-09-03 22:29:42,300] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:29:42,301] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO ResourceUtils: ==============================================================
[2024-09-03 22:29:42,301] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:29:42,321] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:29:42,333] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:29:42,333] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:29:42,385] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:29:42,386] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:29:42,386] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:29:42,386] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:29:42,386] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:29:42,558] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO Utils: Successfully started service 'sparkDriver' on port 37033.
[2024-09-03 22:29:42,597] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:29:42,635] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:29:42,653] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:29:42,654] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:29:42,657] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:29:42,668] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-efe22531-6387-4389-aef3-9c8f1024897a
[2024-09-03 22:29:42,690] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:29:42,705] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:29:42,925] {spark_submit.py:495} INFO - 24/09/03 22:29:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:29:42,934] {spark_submit.py:495} INFO - 24/09/03 22:29:42 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:29:43,010] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:29:43,191] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:29:43,213] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36585.
[2024-09-03 22:29:43,213] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO NettyBlockTransferService: Server created on 192.168.2.128:36585
[2024-09-03 22:29:43,214] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:29:43,221] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36585, None)
[2024-09-03 22:29:43,223] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36585 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36585, None)
[2024-09-03 22:29:43,225] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36585, None)
[2024-09-03 22:29:43,226] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36585, None)
[2024-09-03 22:29:43,695] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:29:43,697] {spark_submit.py:495} INFO - 24/09/03 22:29:43 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:29:44,489] {spark_submit.py:495} INFO - 24/09/03 22:29:44 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 22:29:44,544] {spark_submit.py:495} INFO - 24/09/03 22:29:44 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:29:46,278] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:29:46,278] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:29:46,281] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:29:46,545] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:29:46,592] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:29:46,596] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36585 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:29:46,603] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:46,612] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198809 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:29:46,756] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:46,772] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:29:46,772] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:29:46,772] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:29:46,773] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:29:46,782] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:29:46,855] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:29:46,857] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:29:46,858] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36585 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:29:46,859] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:29:46,869] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:29:46,871] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:29:46,916] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:29:46,930] {spark_submit.py:495} INFO - 24/09/03 22:29:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:29:47,165] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4505, partition values: [empty row]
[2024-09-03 22:29:47,395] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO CodeGenerator: Code generated in 142.323663 ms
[2024-09-03 22:29:47,439] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 22:29:47,447] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 542 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:29:47,449] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:29:47,558] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,765 s
[2024-09-03 22:29:47,561] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:29:47,561] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:29:47,564] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,806451 s
[2024-09-03 22:29:47,949] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:29:47,950] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:29:47,951] {spark_submit.py:495} INFO - 24/09/03 22:29:47 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:29:48,037] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:48,038] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:48,038] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:48,116] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 25.020294 ms
[2024-09-03 22:29:48,163] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 32.806714 ms
[2024-09-03 22:29:48,170] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:29:48,178] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:29:48,179] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36585 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:29:48,180] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:48,185] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198809 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:29:48,238] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:48,239] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:29:48,240] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:29:48,240] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:29:48,240] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:29:48,240] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:29:48,277] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:29:48,279] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:29:48,280] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36585 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:29:48,281] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:29:48,282] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:29:48,282] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:29:48,285] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:29:48,286] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:29:48,334] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:48,335] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:48,335] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:48,403] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 28.59239 ms
[2024-09-03 22:29:48,406] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4505, partition values: [empty row]
[2024-09-03 22:29:48,431] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 20.673654 ms
[2024-09-03 22:29:48,455] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 5.203245 ms
[2024-09-03 22:29:48,500] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: Saved output of task 'attempt_202409032229484198695551885552787_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032229484198695551885552787_0001_m_000000
[2024-09-03 22:29:48,500] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkHadoopMapRedUtil: attempt_202409032229484198695551885552787_0001_m_000000_1: Committed
[2024-09-03 22:29:48,505] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:29:48,512] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 228 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:29:48,512] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:29:48,513] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,271 s
[2024-09-03 22:29:48,514] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:29:48,514] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:29:48,515] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,276489 s
[2024-09-03 22:29:48,562] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileFormatWriter: Write Job 388b6439-276d-4d8b-9a23-dbc1b53a325f committed.
[2024-09-03 22:29:48,568] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileFormatWriter: Finished processing stats for write job 388b6439-276d-4d8b-9a23-dbc1b53a325f.
[2024-09-03 22:29:48,606] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:29:48,606] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:29:48,607] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:29:48,642] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:48,642] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:48,643] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:48,684] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 11.520621 ms
[2024-09-03 22:29:48,688] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:29:48,696] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:29:48,698] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36585 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:29:48,698] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:48,699] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198809 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:29:48,714] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:48,715] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:29:48,715] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:29:48,716] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:29:48,716] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:29:48,717] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:29:48,738] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:29:48,741] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:29:48,742] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36585 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:29:48,743] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:29:48,743] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:29:48,744] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:29:48,744] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:29:48,745] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:29:48,757] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:48,757] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:48,757] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:48,794] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 8.400336 ms
[2024-09-03 22:29:48,796] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4505, partition values: [empty row]
[2024-09-03 22:29:48,813] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO CodeGenerator: Code generated in 13.971698 ms
[2024-09-03 22:29:48,822] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileOutputCommitter: Saved output of task 'attempt_202409032229487319500961824613452_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032229487319500961824613452_0002_m_000000
[2024-09-03 22:29:48,823] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkHadoopMapRedUtil: attempt_202409032229487319500961824613452_0002_m_000000_2: Committed
[2024-09-03 22:29:48,823] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:29:48,824] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 80 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:29:48,826] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:29:48,826] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,108 s
[2024-09-03 22:29:48,827] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:29:48,827] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:29:48,827] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,112718 s
[2024-09-03 22:29:48,848] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileFormatWriter: Write Job 8a3dd015-7632-4fde-935b-aacb50d61db0 committed.
[2024-09-03 22:29:48,848] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO FileFormatWriter: Finished processing stats for write job 8a3dd015-7632-4fde-935b-aacb50d61db0.
[2024-09-03 22:29:48,888] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:29:48,895] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:29:48,908] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:29:48,917] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:29:48,917] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO BlockManager: BlockManager stopped
[2024-09-03 22:29:48,924] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:29:48,927] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:29:48,933] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:29:48,933] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:29:48,934] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-73d74bd5-6c03-458d-80dd-39266a343bf6
[2024-09-03 22:29:48,937] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-73d74bd5-6c03-458d-80dd-39266a343bf6/pyspark-0848d945-ce87-4eb0-ab49-0226370a32b2
[2024-09-03 22:29:48,940] {spark_submit.py:495} INFO - 24/09/03 22:29:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7459074-6394-4fd4-862c-f32411acfb6d
[2024-09-03 22:29:49,006] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T012938, end_date=20240904T012949
[2024-09-03 22:29:49,054] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:29:49,101] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:36:49,284] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:36:49,289] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:36:49,289] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:36:49,289] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:36:49,290] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:36:49,299] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 22:36:49,302] {standard_task_runner.py:52} INFO - Started process 436218 to run task
[2024-09-03 22:36:49,305] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpq01mh4un', '--error-file', '/tmp/tmp10f6yt86']
[2024-09-03 22:36:49,305] {standard_task_runner.py:80} INFO - Job 79: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:36:49,350] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:36:49,399] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 22:36:49,403] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:36:49,404] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 22:36:50,370] {spark_submit.py:495} INFO - 24/09/03 22:36:50 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:36:50,371] {spark_submit.py:495} INFO - 24/09/03 22:36:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:36:50,728] {spark_submit.py:495} INFO - 24/09/03 22:36:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:36:51,258] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:36:51,266] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:36:51,302] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO ResourceUtils: ==============================================================
[2024-09-03 22:36:51,302] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:36:51,302] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO ResourceUtils: ==============================================================
[2024-09-03 22:36:51,302] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:36:51,321] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:36:51,332] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:36:51,332] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:36:51,369] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:36:51,370] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:36:51,370] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:36:51,370] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:36:51,370] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:36:51,519] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO Utils: Successfully started service 'sparkDriver' on port 35097.
[2024-09-03 22:36:51,543] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:36:51,578] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:36:51,595] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:36:51,595] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:36:51,598] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:36:51,608] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-620bd349-a9d6-48ea-a128-66be514db1bb
[2024-09-03 22:36:51,628] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:36:51,644] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:36:51,830] {spark_submit.py:495} INFO - 24/09/03 22:36:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:36:51,837] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:36:51,893] {spark_submit.py:495} INFO - 24/09/03 22:36:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:36:52,092] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:36:52,117] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36143.
[2024-09-03 22:36:52,118] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO NettyBlockTransferService: Server created on 192.168.2.128:36143
[2024-09-03 22:36:52,120] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:36:52,125] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36143, None)
[2024-09-03 22:36:52,128] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36143 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36143, None)
[2024-09-03 22:36:52,132] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36143, None)
[2024-09-03 22:36:52,134] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36143, None)
[2024-09-03 22:36:52,601] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:36:52,601] {spark_submit.py:495} INFO - 24/09/03 22:36:52 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:36:53,387] {spark_submit.py:495} INFO - 24/09/03 22:36:53 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 22:36:53,441] {spark_submit.py:495} INFO - 24/09/03 22:36:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:36:55,201] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:36:55,202] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:36:55,206] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:36:55,462] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:36:55,506] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:36:55,509] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36143 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:36:55,515] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:55,521] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648725 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:36:55,671] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:55,684] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:36:55,684] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:36:55,684] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:36:55,686] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:36:55,703] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:36:55,776] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:36:55,779] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:36:55,779] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36143 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:36:55,780] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:36:55,790] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:36:55,791] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:36:55,836] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:36:55,850] {spark_submit.py:495} INFO - 24/09/03 22:36:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:36:56,137] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454421, partition values: [empty row]
[2024-09-03 22:36:56,373] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO CodeGenerator: Code generated in 150.677504 ms
[2024-09-03 22:36:56,454] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:36:56,581] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 636 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:36:56,592] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:36:56,593] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,878 s
[2024-09-03 22:36:56,599] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:36:56,599] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:36:56,605] {spark_submit.py:495} INFO - 24/09/03 22:36:56 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,931098 s
[2024-09-03 22:36:57,013] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:36:57,016] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:36:57,017] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:36:57,085] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:57,085] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:57,086] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:57,166] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 26.216646 ms
[2024-09-03 22:36:57,211] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 32.263437 ms
[2024-09-03 22:36:57,220] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:36:57,226] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:36:57,227] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36143 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:36:57,228] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:57,234] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648725 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:36:57,296] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:57,300] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:36:57,301] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:36:57,301] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:36:57,301] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:36:57,318] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:36:57,357] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:36:57,359] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:36:57,360] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36143 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:36:57,360] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:36:57,360] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:36:57,361] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:36:57,364] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:36:57,366] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:36:57,411] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:57,411] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:57,412] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:57,479] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 19.71606 ms
[2024-09-03 22:36:57,483] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454421, partition values: [empty row]
[2024-09-03 22:36:57,503] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 17.217785 ms
[2024-09-03 22:36:57,529] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 4.86591 ms
[2024-09-03 22:36:57,643] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: Saved output of task 'attempt_202409032236575382712603184790095_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032236575382712603184790095_0001_m_000000
[2024-09-03 22:36:57,644] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkHadoopMapRedUtil: attempt_202409032236575382712603184790095_0001_m_000000_1: Committed
[2024-09-03 22:36:57,648] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:36:57,656] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 294 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:36:57,658] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:36:57,659] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,339 s
[2024-09-03 22:36:57,659] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:36:57,659] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:36:57,662] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,362859 s
[2024-09-03 22:36:57,676] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileFormatWriter: Write Job abcfa1fd-e5ab-4df7-8190-5e51a0f4e382 committed.
[2024-09-03 22:36:57,678] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileFormatWriter: Finished processing stats for write job abcfa1fd-e5ab-4df7-8190-5e51a0f4e382.
[2024-09-03 22:36:57,722] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:36:57,722] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:36:57,723] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:36:57,746] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:57,748] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:57,748] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:57,795] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 14.181494 ms
[2024-09-03 22:36:57,802] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:36:57,813] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:36:57,816] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36143 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:36:57,818] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:57,819] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648725 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:36:57,839] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:57,841] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:36:57,842] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:36:57,842] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:36:57,842] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:36:57,842] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:36:57,871] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:36:57,876] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:36:57,877] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36143 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:36:57,877] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:36:57,882] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:36:57,882] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:36:57,884] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:36:57,886] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:36:57,906] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:57,906] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:57,906] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:57,956] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 12.815965 ms
[2024-09-03 22:36:57,967] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454421, partition values: [empty row]
[2024-09-03 22:36:57,985] {spark_submit.py:495} INFO - 24/09/03 22:36:57 INFO CodeGenerator: Code generated in 15.031056 ms
[2024-09-03 22:36:58,025] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO FileOutputCommitter: Saved output of task 'attempt_202409032236578349902887824882003_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032236578349902887824882003_0002_m_000000
[2024-09-03 22:36:58,025] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO SparkHadoopMapRedUtil: attempt_202409032236578349902887824882003_0002_m_000000_2: Committed
[2024-09-03 22:36:58,027] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:36:58,029] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 146 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:36:58,031] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,186 s
[2024-09-03 22:36:58,032] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:36:58,032] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:36:58,032] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:36:58,035] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,193372 s
[2024-09-03 22:36:58,073] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO FileFormatWriter: Write Job 2865951e-3103-4843-845d-815d8d2fb3cb committed.
[2024-09-03 22:36:58,073] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO FileFormatWriter: Finished processing stats for write job 2865951e-3103-4843-845d-815d8d2fb3cb.
[2024-09-03 22:36:58,114] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:36:58,126] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:36:58,139] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:36:58,148] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:36:58,151] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO BlockManager: BlockManager stopped
[2024-09-03 22:36:58,158] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:36:58,160] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:36:58,167] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:36:58,167] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:36:58,167] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-78e52e42-6c5b-4eeb-876c-f7c3b4606ebf/pyspark-a10fcf2e-f41d-4958-8b3a-647caecc6927
[2024-09-03 22:36:58,170] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-78e52e42-6c5b-4eeb-876c-f7c3b4606ebf
[2024-09-03 22:36:58,172] {spark_submit.py:495} INFO - 24/09/03 22:36:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bfe5edb-f9f8-410a-94f9-ca8b8d77245c
[2024-09-03 22:36:58,223] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T013649, end_date=20240904T013658
[2024-09-03 22:36:58,256] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:36:58,268] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:57:21,811] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:57:21,816] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 22:57:21,816] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:57:21,816] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:57:21,816] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:57:21,824] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 22:57:21,826] {standard_task_runner.py:52} INFO - Started process 457022 to run task
[2024-09-03 22:57:21,828] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '163', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2kakj26u', '--error-file', '/tmp/tmp_4qn0eqk']
[2024-09-03 22:57:21,829] {standard_task_runner.py:80} INFO - Job 163: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:57:21,874] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:57:21,919] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 22:57:21,923] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:57:21,924] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 22:57:22,955] {spark_submit.py:495} INFO - 24/09/03 22:57:22 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:57:22,956] {spark_submit.py:495} INFO - 24/09/03 22:57:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:57:23,264] {spark_submit.py:495} INFO - 24/09/03 22:57:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:57:23,763] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:57:23,771] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:57:23,807] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO ResourceUtils: ==============================================================
[2024-09-03 22:57:23,808] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:57:23,808] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO ResourceUtils: ==============================================================
[2024-09-03 22:57:23,809] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:57:23,825] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:57:23,839] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:57:23,840] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:57:23,877] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:57:23,877] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:57:23,877] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:57:23,877] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:57:23,877] {spark_submit.py:495} INFO - 24/09/03 22:57:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:57:24,022] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO Utils: Successfully started service 'sparkDriver' on port 43361.
[2024-09-03 22:57:24,046] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:57:24,069] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:57:24,084] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:57:24,084] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:57:24,087] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:57:24,097] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3ddba086-7e25-49a4-8bd6-64072d1104f5
[2024-09-03 22:57:24,114] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:57:24,128] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:57:24,315] {spark_submit.py:495} INFO - 24/09/03 22:57:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:57:24,322] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:57:24,372] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:57:24,577] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:57:24,597] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34109.
[2024-09-03 22:57:24,598] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO NettyBlockTransferService: Server created on 192.168.2.128:34109
[2024-09-03 22:57:24,599] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:57:24,603] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34109, None)
[2024-09-03 22:57:24,605] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34109 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34109, None)
[2024-09-03 22:57:24,608] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34109, None)
[2024-09-03 22:57:24,609] {spark_submit.py:495} INFO - 24/09/03 22:57:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34109, None)
[2024-09-03 22:57:25,072] {spark_submit.py:495} INFO - 24/09/03 22:57:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:57:25,072] {spark_submit.py:495} INFO - 24/09/03 22:57:25 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:57:25,808] {spark_submit.py:495} INFO - 24/09/03 22:57:25 INFO InMemoryFileIndex: It took 29 ms to list leaf files for 1 paths.
[2024-09-03 22:57:25,864] {spark_submit.py:495} INFO - 24/09/03 22:57:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 22:57:27,481] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:57:27,481] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:57:27,487] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:57:27,732] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:57:27,771] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:57:27,774] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34109 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:57:27,778] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:27,785] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648628 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:57:27,916] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:27,930] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:57:27,931] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:57:27,931] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:57:27,933] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:57:27,938] {spark_submit.py:495} INFO - 24/09/03 22:57:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:57:28,010] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:57:28,013] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:57:28,014] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34109 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:57:28,015] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:57:28,029] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:57:28,029] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:57:28,078] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:57:28,105] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:57:28,328] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454324, partition values: [empty row]
[2024-09-03 22:57:28,531] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO CodeGenerator: Code generated in 117.755222 ms
[2024-09-03 22:57:28,603] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:57:28,707] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 546 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:57:28,710] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:57:28,715] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,763 s
[2024-09-03 22:57:28,720] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:57:28,721] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:57:28,725] {spark_submit.py:495} INFO - 24/09/03 22:57:28 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,806801 s
[2024-09-03 22:57:29,077] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:57:29,078] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:57:29,079] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:57:29,143] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:29,143] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:29,143] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:29,216] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 25.907379 ms
[2024-09-03 22:57:29,252] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 23.334076 ms
[2024-09-03 22:57:29,257] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:57:29,264] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:57:29,264] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34109 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:57:29,265] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:29,267] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648628 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:57:29,315] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:29,316] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:57:29,317] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:57:29,317] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:57:29,317] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:57:29,319] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:57:29,365] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:57:29,368] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:57:29,369] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34109 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:57:29,370] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:57:29,371] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:57:29,372] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:57:29,376] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:57:29,376] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:57:29,418] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:29,418] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:29,419] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:29,478] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 18.89934 ms
[2024-09-03 22:57:29,481] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454324, partition values: [empty row]
[2024-09-03 22:57:29,500] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 16.407716 ms
[2024-09-03 22:57:29,527] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 5.130383 ms
[2024-09-03 22:57:29,615] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409032257293145134341183069262_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032257293145134341183069262_0001_m_000000
[2024-09-03 22:57:29,615] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkHadoopMapRedUtil: attempt_202409032257293145134341183069262_0001_m_000000_1: Committed
[2024-09-03 22:57:29,619] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:57:29,621] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 248 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:57:29,621] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:57:29,622] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,301 s
[2024-09-03 22:57:29,622] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:57:29,622] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:57:29,623] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,307579 s
[2024-09-03 22:57:29,646] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileFormatWriter: Write Job feaef9ad-3909-43ca-a9bd-cf3e1957cc70 committed.
[2024-09-03 22:57:29,650] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileFormatWriter: Finished processing stats for write job feaef9ad-3909-43ca-a9bd-cf3e1957cc70.
[2024-09-03 22:57:29,683] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:57:29,684] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:57:29,684] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:57:29,694] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:29,694] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:29,694] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:29,725] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 9.261124 ms
[2024-09-03 22:57:29,731] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:57:29,738] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:57:29,738] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34109 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:57:29,739] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:29,740] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648628 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:57:29,758] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:29,759] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:57:29,759] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:57:29,759] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:57:29,760] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:57:29,760] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:57:29,782] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:57:29,784] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:57:29,785] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34109 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:57:29,788] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:57:29,792] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:57:29,792] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:57:29,794] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:57:29,798] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:57:29,804] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:29,806] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:29,807] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:29,839] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 9.748991 ms
[2024-09-03 22:57:29,842] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-454324, partition values: [empty row]
[2024-09-03 22:57:29,854] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO CodeGenerator: Code generated in 10.004129 ms
[2024-09-03 22:57:29,881] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409032257297499275363794016386_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032257297499275363794016386_0002_m_000000
[2024-09-03 22:57:29,881] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkHadoopMapRedUtil: attempt_202409032257297499275363794016386_0002_m_000000_2: Committed
[2024-09-03 22:57:29,882] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:57:29,884] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 91 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:57:29,884] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,123 s
[2024-09-03 22:57:29,885] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:57:29,886] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:57:29,886] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:57:29,889] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,128954 s
[2024-09-03 22:57:29,924] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileFormatWriter: Write Job a0614aef-f01f-41ea-a749-708873092348 committed.
[2024-09-03 22:57:29,925] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO FileFormatWriter: Finished processing stats for write job a0614aef-f01f-41ea-a749-708873092348.
[2024-09-03 22:57:29,959] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:57:29,966] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:57:29,977] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:57:29,986] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:57:29,986] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO BlockManager: BlockManager stopped
[2024-09-03 22:57:29,994] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:57:29,997] {spark_submit.py:495} INFO - 24/09/03 22:57:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:57:30,001] {spark_submit.py:495} INFO - 24/09/03 22:57:30 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:57:30,002] {spark_submit.py:495} INFO - 24/09/03 22:57:30 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:57:30,002] {spark_submit.py:495} INFO - 24/09/03 22:57:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-35dff5b3-37cc-4b56-bb06-2b70427e80df
[2024-09-03 22:57:30,005] {spark_submit.py:495} INFO - 24/09/03 22:57:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-35dff5b3-37cc-4b56-bb06-2b70427e80df/pyspark-4518878f-1b60-4a93-8d31-d11da350181e
[2024-09-03 22:57:30,009] {spark_submit.py:495} INFO - 24/09/03 22:57:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-2825f219-22b9-4777-b657-078b99216005
[2024-09-03 22:57:30,130] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T015721, end_date=20240904T015730
[2024-09-03 22:57:30,163] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:57:30,176] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 23:36:09,315] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 23:36:09,322] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [queued]>
[2024-09-03 23:36:09,323] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:36:09,323] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 23:36:09,323] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:36:09,333] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-03 00:00:00+00:00
[2024-09-03 23:36:09,336] {standard_task_runner.py:52} INFO - Started process 476091 to run task
[2024-09-03 23:36:09,340] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-03T00:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2ffr34fz', '--error-file', '/tmp/tmpkq8cnwbg']
[2024-09-03 23:36:09,341] {standard_task_runner.py:80} INFO - Job 88: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 23:36:09,379] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-03T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 23:36:09,422] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-03T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-03T00:00:00+00:00
[2024-09-03 23:36:09,426] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 23:36:09,427] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240903
[2024-09-03 23:36:10,563] {spark_submit.py:495} INFO - 24/09/03 23:36:10 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 23:36:10,564] {spark_submit.py:495} INFO - 24/09/03 23:36:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 23:36:10,898] {spark_submit.py:495} INFO - 24/09/03 23:36:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 23:36:11,411] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 23:36:11,421] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 23:36:11,457] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO ResourceUtils: ==============================================================
[2024-09-03 23:36:11,458] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 23:36:11,458] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO ResourceUtils: ==============================================================
[2024-09-03 23:36:11,458] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 23:36:11,477] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 23:36:11,491] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 23:36:11,491] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 23:36:11,538] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 23:36:11,538] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 23:36:11,538] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 23:36:11,539] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 23:36:11,539] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 23:36:11,692] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO Utils: Successfully started service 'sparkDriver' on port 36089.
[2024-09-03 23:36:11,713] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 23:36:11,748] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 23:36:11,762] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 23:36:11,762] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 23:36:11,766] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 23:36:11,777] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e66cc0a-2eef-437c-b02b-3d0346857f30
[2024-09-03 23:36:11,795] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 23:36:11,809] {spark_submit.py:495} INFO - 24/09/03 23:36:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 23:36:11,996] {spark_submit.py:495} INFO - 24/09/03 23:36:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 23:36:12,002] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 23:36:12,054] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 23:36:12,249] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 23:36:12,271] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40781.
[2024-09-03 23:36:12,271] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO NettyBlockTransferService: Server created on 192.168.2.128:40781
[2024-09-03 23:36:12,273] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 23:36:12,277] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40781, None)
[2024-09-03 23:36:12,281] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40781 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40781, None)
[2024-09-03 23:36:12,282] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40781, None)
[2024-09-03 23:36:12,283] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40781, None)
[2024-09-03 23:36:12,694] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 23:36:12,694] {spark_submit.py:495} INFO - 24/09/03 23:36:12 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 23:36:13,518] {spark_submit.py:495} INFO - 24/09/03 23:36:13 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 23:36:13,574] {spark_submit.py:495} INFO - 24/09/03 23:36:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 23:36:15,288] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:36:15,289] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:36:15,292] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 23:36:15,543] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 23:36:15,584] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 23:36:15,587] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40781 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 23:36:15,591] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:36:15,598] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198837 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:36:15,746] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:36:15,760] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:36:15,761] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:36:15,761] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:36:15,763] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:36:15,776] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:36:15,869] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 23:36:15,872] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 23:36:15,873] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40781 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 23:36:15,873] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:36:15,885] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:36:15,887] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 23:36:15,937] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 23:36:15,953] {spark_submit.py:495} INFO - 24/09/03 23:36:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 23:36:16,283] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4533, partition values: [empty row]
[2024-09-03 23:36:16,549] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO CodeGenerator: Code generated in 151.218603 ms
[2024-09-03 23:36:16,597] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 23:36:16,616] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 691 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:36:16,618] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 23:36:16,623] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,828 s
[2024-09-03 23:36:16,726] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:36:16,727] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 23:36:16,730] {spark_submit.py:495} INFO - 24/09/03 23:36:16 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,983374 s
[2024-09-03 23:36:17,115] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 23:36:17,117] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 23:36:17,117] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 23:36:17,192] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:36:17,193] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:36:17,193] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:36:17,273] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 25.198983 ms
[2024-09-03 23:36:17,316] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 29.08331 ms
[2024-09-03 23:36:17,324] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 23:36:17,333] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 23:36:17,334] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40781 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 23:36:17,335] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:36:17,338] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198837 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:36:17,394] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:36:17,396] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:36:17,396] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:36:17,396] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:36:17,396] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:36:17,397] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:36:17,442] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 23:36:17,444] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 23:36:17,445] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40781 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 23:36:17,445] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:36:17,446] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:36:17,446] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 23:36:17,450] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:36:17,451] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 23:36:17,500] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:36:17,500] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:36:17,501] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:36:17,578] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 33.729221 ms
[2024-09-03 23:36:17,582] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4533, partition values: [empty row]
[2024-09-03 23:36:17,604] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 19.710661 ms
[2024-09-03 23:36:17,627] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 4.488952 ms
[2024-09-03 23:36:17,673] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: Saved output of task 'attempt_202409032336174568998242917725578_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240903/_temporary/0/task_202409032336174568998242917725578_0001_m_000000
[2024-09-03 23:36:17,673] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkHadoopMapRedUtil: attempt_202409032336174568998242917725578_0001_m_000000_1: Committed
[2024-09-03 23:36:17,677] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 23:36:17,678] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 231 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:36:17,678] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 23:36:17,679] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,280 s
[2024-09-03 23:36:17,680] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:36:17,680] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 23:36:17,681] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,286712 s
[2024-09-03 23:36:17,700] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileFormatWriter: Write Job bf606635-cec6-4b5a-ae5a-bc7c9c2a7c28 committed.
[2024-09-03 23:36:17,703] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileFormatWriter: Finished processing stats for write job bf606635-cec6-4b5a-ae5a-bc7c9c2a7c28.
[2024-09-03 23:36:17,754] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:36:17,754] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:36:17,754] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 23:36:17,763] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:36:17,764] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:36:17,764] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:36:17,793] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 9.346665 ms
[2024-09-03 23:36:17,796] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 23:36:17,805] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 23:36:17,806] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40781 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 23:36:17,807] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:36:17,808] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198837 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:36:17,831] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:36:17,832] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:36:17,833] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:36:17,833] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:36:17,835] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:36:17,838] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:36:17,856] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 23:36:17,858] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 23:36:17,859] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40781 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 23:36:17,859] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:36:17,859] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:36:17,860] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 23:36:17,861] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:36:17,861] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 23:36:17,873] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:36:17,873] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:36:17,873] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:36:17,902] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 9.819271 ms
[2024-09-03 23:36:17,905] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-03T00:00:00.00Z/data_engineer_20240903.json, range: 0-4533, partition values: [empty row]
[2024-09-03 23:36:17,919] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO CodeGenerator: Code generated in 11.73483 ms
[2024-09-03 23:36:17,929] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileOutputCommitter: Saved output of task 'attempt_202409032336173571843465993242204_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240903/_temporary/0/task_202409032336173571843465993242204_0002_m_000000
[2024-09-03 23:36:17,929] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkHadoopMapRedUtil: attempt_202409032336173571843465993242204_0002_m_000000_2: Committed
[2024-09-03 23:36:17,932] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 23:36:17,933] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 72 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:36:17,935] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 23:36:17,935] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,097 s
[2024-09-03 23:36:17,935] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:36:17,936] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 23:36:17,936] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,104453 s
[2024-09-03 23:36:17,950] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileFormatWriter: Write Job 8dc341bd-be4c-49c4-8f63-4ce8e03099c9 committed.
[2024-09-03 23:36:17,950] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO FileFormatWriter: Finished processing stats for write job 8dc341bd-be4c-49c4-8f63-4ce8e03099c9.
[2024-09-03 23:36:17,989] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 23:36:17,996] {spark_submit.py:495} INFO - 24/09/03 23:36:17 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 23:36:18,009] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 23:36:18,018] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO MemoryStore: MemoryStore cleared
[2024-09-03 23:36:18,019] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO BlockManager: BlockManager stopped
[2024-09-03 23:36:18,027] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 23:36:18,040] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 23:36:18,051] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 23:36:18,051] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 23:36:18,052] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-af7ab2c2-e430-484c-8bc2-311291f7775d
[2024-09-03 23:36:18,055] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-af7ab2c2-e430-484c-8bc2-311291f7775d/pyspark-90e098d2-d969-4e04-a2b2-3014d877f2b0
[2024-09-03 23:36:18,058] {spark_submit.py:495} INFO - 24/09/03 23:36:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-142550b5-1f54-48b6-a942-1af6fcb40974
[2024-09-03 23:36:18,127] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240903T000000, start_date=20240904T023609, end_date=20240904T023618
[2024-09-03 23:36:18,173] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 23:36:18,192] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
