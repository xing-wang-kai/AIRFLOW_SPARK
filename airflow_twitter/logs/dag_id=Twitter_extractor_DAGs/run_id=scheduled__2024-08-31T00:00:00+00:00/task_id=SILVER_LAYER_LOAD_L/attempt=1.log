[2024-09-03 16:25:20,959] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 16:25:20,963] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 16:25:20,963] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:25:20,963] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:25:20,963] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:25:20,973] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 16:25:20,975] {standard_task_runner.py:52} INFO - Started process 286741 to run task
[2024-09-03 16:25:20,977] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpjmhtwwdv', '--error-file', '/tmp/tmpdaefve3x']
[2024-09-03 16:25:20,978] {standard_task_runner.py:80} INFO - Job 99: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:25:21,006] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:25:21,045] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 16:25:21,049] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:25:21,050] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 16:25:22,013] {spark_submit.py:495} INFO - 24/09/03 16:25:22 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:25:22,013] {spark_submit.py:495} INFO - 24/09/03 16:25:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:25:22,380] {spark_submit.py:495} INFO - 24/09/03 16:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:25:22,854] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:25:22,866] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:25:22,908] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO ResourceUtils: ==============================================================
[2024-09-03 16:25:22,908] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:25:22,908] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO ResourceUtils: ==============================================================
[2024-09-03 16:25:22,909] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:25:22,933] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:25:22,944] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:25:22,944] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:25:22,985] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:25:22,985] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:25:22,985] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:25:22,986] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:25:22,986] {spark_submit.py:495} INFO - 24/09/03 16:25:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:25:23,129] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO Utils: Successfully started service 'sparkDriver' on port 44509.
[2024-09-03 16:25:23,150] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:25:23,176] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:25:23,190] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:25:23,191] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:25:23,194] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:25:23,204] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c8cc3de1-92be-4b76-a668-0c32bf55917d
[2024-09-03 16:25:23,222] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:25:23,235] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:25:23,441] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:25:23,473] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:25:23,654] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:25:23,677] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35883.
[2024-09-03 16:25:23,677] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO NettyBlockTransferService: Server created on 192.168.2.128:35883
[2024-09-03 16:25:23,678] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:25:23,683] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 35883, None)
[2024-09-03 16:25:23,685] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:35883 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 35883, None)
[2024-09-03 16:25:23,687] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 35883, None)
[2024-09-03 16:25:23,687] {spark_submit.py:495} INFO - 24/09/03 16:25:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 35883, None)
[2024-09-03 16:25:24,073] {spark_submit.py:495} INFO - 24/09/03 16:25:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:25:24,073] {spark_submit.py:495} INFO - 24/09/03 16:25:24 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:25:24,734] {spark_submit.py:495} INFO - 24/09/03 16:25:24 INFO InMemoryFileIndex: It took 26 ms to list leaf files for 1 paths.
[2024-09-03 16:25:24,781] {spark_submit.py:495} INFO - 24/09/03 16:25:24 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:25:26,319] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:25:26,320] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:25:26,323] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:25:26,600] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:25:26,641] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:25:26,644] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:35883 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:25:26,661] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:26,670] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649276 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:25:26,821] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:26,834] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:25:26,835] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:25:26,835] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:25:26,837] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:25:26,841] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:25:26,944] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:25:26,946] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:25:26,947] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:35883 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:25:26,948] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:25:26,960] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:25:26,961] {spark_submit.py:495} INFO - 24/09/03 16:25:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:25:27,010] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:25:27,034] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:25:27,336] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-454972, partition values: [empty row]
[2024-09-03 16:25:27,553] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO CodeGenerator: Code generated in 132.244944 ms
[2024-09-03 16:25:27,635] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 16:25:27,747] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 647 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:25:27,747] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:25:27,754] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,898 s
[2024-09-03 16:25:27,755] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:25:27,755] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:25:27,770] {spark_submit.py:495} INFO - 24/09/03 16:25:27 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,947181 s
[2024-09-03 16:25:28,168] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:25:28,169] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:25:28,169] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:25:28,234] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:28,234] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:28,235] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:28,327] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO CodeGenerator: Code generated in 28.939068 ms
[2024-09-03 16:25:28,365] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO CodeGenerator: Code generated in 25.219958 ms
[2024-09-03 16:25:28,372] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:25:28,380] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:25:28,380] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:35883 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:25:28,381] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:28,385] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649276 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:25:28,436] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:28,437] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:25:28,437] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:25:28,438] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:25:28,438] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:25:28,438] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:25:28,484] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:25:28,487] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:25:28,487] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:35883 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:25:28,488] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:25:28,489] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:25:28,490] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:25:28,495] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:25:28,496] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:25:28,564] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:28,564] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:28,564] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:28,625] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO CodeGenerator: Code generated in 21.710795 ms
[2024-09-03 16:25:28,628] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-454972, partition values: [empty row]
[2024-09-03 16:25:28,650] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO CodeGenerator: Code generated in 18.512659 ms
[2024-09-03 16:25:28,674] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO CodeGenerator: Code generated in 5.011411 ms
[2024-09-03 16:25:28,770] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: Saved output of task 'attempt_202409031625285406533711499687077_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_202409031625285406533711499687077_0001_m_000000
[2024-09-03 16:25:28,770] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkHadoopMapRedUtil: attempt_202409031625285406533711499687077_0001_m_000000_1: Committed
[2024-09-03 16:25:28,775] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:25:28,777] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 286 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:25:28,778] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,338 s
[2024-09-03 16:25:28,778] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:25:28,779] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:25:28,779] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:25:28,779] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,342742 s
[2024-09-03 16:25:28,810] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileFormatWriter: Write Job 0d321589-af37-436b-a5a7-905172af96ec committed.
[2024-09-03 16:25:28,813] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileFormatWriter: Finished processing stats for write job 0d321589-af37-436b-a5a7-905172af96ec.
[2024-09-03 16:25:28,872] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:25:28,873] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:25:28,874] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:25:28,891] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:28,891] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:28,892] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:28,933] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO CodeGenerator: Code generated in 14.730849 ms
[2024-09-03 16:25:28,937] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:25:28,946] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:25:28,946] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:35883 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:25:28,947] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:28,948] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649276 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:25:28,962] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:28,963] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:25:28,964] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:25:28,965] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:25:28,966] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:25:28,967] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:25:28,984] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:25:28,986] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:25:28,986] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:35883 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:25:28,987] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:25:28,987] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:25:28,987] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:25:28,988] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:25:28,989] {spark_submit.py:495} INFO - 24/09/03 16:25:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:25:29,001] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:29,002] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:29,002] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:29,037] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO CodeGenerator: Code generated in 8.768975 ms
[2024-09-03 16:25:29,039] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-454972, partition values: [empty row]
[2024-09-03 16:25:29,056] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO CodeGenerator: Code generated in 13.21688 ms
[2024-09-03 16:25:29,085] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409031625284050781941094051784_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409031625284050781941094051784_0002_m_000000
[2024-09-03 16:25:29,085] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO SparkHadoopMapRedUtil: attempt_202409031625284050781941094051784_0002_m_000000_2: Committed
[2024-09-03 16:25:29,086] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:25:29,087] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 99 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:25:29,089] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,121 s
[2024-09-03 16:25:29,089] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:25:29,089] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:25:29,090] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:25:29,093] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,126299 s
[2024-09-03 16:25:29,107] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO FileFormatWriter: Write Job 38f0dbd8-4c23-4bb8-83f6-62e58acfdb24 committed.
[2024-09-03 16:25:29,108] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO FileFormatWriter: Finished processing stats for write job 38f0dbd8-4c23-4bb8-83f6-62e58acfdb24.
[2024-09-03 16:25:29,153] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:25:29,163] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:25:29,175] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:25:29,184] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:25:29,185] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO BlockManager: BlockManager stopped
[2024-09-03 16:25:29,191] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:25:29,198] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:25:29,202] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:25:29,202] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:25:29,203] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f1e3241-3182-474f-88f7-91f917b18cbf/pyspark-2908b69d-7310-40d1-ab7f-73a57c0c49a8
[2024-09-03 16:25:29,207] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-61c20431-bb34-4624-bfd3-72dee04a6560
[2024-09-03 16:25:29,209] {spark_submit.py:495} INFO - 24/09/03 16:25:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f1e3241-3182-474f-88f7-91f917b18cbf
[2024-09-03 16:25:29,320] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240903T192520, end_date=20240903T192529
[2024-09-03 16:25:29,337] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:25:29,345] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:43:10,182] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 16:43:10,190] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 16:43:10,190] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:43:10,190] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:43:10,190] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:43:10,200] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 16:43:10,203] {standard_task_runner.py:52} INFO - Started process 297286 to run task
[2024-09-03 16:43:10,206] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpjwogdq5z', '--error-file', '/tmp/tmpo39yz8_s']
[2024-09-03 16:43:10,207] {standard_task_runner.py:80} INFO - Job 99: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:43:10,242] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:43:10,286] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 16:43:10,289] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:43:10,290] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 16:43:11,292] {spark_submit.py:495} INFO - 24/09/03 16:43:11 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:43:11,293] {spark_submit.py:495} INFO - 24/09/03 16:43:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:43:11,615] {spark_submit.py:495} INFO - 24/09/03 16:43:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:43:12,120] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:43:12,127] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:43:12,164] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO ResourceUtils: ==============================================================
[2024-09-03 16:43:12,164] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:43:12,164] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO ResourceUtils: ==============================================================
[2024-09-03 16:43:12,165] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:43:12,184] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:43:12,200] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:43:12,201] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:43:12,234] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:43:12,235] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:43:12,235] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:43:12,235] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:43:12,235] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:43:12,376] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO Utils: Successfully started service 'sparkDriver' on port 34377.
[2024-09-03 16:43:12,403] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:43:12,434] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:43:12,450] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:43:12,450] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:43:12,453] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:43:12,464] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-69abcd5b-b921-41e1-8105-6f80affd91df
[2024-09-03 16:43:12,482] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:43:12,497] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:43:12,707] {spark_submit.py:495} INFO - 24/09/03 16:43:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:43:12,713] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:43:12,756] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:43:12,924] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:43:12,945] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38799.
[2024-09-03 16:43:12,945] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO NettyBlockTransferService: Server created on 192.168.2.128:38799
[2024-09-03 16:43:12,947] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:43:12,951] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38799, None)
[2024-09-03 16:43:12,954] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38799 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38799, None)
[2024-09-03 16:43:12,956] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38799, None)
[2024-09-03 16:43:12,957] {spark_submit.py:495} INFO - 24/09/03 16:43:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38799, None)
[2024-09-03 16:43:13,360] {spark_submit.py:495} INFO - 24/09/03 16:43:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:43:13,360] {spark_submit.py:495} INFO - 24/09/03 16:43:13 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:43:14,197] {spark_submit.py:495} INFO - 24/09/03 16:43:14 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 16:43:14,263] {spark_submit.py:495} INFO - 24/09/03 16:43:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 16:43:15,974] {spark_submit.py:495} INFO - 24/09/03 16:43:15 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:43:15,975] {spark_submit.py:495} INFO - 24/09/03 16:43:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:43:15,979] {spark_submit.py:495} INFO - 24/09/03 16:43:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:43:16,226] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:43:16,268] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:43:16,269] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38799 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:43:16,274] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:16,281] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649576 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:43:16,434] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:16,448] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:43:16,448] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:43:16,448] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:43:16,449] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:43:16,460] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:43:16,566] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:43:16,568] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:43:16,575] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38799 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:43:16,576] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:43:16,588] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:43:16,590] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:43:16,636] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:43:16,652] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:43:16,897] {spark_submit.py:495} INFO - 24/09/03 16:43:16 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-455272, partition values: [empty row]
[2024-09-03 16:43:17,117] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO CodeGenerator: Code generated in 131.79362 ms
[2024-09-03 16:43:17,198] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 16:43:17,330] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 586 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:43:17,343] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:43:17,344] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,860 s
[2024-09-03 16:43:17,347] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:43:17,347] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:43:17,356] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,915292 s
[2024-09-03 16:43:17,821] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:43:17,823] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:43:17,824] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:43:17,884] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:17,886] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:17,887] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:17,965] {spark_submit.py:495} INFO - 24/09/03 16:43:17 INFO CodeGenerator: Code generated in 28.842219 ms
[2024-09-03 16:43:18,005] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 24.542077 ms
[2024-09-03 16:43:18,011] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:43:18,019] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:43:18,021] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38799 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:43:18,023] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:18,026] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649576 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:43:18,082] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:18,083] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:43:18,084] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:43:18,084] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:43:18,084] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:43:18,085] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:43:18,133] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:43:18,136] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:43:18,137] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38799 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:43:18,139] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:43:18,140] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:43:18,141] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:43:18,145] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:43:18,146] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:43:18,211] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:18,211] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:18,213] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:18,278] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 21.124583 ms
[2024-09-03 16:43:18,282] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-455272, partition values: [empty row]
[2024-09-03 16:43:18,312] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 27.408148 ms
[2024-09-03 16:43:18,336] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 4.700707 ms
[2024-09-03 16:43:18,453] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409031643188945700001842973448_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_202409031643188945700001842973448_0001_m_000000
[2024-09-03 16:43:18,453] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkHadoopMapRedUtil: attempt_202409031643188945700001842973448_0001_m_000000_1: Committed
[2024-09-03 16:43:18,456] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:43:18,458] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 315 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:43:18,462] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:43:18,462] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,374 s
[2024-09-03 16:43:18,462] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:43:18,462] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:43:18,465] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,382994 s
[2024-09-03 16:43:18,488] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileFormatWriter: Write Job e90dd959-756a-4f01-a33d-d9ab7809bdfa committed.
[2024-09-03 16:43:18,492] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileFormatWriter: Finished processing stats for write job e90dd959-756a-4f01-a33d-d9ab7809bdfa.
[2024-09-03 16:43:18,536] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:43:18,537] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:43:18,537] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:43:18,548] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:18,549] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:18,549] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:18,575] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 10.325549 ms
[2024-09-03 16:43:18,579] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:43:18,587] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:43:18,588] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38799 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:43:18,589] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:18,590] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649576 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:43:18,609] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:18,611] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:43:18,611] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:43:18,611] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:43:18,611] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:43:18,614] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:43:18,634] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:43:18,639] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:43:18,642] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38799 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:43:18,643] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:43:18,644] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:43:18,645] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:43:18,657] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:43:18,657] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:43:18,668] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:18,668] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:18,669] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:18,711] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 9.06386 ms
[2024-09-03 16:43:18,718] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-455272, partition values: [empty row]
[2024-09-03 16:43:18,730] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO CodeGenerator: Code generated in 9.361266 ms
[2024-09-03 16:43:18,758] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409031643182707795262768759980_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409031643182707795262768759980_0002_m_000000
[2024-09-03 16:43:18,758] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkHadoopMapRedUtil: attempt_202409031643182707795262768759980_0002_m_000000_2: Committed
[2024-09-03 16:43:18,759] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:43:18,761] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 114 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:43:18,761] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:43:18,763] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,148 s
[2024-09-03 16:43:18,763] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:43:18,763] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:43:18,773] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,154295 s
[2024-09-03 16:43:18,792] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileFormatWriter: Write Job c16aeb80-6e9b-4df7-9711-af6be69d73b7 committed.
[2024-09-03 16:43:18,793] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO FileFormatWriter: Finished processing stats for write job c16aeb80-6e9b-4df7-9711-af6be69d73b7.
[2024-09-03 16:43:18,841] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:43:18,855] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:43:18,867] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:43:18,876] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:43:18,877] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO BlockManager: BlockManager stopped
[2024-09-03 16:43:18,895] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:43:18,900] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:43:18,915] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:43:18,916] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:43:18,918] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f9c22b14-d25b-487e-aa3a-c78c24b5adbd
[2024-09-03 16:43:18,920] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f83b69de-b929-4fc2-83de-1b30dde5e4d0
[2024-09-03 16:43:18,924] {spark_submit.py:495} INFO - 24/09/03 16:43:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f83b69de-b929-4fc2-83de-1b30dde5e4d0/pyspark-6cd5898d-aab1-47f7-9a04-44fed2f7a121
[2024-09-03 16:43:18,987] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240903T194310, end_date=20240903T194318
[2024-09-03 16:43:19,004] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:43:19,011] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:57:48,641] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 17:57:48,648] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 17:57:48,648] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:57:48,648] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:57:48,648] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:57:48,657] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 17:57:48,659] {standard_task_runner.py:52} INFO - Started process 328337 to run task
[2024-09-03 17:57:48,662] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '112', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_9h_eyk6', '--error-file', '/tmp/tmp88357zbl']
[2024-09-03 17:57:48,663] {standard_task_runner.py:80} INFO - Job 112: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:57:48,692] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:57:48,731] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 17:57:48,735] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:57:48,736] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 17:57:49,787] {spark_submit.py:495} INFO - 24/09/03 17:57:49 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:57:49,787] {spark_submit.py:495} INFO - 24/09/03 17:57:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:57:50,116] {spark_submit.py:495} INFO - 24/09/03 17:57:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:57:50,643] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:57:50,661] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:57:50,703] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO ResourceUtils: ==============================================================
[2024-09-03 17:57:50,708] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:57:50,708] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO ResourceUtils: ==============================================================
[2024-09-03 17:57:50,708] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:57:50,729] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:57:50,744] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:57:50,745] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:57:50,785] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:57:50,785] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:57:50,786] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:57:50,786] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:57:50,786] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:57:50,929] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO Utils: Successfully started service 'sparkDriver' on port 45851.
[2024-09-03 17:57:50,952] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:57:50,978] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:57:50,991] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:57:50,991] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:57:50,994] {spark_submit.py:495} INFO - 24/09/03 17:57:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:57:51,005] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-44d84090-c494-4831-9a86-a8dd684d982a
[2024-09-03 17:57:51,022] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:57:51,035] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:57:51,203] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:57:51,250] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:57:51,458] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:57:51,481] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44649.
[2024-09-03 17:57:51,482] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO NettyBlockTransferService: Server created on 192.168.2.128:44649
[2024-09-03 17:57:51,483] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:57:51,487] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 44649, None)
[2024-09-03 17:57:51,490] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:44649 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 44649, None)
[2024-09-03 17:57:51,492] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 44649, None)
[2024-09-03 17:57:51,493] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 44649, None)
[2024-09-03 17:57:51,957] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:57:51,958] {spark_submit.py:495} INFO - 24/09/03 17:57:51 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:57:52,706] {spark_submit.py:495} INFO - 24/09/03 17:57:52 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 17:57:52,759] {spark_submit.py:495} INFO - 24/09/03 17:57:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:57:54,379] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:57:54,380] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:57:54,383] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:57:54,632] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:57:54,677] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:57:54,681] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:44649 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:57:54,685] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:54,692] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649682 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:57:54,828] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:54,845] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:57:54,846] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:57:54,847] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:57:54,848] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:57:54,866] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:57:54,941] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:57:54,943] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:57:54,944] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:44649 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:57:54,944] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:57:54,956] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:57:54,957] {spark_submit.py:495} INFO - 24/09/03 17:57:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:57:55,005] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:57:55,020] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:57:55,276] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-455378, partition values: [empty row]
[2024-09-03 17:57:55,508] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO CodeGenerator: Code generated in 137.713129 ms
[2024-09-03 17:57:55,623] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 17:57:55,765] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 650 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:57:55,770] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:57:55,773] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,892 s
[2024-09-03 17:57:55,777] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:57:55,777] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:57:55,783] {spark_submit.py:495} INFO - 24/09/03 17:57:55 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,951626 s
[2024-09-03 17:57:56,150] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:57:56,151] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:57:56,152] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:57:56,218] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:56,218] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:56,220] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:56,304] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO CodeGenerator: Code generated in 26.495742 ms
[2024-09-03 17:57:56,344] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO CodeGenerator: Code generated in 24.691726 ms
[2024-09-03 17:57:56,352] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:57:56,360] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:57:56,361] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:44649 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:57:56,362] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:56,365] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649682 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:57:56,420] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:56,422] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:57:56,422] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:57:56,422] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:57:56,422] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:57:56,423] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:57:56,463] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 17:57:56,467] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:57:56,467] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:44649 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:57:56,468] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:57:56,470] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:57:56,470] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:57:56,473] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:57:56,474] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:57:56,527] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:56,528] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:56,528] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:56,589] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO CodeGenerator: Code generated in 21.992562 ms
[2024-09-03 17:57:56,592] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-455378, partition values: [empty row]
[2024-09-03 17:57:56,612] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO CodeGenerator: Code generated in 17.615775 ms
[2024-09-03 17:57:56,635] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO CodeGenerator: Code generated in 6.452417 ms
[2024-09-03 17:57:56,756] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: Saved output of task 'attempt_202409031757563883621260215543451_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_202409031757563883621260215543451_0001_m_000000
[2024-09-03 17:57:56,757] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkHadoopMapRedUtil: attempt_202409031757563883621260215543451_0001_m_000000_1: Committed
[2024-09-03 17:57:56,763] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:57:56,771] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 299 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:57:56,771] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:57:56,772] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,348 s
[2024-09-03 17:57:56,772] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:57:56,773] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:57:56,774] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,352824 s
[2024-09-03 17:57:56,791] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileFormatWriter: Write Job 726a1b25-f49d-445e-9820-166754f89fc2 committed.
[2024-09-03 17:57:56,793] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileFormatWriter: Finished processing stats for write job 726a1b25-f49d-445e-9820-166754f89fc2.
[2024-09-03 17:57:56,840] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:57:56,841] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:57:56,841] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:57:56,855] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:56,855] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:56,856] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:56,902] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO CodeGenerator: Code generated in 10.776149 ms
[2024-09-03 17:57:56,906] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:57:56,913] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:57:56,915] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:44649 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:57:56,917] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:56,919] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649682 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:57:56,935] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:56,937] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:57:56,937] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:57:56,937] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:57:56,937] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:57:56,939] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:57:56,957] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:57:56,959] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:57:56,961] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:44649 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:57:56,962] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:57:56,963] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:57:56,963] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:57:56,965] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:57:56,966] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:57:56,978] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:56,978] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:56,979] {spark_submit.py:495} INFO - 24/09/03 17:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:57,020] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO CodeGenerator: Code generated in 9.82866 ms
[2024-09-03 17:57:57,023] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-455378, partition values: [empty row]
[2024-09-03 17:57:57,040] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO CodeGenerator: Code generated in 14.490718 ms
[2024-09-03 17:57:57,076] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO FileOutputCommitter: Saved output of task 'attempt_202409031757566126174289534565809_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409031757566126174289534565809_0002_m_000000
[2024-09-03 17:57:57,076] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO SparkHadoopMapRedUtil: attempt_202409031757566126174289534565809_0002_m_000000_2: Committed
[2024-09-03 17:57:57,078] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:57:57,085] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 120 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:57:57,087] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,146 s
[2024-09-03 17:57:57,087] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:57:57,088] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:57:57,088] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:57:57,088] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,152476 s
[2024-09-03 17:57:57,106] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO FileFormatWriter: Write Job add5a46a-e187-43f0-8df9-55703317521b committed.
[2024-09-03 17:57:57,107] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO FileFormatWriter: Finished processing stats for write job add5a46a-e187-43f0-8df9-55703317521b.
[2024-09-03 17:57:57,137] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:57:57,143] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:57:57,154] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:57:57,161] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:57:57,162] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO BlockManager: BlockManager stopped
[2024-09-03 17:57:57,167] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:57:57,169] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:57:57,173] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:57:57,174] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:57:57,174] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-c8179027-c6ca-45d6-8572-2407b39549b1/pyspark-4180eb1c-d074-4ea3-ad5e-3d737b73de4a
[2024-09-03 17:57:57,176] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-09220bed-cc92-440f-be76-f568f70861da
[2024-09-03 17:57:57,179] {spark_submit.py:495} INFO - 24/09/03 17:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-c8179027-c6ca-45d6-8572-2407b39549b1
[2024-09-03 17:57:57,237] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240903T205748, end_date=20240903T205757
[2024-09-03 17:57:57,267] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:57:57,300] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:18:53,555] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 18:18:53,565] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 18:18:53,565] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:18:53,566] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:18:53,566] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:18:53,576] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 18:18:53,580] {standard_task_runner.py:52} INFO - Started process 340520 to run task
[2024-09-03 18:18:53,584] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '111', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpch51cn8j', '--error-file', '/tmp/tmpk5_hosz1']
[2024-09-03 18:18:53,585] {standard_task_runner.py:80} INFO - Job 111: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:18:53,636] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:18:53,702] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 18:18:53,707] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:18:53,709] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 18:18:55,177] {spark_submit.py:495} INFO - 24/09/03 18:18:55 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:18:55,178] {spark_submit.py:495} INFO - 24/09/03 18:18:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:18:55,600] {spark_submit.py:495} INFO - 24/09/03 18:18:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:18:56,327] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:18:56,338] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:18:56,401] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO ResourceUtils: ==============================================================
[2024-09-03 18:18:56,401] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:18:56,401] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO ResourceUtils: ==============================================================
[2024-09-03 18:18:56,401] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:18:56,427] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:18:56,440] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:18:56,441] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:18:56,483] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:18:56,483] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:18:56,483] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:18:56,483] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:18:56,483] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:18:56,680] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO Utils: Successfully started service 'sparkDriver' on port 41633.
[2024-09-03 18:18:56,723] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:18:56,762] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:18:56,783] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:18:56,784] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:18:56,788] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:18:56,800] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d9db1ba8-ee4e-45f2-8845-b0ec28584668
[2024-09-03 18:18:56,821] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:18:56,844] {spark_submit.py:495} INFO - 24/09/03 18:18:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:18:57,161] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:18:57,238] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:18:57,511] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:18:57,556] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41237.
[2024-09-03 18:18:57,558] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO NettyBlockTransferService: Server created on 192.168.2.128:41237
[2024-09-03 18:18:57,562] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:18:57,568] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 41237, None)
[2024-09-03 18:18:57,572] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:41237 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 41237, None)
[2024-09-03 18:18:57,580] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 41237, None)
[2024-09-03 18:18:57,583] {spark_submit.py:495} INFO - 24/09/03 18:18:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 41237, None)
[2024-09-03 18:18:58,370] {spark_submit.py:495} INFO - 24/09/03 18:18:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:18:58,370] {spark_submit.py:495} INFO - 24/09/03 18:18:58 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:18:59,767] {spark_submit.py:495} INFO - 24/09/03 18:18:59 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.
[2024-09-03 18:18:59,835] {spark_submit.py:495} INFO - 24/09/03 18:18:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:19:02,278] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:19:02,280] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:19:02,285] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:19:02,649] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:19:02,711] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:19:02,715] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:41237 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:19:02,720] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:02,733] {spark_submit.py:495} INFO - 24/09/03 18:19:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:19:03,049] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:03,142] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:19:03,142] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:19:03,144] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:19:03,145] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:19:03,152] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:19:03,353] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:19:03,358] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:19:03,359] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:41237 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:19:03,360] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:19:03,386] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:19:03,388] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:19:03,505] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:19:03,551] {spark_submit.py:495} INFO - 24/09/03 18:19:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:19:04,181] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4589, partition values: [empty row]
[2024-09-03 18:19:04,537] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO CodeGenerator: Code generated in 235.401645 ms
[2024-09-03 18:19:04,595] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:19:04,619] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1135 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:19:04,621] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:19:04,778] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,440 s
[2024-09-03 18:19:04,799] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:19:04,799] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:19:04,805] {spark_submit.py:495} INFO - 24/09/03 18:19:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,751683 s
[2024-09-03 18:19:05,297] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:19:05,299] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:19:05,299] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:19:05,400] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:05,400] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:05,401] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:05,528] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO CodeGenerator: Code generated in 39.406536 ms
[2024-09-03 18:19:05,606] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO CodeGenerator: Code generated in 55.891778 ms
[2024-09-03 18:19:05,619] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:19:05,649] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:19:05,651] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:41237 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:19:05,653] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:05,661] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:19:05,773] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:05,777] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:19:05,778] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:19:05,779] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:19:05,779] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:19:05,803] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:19:05,889] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:19:05,893] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:19:05,896] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:41237 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:19:05,897] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:19:05,898] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:19:05,898] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:19:05,904] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:19:05,905] {spark_submit.py:495} INFO - 24/09/03 18:19:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:19:06,032] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:06,033] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:06,034] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:06,137] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO CodeGenerator: Code generated in 26.468792 ms
[2024-09-03 18:19:06,143] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4589, partition values: [empty row]
[2024-09-03 18:19:06,169] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO CodeGenerator: Code generated in 23.698899 ms
[2024-09-03 18:19:06,217] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO CodeGenerator: Code generated in 15.626245 ms
[2024-09-03 18:19:06,272] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: Saved output of task 'attempt_20240903181905133208976198530655_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_20240903181905133208976198530655_0001_m_000000
[2024-09-03 18:19:06,274] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SparkHadoopMapRedUtil: attempt_20240903181905133208976198530655_0001_m_000000_1: Committed
[2024-09-03 18:19:06,280] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:19:06,283] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 383 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:19:06,285] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,478 s
[2024-09-03 18:19:06,285] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:19:06,286] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:19:06,286] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:19:06,295] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,513324 s
[2024-09-03 18:19:06,328] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileFormatWriter: Write Job a2a1ab9e-4f26-44e2-817a-c450661e208d committed.
[2024-09-03 18:19:06,332] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileFormatWriter: Finished processing stats for write job a2a1ab9e-4f26-44e2-817a-c450661e208d.
[2024-09-03 18:19:06,394] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:19:06,395] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:19:06,396] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:19:06,417] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:06,419] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:06,420] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:06,468] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO CodeGenerator: Code generated in 12.518426 ms
[2024-09-03 18:19:06,475] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:19:06,489] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:19:06,491] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:41237 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:19:06,492] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:06,498] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:19:06,549] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:19:06,574] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:19:06,576] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:19:06,576] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:19:06,577] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:19:06,580] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:19:06,636] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:19:06,639] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:19:06,640] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:41237 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:19:06,641] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:19:06,645] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:19:06,646] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:19:06,651] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:19:06,655] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:19:06,681] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:19:06,682] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:19:06,683] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:19:06,816] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO CodeGenerator: Code generated in 26.29744 ms
[2024-09-03 18:19:06,820] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4589, partition values: [empty row]
[2024-09-03 18:19:06,852] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO CodeGenerator: Code generated in 21.903241 ms
[2024-09-03 18:19:06,873] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileOutputCommitter: Saved output of task 'attempt_202409031819067127812754747784287_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409031819067127812754747784287_0002_m_000000
[2024-09-03 18:19:06,874] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SparkHadoopMapRedUtil: attempt_202409031819067127812754747784287_0002_m_000000_2: Committed
[2024-09-03 18:19:06,882] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:19:06,886] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 234 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:19:06,886] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:19:06,888] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,305 s
[2024-09-03 18:19:06,888] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:19:06,889] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:19:06,890] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,330192 s
[2024-09-03 18:19:06,924] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileFormatWriter: Write Job 043f7821-50a9-4624-b172-a815909b6f85 committed.
[2024-09-03 18:19:06,931] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO FileFormatWriter: Finished processing stats for write job 043f7821-50a9-4624-b172-a815909b6f85.
[2024-09-03 18:19:06,986] {spark_submit.py:495} INFO - 24/09/03 18:19:06 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:19:07,003] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:19:07,034] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:19:07,087] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:19:07,087] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO BlockManager: BlockManager stopped
[2024-09-03 18:19:07,103] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:19:07,107] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:19:07,142] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:19:07,143] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:19:07,144] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-4122858d-bee3-41a0-9f28-2f555ba5ee9e
[2024-09-03 18:19:07,150] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-4122858d-bee3-41a0-9f28-2f555ba5ee9e/pyspark-760c8e1c-0b27-4706-81f4-d10e4bd1b472
[2024-09-03 18:19:07,155] {spark_submit.py:495} INFO - 24/09/03 18:19:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-618acb6a-b0eb-410f-9386-3e4419d9d260
[2024-09-03 18:19:07,247] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240903T211853, end_date=20240903T211907
[2024-09-03 18:19:07,287] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:19:07,305] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:24:58,166] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 18:24:58,172] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 18:24:58,172] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:24:58,172] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:24:58,172] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:24:58,184] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 18:24:58,186] {standard_task_runner.py:52} INFO - Started process 346940 to run task
[2024-09-03 18:24:58,190] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '110', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpng4lv5xr', '--error-file', '/tmp/tmpload0as1']
[2024-09-03 18:24:58,190] {standard_task_runner.py:80} INFO - Job 110: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:24:58,229] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:24:58,276] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 18:24:58,281] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:24:58,282] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 18:24:59,535] {spark_submit.py:495} INFO - 24/09/03 18:24:59 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:24:59,539] {spark_submit.py:495} INFO - 24/09/03 18:24:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:24:59,986] {spark_submit.py:495} INFO - 24/09/03 18:24:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:25:00,652] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:25:00,664] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:25:00,710] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO ResourceUtils: ==============================================================
[2024-09-03 18:25:00,711] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:25:00,711] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO ResourceUtils: ==============================================================
[2024-09-03 18:25:00,711] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:25:00,735] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:25:00,750] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:25:00,751] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:25:00,813] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:25:00,813] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:25:00,813] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:25:00,813] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:25:00,814] {spark_submit.py:495} INFO - 24/09/03 18:25:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:25:01,018] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO Utils: Successfully started service 'sparkDriver' on port 41565.
[2024-09-03 18:25:01,045] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:25:01,076] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:25:01,098] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:25:01,099] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:25:01,104] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:25:01,126] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2dbdab2a-5e31-4e7f-b065-bc4c39d1f18d
[2024-09-03 18:25:01,171] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:25:01,214] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:25:01,466] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:25:01,563] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:25:01,926] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:25:01,955] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39915.
[2024-09-03 18:25:01,956] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO NettyBlockTransferService: Server created on 192.168.2.128:39915
[2024-09-03 18:25:01,959] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:25:01,965] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 39915, None)
[2024-09-03 18:25:01,970] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:39915 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 39915, None)
[2024-09-03 18:25:01,977] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 39915, None)
[2024-09-03 18:25:01,979] {spark_submit.py:495} INFO - 24/09/03 18:25:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 39915, None)
[2024-09-03 18:25:02,572] {spark_submit.py:495} INFO - 24/09/03 18:25:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:25:02,572] {spark_submit.py:495} INFO - 24/09/03 18:25:02 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:25:03,528] {spark_submit.py:495} INFO - 24/09/03 18:25:03 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2024-09-03 18:25:03,585] {spark_submit.py:495} INFO - 24/09/03 18:25:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:25:06,631] {spark_submit.py:495} INFO - 24/09/03 18:25:06 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:25:06,632] {spark_submit.py:495} INFO - 24/09/03 18:25:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:25:06,636] {spark_submit.py:495} INFO - 24/09/03 18:25:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:25:07,022] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:25:07,088] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:25:07,093] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:39915 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:25:07,098] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:07,107] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198758 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:25:07,379] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:07,426] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:25:07,428] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:25:07,428] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:25:07,433] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:25:07,464] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:25:07,603] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:25:07,611] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:25:07,613] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:39915 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:25:07,614] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:25:07,635] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:25:07,636] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:25:07,711] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:25:07,771] {spark_submit.py:495} INFO - 24/09/03 18:25:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:25:08,129] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4454, partition values: [empty row]
[2024-09-03 18:25:08,491] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO CodeGenerator: Code generated in 239.998216 ms
[2024-09-03 18:25:08,557] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:25:08,569] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 883 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:25:08,586] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,087 s
[2024-09-03 18:25:08,715] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:25:08,722] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:25:08,722] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:25:08,726] {spark_submit.py:495} INFO - 24/09/03 18:25:08 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,345196 s
[2024-09-03 18:25:09,269] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:25:09,271] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:25:09,273] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:25:09,381] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:09,381] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:09,383] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:09,532] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO CodeGenerator: Code generated in 49.422179 ms
[2024-09-03 18:25:09,612] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO CodeGenerator: Code generated in 50.350894 ms
[2024-09-03 18:25:09,623] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:25:09,639] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:25:09,646] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:39915 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:25:09,648] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:09,651] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198758 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:25:09,731] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:09,735] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:25:09,736] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:25:09,737] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:25:09,737] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:25:09,744] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:25:09,833] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 18:25:09,837] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:25:09,838] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:39915 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:25:09,845] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:25:09,847] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:25:09,848] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:25:09,854] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:25:09,861] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:25:09,997] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:09,997] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:10,000] {spark_submit.py:495} INFO - 24/09/03 18:25:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:10,165] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO CodeGenerator: Code generated in 70.830959 ms
[2024-09-03 18:25:10,170] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4454, partition values: [empty row]
[2024-09-03 18:25:10,232] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO CodeGenerator: Code generated in 51.547092 ms
[2024-09-03 18:25:10,288] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO CodeGenerator: Code generated in 16.490908 ms
[2024-09-03 18:25:10,354] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileOutputCommitter: Saved output of task 'attempt_202409031825091837719864537879204_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_202409031825091837719864537879204_0001_m_000000
[2024-09-03 18:25:10,355] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SparkHadoopMapRedUtil: attempt_202409031825091837719864537879204_0001_m_000000_1: Committed
[2024-09-03 18:25:10,368] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:25:10,370] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 519 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:25:10,370] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:25:10,372] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,624 s
[2024-09-03 18:25:10,373] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:25:10,373] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:25:10,373] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,640748 s
[2024-09-03 18:25:10,419] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileFormatWriter: Write Job 32d9f45a-825f-45c7-9a40-a5c5b89fcce7 committed.
[2024-09-03 18:25:10,425] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileFormatWriter: Finished processing stats for write job 32d9f45a-825f-45c7-9a40-a5c5b89fcce7.
[2024-09-03 18:25:10,502] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:25:10,503] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:25:10,504] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:25:10,524] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:10,528] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:10,528] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:10,597] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO CodeGenerator: Code generated in 23.813778 ms
[2024-09-03 18:25:10,605] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:25:10,636] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:25:10,642] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:39915 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:25:10,643] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:10,648] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198758 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:25:10,683] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:10,685] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:25:10,685] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:25:10,685] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:25:10,686] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:25:10,698] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:25:10,754] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:25:10,762] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:25:10,763] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:39915 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:25:10,766] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:25:10,767] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:25:10,767] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:25:10,774] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:25:10,779] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:25:10,825] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:10,825] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:10,831] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:10,934] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO CodeGenerator: Code generated in 18.863744 ms
[2024-09-03 18:25:10,937] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4454, partition values: [empty row]
[2024-09-03 18:25:10,966] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO CodeGenerator: Code generated in 26.936316 ms
[2024-09-03 18:25:10,983] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO FileOutputCommitter: Saved output of task 'attempt_202409031825104595613574158624934_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409031825104595613574158624934_0002_m_000000
[2024-09-03 18:25:10,984] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO SparkHadoopMapRedUtil: attempt_202409031825104595613574158624934_0002_m_000000_2: Committed
[2024-09-03 18:25:10,986] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:25:10,991] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 219 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:25:10,992] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:25:10,998] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,297 s
[2024-09-03 18:25:10,998] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:25:10,998] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:25:10,998] {spark_submit.py:495} INFO - 24/09/03 18:25:10 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,313812 s
[2024-09-03 18:25:11,020] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO FileFormatWriter: Write Job ec4e41b3-5349-4917-93bd-42edefe91e79 committed.
[2024-09-03 18:25:11,021] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO FileFormatWriter: Finished processing stats for write job ec4e41b3-5349-4917-93bd-42edefe91e79.
[2024-09-03 18:25:11,090] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:25:11,105] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:25:11,154] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:25:11,179] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:25:11,179] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO BlockManager: BlockManager stopped
[2024-09-03 18:25:11,187] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:25:11,193] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:25:11,237] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:25:11,237] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:25:11,237] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-513d84d7-b6b0-4826-8825-6e3e05c4c09f/pyspark-7a070953-1d37-445f-b1b2-c85904b601ff
[2024-09-03 18:25:11,251] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-513d84d7-b6b0-4826-8825-6e3e05c4c09f
[2024-09-03 18:25:11,257] {spark_submit.py:495} INFO - 24/09/03 18:25:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c13b70b-1736-4bfc-b511-8e71c664fab4
[2024-09-03 18:25:11,493] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240903T212458, end_date=20240903T212511
[2024-09-03 18:25:11,533] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:25:11,565] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:55:36,279] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 22:55:36,284] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 22:55:36,284] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:55:36,284] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:55:36,284] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:55:36,294] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 22:55:36,296] {standard_task_runner.py:52} INFO - Started process 455132 to run task
[2024-09-03 22:55:36,299] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '155', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpken5yk2a', '--error-file', '/tmp/tmpa6_3p16t']
[2024-09-03 22:55:36,299] {standard_task_runner.py:80} INFO - Job 155: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:55:36,333] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:55:36,376] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 22:55:36,379] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:55:36,380] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 22:55:37,371] {spark_submit.py:495} INFO - 24/09/03 22:55:37 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:55:37,372] {spark_submit.py:495} INFO - 24/09/03 22:55:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:55:37,711] {spark_submit.py:495} INFO - 24/09/03 22:55:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:55:38,282] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:55:38,291] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:55:38,324] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO ResourceUtils: ==============================================================
[2024-09-03 22:55:38,325] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:55:38,326] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO ResourceUtils: ==============================================================
[2024-09-03 22:55:38,327] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:55:38,346] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:55:38,360] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:55:38,361] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:55:38,396] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:55:38,397] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:55:38,397] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:55:38,397] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:55:38,397] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:55:38,566] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO Utils: Successfully started service 'sparkDriver' on port 38413.
[2024-09-03 22:55:38,590] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:55:38,617] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:55:38,634] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:55:38,634] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:55:38,638] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:55:38,650] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-97fbb5fc-13e5-4e3c-9608-70c2073fef93
[2024-09-03 22:55:38,671] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:55:38,687] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:55:38,881] {spark_submit.py:495} INFO - 24/09/03 22:55:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:55:38,889] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:55:38,940] {spark_submit.py:495} INFO - 24/09/03 22:55:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:55:39,164] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:55:39,189] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41529.
[2024-09-03 22:55:39,189] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO NettyBlockTransferService: Server created on 192.168.2.128:41529
[2024-09-03 22:55:39,190] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:55:39,195] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 41529, None)
[2024-09-03 22:55:39,197] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:41529 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 41529, None)
[2024-09-03 22:55:39,200] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 41529, None)
[2024-09-03 22:55:39,200] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 41529, None)
[2024-09-03 22:55:39,640] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:55:39,641] {spark_submit.py:495} INFO - 24/09/03 22:55:39 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:55:40,450] {spark_submit.py:495} INFO - 24/09/03 22:55:40 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 22:55:40,501] {spark_submit.py:495} INFO - 24/09/03 22:55:40 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:55:42,180] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:55:42,181] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:55:42,185] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:55:42,450] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:55:42,493] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:55:42,495] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:41529 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:55:42,500] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:42,507] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198832 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:55:42,677] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:42,691] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:55:42,691] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:55:42,691] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:55:42,692] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:55:42,697] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:55:42,775] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:55:42,778] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:55:42,778] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:41529 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:55:42,779] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:55:42,791] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:55:42,793] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:55:42,839] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:55:42,855] {spark_submit.py:495} INFO - 24/09/03 22:55:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:55:43,065] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4528, partition values: [empty row]
[2024-09-03 22:55:43,306] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO CodeGenerator: Code generated in 154.439147 ms
[2024-09-03 22:55:43,349] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 22:55:43,360] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 531 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:55:43,363] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:55:43,367] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,657 s
[2024-09-03 22:55:43,466] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:55:43,466] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:55:43,469] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,790048 s
[2024-09-03 22:55:43,857] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:55:43,857] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:55:43,857] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:55:43,920] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:43,920] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:43,922] {spark_submit.py:495} INFO - 24/09/03 22:55:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:44,002] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 28.876942 ms
[2024-09-03 22:55:44,040] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 25.036998 ms
[2024-09-03 22:55:44,045] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:55:44,052] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:55:44,053] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:41529 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:55:44,054] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:44,056] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198832 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:55:44,115] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:44,117] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:55:44,117] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:55:44,117] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:55:44,117] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:55:44,121] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:55:44,165] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 22:55:44,167] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:55:44,168] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:41529 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:55:44,169] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:55:44,170] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:55:44,170] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:55:44,173] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:55:44,174] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:55:44,226] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:44,226] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:44,227] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:44,295] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 23.0578 ms
[2024-09-03 22:55:44,298] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4528, partition values: [empty row]
[2024-09-03 22:55:44,325] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 23.164811 ms
[2024-09-03 22:55:44,351] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 4.179213 ms
[2024-09-03 22:55:44,385] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: Saved output of task 'attempt_202409032255446972497149643122985_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_202409032255446972497149643122985_0001_m_000000
[2024-09-03 22:55:44,386] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkHadoopMapRedUtil: attempt_202409032255446972497149643122985_0001_m_000000_1: Committed
[2024-09-03 22:55:44,391] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:55:44,398] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 227 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:55:44,398] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:55:44,399] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,276 s
[2024-09-03 22:55:44,402] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:55:44,402] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:55:44,402] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,284990 s
[2024-09-03 22:55:44,414] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileFormatWriter: Write Job fccfafec-c81b-4af4-85e6-23767780030b committed.
[2024-09-03 22:55:44,417] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileFormatWriter: Finished processing stats for write job fccfafec-c81b-4af4-85e6-23767780030b.
[2024-09-03 22:55:44,461] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:55:44,461] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:55:44,461] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:55:44,472] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:44,473] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:44,473] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:44,497] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 9.683629 ms
[2024-09-03 22:55:44,501] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:55:44,510] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:55:44,511] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:41529 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:55:44,512] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:44,513] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198832 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:55:44,528] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:44,529] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:55:44,529] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:55:44,529] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:55:44,529] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:55:44,530] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:55:44,547] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:55:44,549] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:55:44,550] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:41529 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:55:44,551] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:55:44,555] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:55:44,555] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:55:44,556] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:55:44,557] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:55:44,571] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:44,571] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:44,571] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:44,600] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 9.171363 ms
[2024-09-03 22:55:44,604] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-4528, partition values: [empty row]
[2024-09-03 22:55:44,617] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO CodeGenerator: Code generated in 10.761108 ms
[2024-09-03 22:55:44,626] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileOutputCommitter: Saved output of task 'attempt_202409032255446390005065250765361_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409032255446390005065250765361_0002_m_000000
[2024-09-03 22:55:44,626] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkHadoopMapRedUtil: attempt_202409032255446390005065250765361_0002_m_000000_2: Committed
[2024-09-03 22:55:44,627] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:55:44,637] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 80 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:55:44,637] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:55:44,638] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,107 s
[2024-09-03 22:55:44,642] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:55:44,643] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:55:44,644] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,110829 s
[2024-09-03 22:55:44,660] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileFormatWriter: Write Job ebefbe4c-7a09-4a7e-a275-272b62575faf committed.
[2024-09-03 22:55:44,664] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO FileFormatWriter: Finished processing stats for write job ebefbe4c-7a09-4a7e-a275-272b62575faf.
[2024-09-03 22:55:44,698] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:55:44,708] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:55:44,721] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:55:44,776] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:55:44,777] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO BlockManager: BlockManager stopped
[2024-09-03 22:55:44,782] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:55:44,784] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:55:44,792] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:55:44,792] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:55:44,793] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-582d3912-cd13-4f07-ba4c-fe04637c683f/pyspark-1904cd70-4a7a-41e7-979f-fe5af8bc219b
[2024-09-03 22:55:44,795] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc851f5f-3663-426e-8465-34b9076d68fd
[2024-09-03 22:55:44,797] {spark_submit.py:495} INFO - 24/09/03 22:55:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-582d3912-cd13-4f07-ba4c-fe04637c683f
[2024-09-03 22:55:44,914] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240904T015536, end_date=20240904T015544
[2024-09-03 22:55:44,952] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:55:44,967] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 23:34:37,845] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 23:34:37,851] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [queued]>
[2024-09-03 23:34:37,851] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:34:37,851] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 23:34:37,851] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:34:37,864] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-31 00:00:00+00:00
[2024-09-03 23:34:37,869] {standard_task_runner.py:52} INFO - Started process 474239 to run task
[2024-09-03 23:34:37,874] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-31T00:00:00+00:00', '--job-id', '80', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpqjwq7s72', '--error-file', '/tmp/tmpezajbeu1']
[2024-09-03 23:34:37,875] {standard_task_runner.py:80} INFO - Job 80: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 23:34:37,962] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-31T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 23:34:38,027] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-31T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-31T00:00:00+00:00
[2024-09-03 23:34:38,032] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 23:34:38,033] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240831
[2024-09-03 23:34:39,485] {spark_submit.py:495} INFO - 24/09/03 23:34:39 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 23:34:39,485] {spark_submit.py:495} INFO - 24/09/03 23:34:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 23:34:40,002] {spark_submit.py:495} INFO - 24/09/03 23:34:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 23:34:40,724] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 23:34:40,743] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 23:34:40,799] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO ResourceUtils: ==============================================================
[2024-09-03 23:34:40,801] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 23:34:40,802] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO ResourceUtils: ==============================================================
[2024-09-03 23:34:40,803] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 23:34:40,843] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 23:34:40,863] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 23:34:40,865] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 23:34:40,944] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 23:34:40,946] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 23:34:40,947] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 23:34:40,948] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 23:34:40,949] {spark_submit.py:495} INFO - 24/09/03 23:34:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 23:34:41,238] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO Utils: Successfully started service 'sparkDriver' on port 36155.
[2024-09-03 23:34:41,304] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 23:34:41,386] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 23:34:41,416] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 23:34:41,418] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 23:34:41,425] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 23:34:41,445] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3b2bf138-f433-498f-acc0-b747cf04ea78
[2024-09-03 23:34:41,477] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 23:34:41,509] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 23:34:41,795] {spark_submit.py:495} INFO - 24/09/03 23:34:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 23:34:41,806] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 23:34:41,882] {spark_submit.py:495} INFO - 24/09/03 23:34:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 23:34:42,148] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 23:34:42,174] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36171.
[2024-09-03 23:34:42,174] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO NettyBlockTransferService: Server created on 192.168.2.128:36171
[2024-09-03 23:34:42,175] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 23:34:42,184] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36171, None)
[2024-09-03 23:34:42,187] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36171 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36171, None)
[2024-09-03 23:34:42,190] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36171, None)
[2024-09-03 23:34:42,191] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36171, None)
[2024-09-03 23:34:42,890] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 23:34:42,891] {spark_submit.py:495} INFO - 24/09/03 23:34:42 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 23:34:43,719] {spark_submit.py:495} INFO - 24/09/03 23:34:43 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.
[2024-09-03 23:34:43,777] {spark_submit.py:495} INFO - 24/09/03 23:34:43 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 23:34:45,780] {spark_submit.py:495} INFO - 24/09/03 23:34:45 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:34:45,781] {spark_submit.py:495} INFO - 24/09/03 23:34:45 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:34:45,784] {spark_submit.py:495} INFO - 24/09/03 23:34:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 23:34:46,056] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 23:34:46,102] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 23:34:46,105] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36171 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 23:34:46,114] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:46,119] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650692 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:34:46,264] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:46,285] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:34:46,286] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:34:46,287] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:34:46,287] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:34:46,293] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:34:46,388] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 23:34:46,392] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 23:34:46,394] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36171 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 23:34:46,394] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:34:46,407] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:34:46,408] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 23:34:46,474] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 23:34:46,500] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 23:34:46,800] {spark_submit.py:495} INFO - 24/09/03 23:34:46 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-456388, partition values: [empty row]
[2024-09-03 23:34:47,051] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO CodeGenerator: Code generated in 145.64771 ms
[2024-09-03 23:34:47,148] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 23:34:47,273] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 696 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:34:47,275] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 23:34:47,291] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,979 s
[2024-09-03 23:34:47,309] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:34:47,311] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 23:34:47,316] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,050950 s
[2024-09-03 23:34:47,769] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 23:34:47,771] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 23:34:47,771] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 23:34:47,850] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:47,850] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:47,851] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:47,953] {spark_submit.py:495} INFO - 24/09/03 23:34:47 INFO CodeGenerator: Code generated in 41.109141 ms
[2024-09-03 23:34:48,003] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 33.514931 ms
[2024-09-03 23:34:48,009] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 23:34:48,023] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 23:34:48,024] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36171 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 23:34:48,025] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:48,028] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650692 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:34:48,087] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:48,090] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:34:48,090] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:34:48,090] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:34:48,090] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:34:48,092] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:34:48,137] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 23:34:48,139] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 23:34:48,140] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36171 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 23:34:48,141] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:34:48,142] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:34:48,143] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 23:34:48,147] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:34:48,147] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 23:34:48,193] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:48,194] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:48,195] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:48,257] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 20.66731 ms
[2024-09-03 23:34:48,260] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-456388, partition values: [empty row]
[2024-09-03 23:34:48,285] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 20.487844 ms
[2024-09-03 23:34:48,306] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 6.792976 ms
[2024-09-03 23:34:48,410] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: Saved output of task 'attempt_202409032334488980702493206154651_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240831/_temporary/0/task_202409032334488980702493206154651_0001_m_000000
[2024-09-03 23:34:48,411] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkHadoopMapRedUtil: attempt_202409032334488980702493206154651_0001_m_000000_1: Committed
[2024-09-03 23:34:48,416] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 23:34:48,422] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 277 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:34:48,423] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,331 s
[2024-09-03 23:34:48,423] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:34:48,425] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 23:34:48,426] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 23:34:48,431] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,338881 s
[2024-09-03 23:34:48,442] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileFormatWriter: Write Job 753c1fad-e86f-4461-8322-5776cc794605 committed.
[2024-09-03 23:34:48,447] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileFormatWriter: Finished processing stats for write job 753c1fad-e86f-4461-8322-5776cc794605.
[2024-09-03 23:34:48,500] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:34:48,501] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:34:48,502] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 23:34:48,513] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:48,513] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:48,514] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:48,543] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 11.292869 ms
[2024-09-03 23:34:48,547] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 23:34:48,555] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 23:34:48,556] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36171 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 23:34:48,557] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:48,558] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650692 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:34:48,574] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:48,579] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:34:48,579] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:34:48,579] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:34:48,580] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:34:48,582] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:34:48,597] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 23:34:48,599] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 23:34:48,600] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36171 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 23:34:48,601] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:34:48,602] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:34:48,603] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 23:34:48,605] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:34:48,615] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 23:34:48,617] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:48,617] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:48,618] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:48,646] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 9.511727 ms
[2024-09-03 23:34:48,648] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-31T00:00:00.00Z/data_engineer_20240831.json, range: 0-456388, partition values: [empty row]
[2024-09-03 23:34:48,663] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO CodeGenerator: Code generated in 12.462594 ms
[2024-09-03 23:34:48,695] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileOutputCommitter: Saved output of task 'attempt_202409032334482473642390162116205_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240831/_temporary/0/task_202409032334482473642390162116205_0002_m_000000
[2024-09-03 23:34:48,695] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkHadoopMapRedUtil: attempt_202409032334482473642390162116205_0002_m_000000_2: Committed
[2024-09-03 23:34:48,695] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 23:34:48,696] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:34:48,698] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 23:34:48,699] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,115 s
[2024-09-03 23:34:48,699] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:34:48,699] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 23:34:48,699] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,123094 s
[2024-09-03 23:34:48,738] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileFormatWriter: Write Job 869bb328-3910-49a6-a982-44734b43775c committed.
[2024-09-03 23:34:48,738] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO FileFormatWriter: Finished processing stats for write job 869bb328-3910-49a6-a982-44734b43775c.
[2024-09-03 23:34:48,775] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 23:34:48,784] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 23:34:48,797] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 23:34:48,806] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO MemoryStore: MemoryStore cleared
[2024-09-03 23:34:48,806] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO BlockManager: BlockManager stopped
[2024-09-03 23:34:48,813] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 23:34:48,815] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 23:34:48,821] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 23:34:48,822] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 23:34:48,822] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-2e3ea1ce-4fac-4b14-b031-50fbad924a12
[2024-09-03 23:34:48,824] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-d4d24273-ff80-4c81-a591-584362a4bca1
[2024-09-03 23:34:48,831] {spark_submit.py:495} INFO - 24/09/03 23:34:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-2e3ea1ce-4fac-4b14-b031-50fbad924a12/pyspark-ef376c72-cf42-42bc-b746-8f4eaf1915aa
[2024-09-03 23:34:48,902] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240831T000000, start_date=20240904T023437, end_date=20240904T023448
[2024-09-03 23:34:48,957] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 23:34:48,972] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
