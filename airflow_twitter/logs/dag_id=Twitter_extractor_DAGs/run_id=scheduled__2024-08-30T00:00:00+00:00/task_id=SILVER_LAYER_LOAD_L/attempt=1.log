[2024-09-03 16:25:11,732] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 16:25:11,738] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 16:25:11,738] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:25:11,739] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:25:11,739] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:25:11,750] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 16:25:11,752] {standard_task_runner.py:52} INFO - Started process 286491 to run task
[2024-09-03 16:25:11,755] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '98', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpjl8dfehx', '--error-file', '/tmp/tmpzivhmiv8']
[2024-09-03 16:25:11,755] {standard_task_runner.py:80} INFO - Job 98: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:25:11,785] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:25:11,821] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 16:25:11,824] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:25:11,825] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 16:25:12,806] {spark_submit.py:495} INFO - 24/09/03 16:25:12 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:25:12,807] {spark_submit.py:495} INFO - 24/09/03 16:25:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:25:13,113] {spark_submit.py:495} INFO - 24/09/03 16:25:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:25:13,605] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:25:13,613] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:25:13,646] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO ResourceUtils: ==============================================================
[2024-09-03 16:25:13,647] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:25:13,648] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO ResourceUtils: ==============================================================
[2024-09-03 16:25:13,648] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:25:13,674] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:25:13,689] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:25:13,689] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:25:13,724] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:25:13,725] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:25:13,725] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:25:13,725] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:25:13,726] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:25:13,872] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO Utils: Successfully started service 'sparkDriver' on port 35617.
[2024-09-03 16:25:13,893] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:25:13,917] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:25:13,931] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:25:13,932] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:25:13,935] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:25:13,945] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5bbf2400-aa56-4027-aef3-bb03b4725fb5
[2024-09-03 16:25:13,962] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:25:13,974] {spark_submit.py:495} INFO - 24/09/03 16:25:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:25:14,138] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:25:14,182] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:25:14,384] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:25:14,405] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45407.
[2024-09-03 16:25:14,405] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO NettyBlockTransferService: Server created on 192.168.2.128:45407
[2024-09-03 16:25:14,406] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:25:14,414] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 45407, None)
[2024-09-03 16:25:14,417] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:45407 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 45407, None)
[2024-09-03 16:25:14,418] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 45407, None)
[2024-09-03 16:25:14,419] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 45407, None)
[2024-09-03 16:25:14,819] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:25:14,819] {spark_submit.py:495} INFO - 24/09/03 16:25:14 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:25:15,511] {spark_submit.py:495} INFO - 24/09/03 16:25:15 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 16:25:15,562] {spark_submit.py:495} INFO - 24/09/03 16:25:15 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:25:17,154] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:25:17,155] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:25:17,158] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:25:17,419] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:25:17,475] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:25:17,478] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:45407 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:25:17,482] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:17,490] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:25:17,644] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:17,660] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:25:17,660] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:25:17,660] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:25:17,662] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:25:17,674] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:25:17,749] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:25:17,753] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:25:17,754] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:45407 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:25:17,754] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:25:17,773] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:25:17,773] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:25:17,838] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:25:17,863] {spark_submit.py:495} INFO - 24/09/03 16:25:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:25:18,148] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-456000, partition values: [empty row]
[2024-09-03 16:25:18,377] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO CodeGenerator: Code generated in 138.061846 ms
[2024-09-03 16:25:18,451] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 16:25:18,463] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 643 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:25:18,560] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:25:18,563] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,871 s
[2024-09-03 16:25:18,568] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:25:18,572] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:25:18,581] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,932072 s
[2024-09-03 16:25:18,960] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:25:18,961] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:25:18,961] {spark_submit.py:495} INFO - 24/09/03 16:25:18 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:25:19,040] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:19,041] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:19,041] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:19,195] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 47.562316 ms
[2024-09-03 16:25:19,231] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 22.323037 ms
[2024-09-03 16:25:19,242] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:25:19,252] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:25:19,253] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:45407 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:25:19,254] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:19,257] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:25:19,326] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:19,328] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:25:19,328] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:25:19,328] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:25:19,328] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:25:19,330] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:25:19,370] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:25:19,372] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:25:19,373] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:45407 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:25:19,373] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:25:19,374] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:25:19,374] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:25:19,379] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:25:19,379] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:25:19,427] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:19,428] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:19,428] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:19,495] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 20.350038 ms
[2024-09-03 16:25:19,498] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-456000, partition values: [empty row]
[2024-09-03 16:25:19,523] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 21.561075 ms
[2024-09-03 16:25:19,548] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 7.456631 ms
[2024-09-03 16:25:19,656] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: Saved output of task 'attempt_202409031625195581134801043005398_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409031625195581134801043005398_0001_m_000000
[2024-09-03 16:25:19,656] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkHadoopMapRedUtil: attempt_202409031625195581134801043005398_0001_m_000000_1: Committed
[2024-09-03 16:25:19,661] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:25:19,666] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 291 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:25:19,666] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:25:19,667] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,337 s
[2024-09-03 16:25:19,667] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:25:19,668] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:25:19,669] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,341929 s
[2024-09-03 16:25:19,713] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileFormatWriter: Write Job f9014179-1b50-463c-b7bf-4424e9ebbab9 committed.
[2024-09-03 16:25:19,719] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileFormatWriter: Finished processing stats for write job f9014179-1b50-463c-b7bf-4424e9ebbab9.
[2024-09-03 16:25:19,761] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:25:19,762] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:25:19,762] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:25:19,773] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:19,773] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:19,773] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:19,799] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 9.75464 ms
[2024-09-03 16:25:19,803] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:25:19,809] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:25:19,810] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:45407 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:25:19,811] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:19,812] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:25:19,827] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:25:19,829] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:25:19,829] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:25:19,829] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:25:19,829] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:25:19,830] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:25:19,854] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:25:19,855] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:25:19,856] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:45407 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:25:19,856] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:25:19,857] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:25:19,857] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:25:19,859] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:25:19,859] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:25:19,869] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:25:19,869] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:25:19,869] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:25:19,895] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 10.021634 ms
[2024-09-03 16:25:19,897] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-456000, partition values: [empty row]
[2024-09-03 16:25:19,909] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO CodeGenerator: Code generated in 9.460591 ms
[2024-09-03 16:25:19,936] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileOutputCommitter: Saved output of task 'attempt_202409031625192255496559133022408_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409031625192255496559133022408_0002_m_000000
[2024-09-03 16:25:19,936] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO SparkHadoopMapRedUtil: attempt_202409031625192255496559133022408_0002_m_000000_2: Committed
[2024-09-03 16:25:19,937] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:25:19,940] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 82 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:25:19,940] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:25:19,942] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,110 s
[2024-09-03 16:25:19,942] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:25:19,942] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:25:19,942] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,113955 s
[2024-09-03 16:25:19,973] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileFormatWriter: Write Job 266d248c-0530-4fcf-b433-64719e5c2665 committed.
[2024-09-03 16:25:19,976] {spark_submit.py:495} INFO - 24/09/03 16:25:19 INFO FileFormatWriter: Finished processing stats for write job 266d248c-0530-4fcf-b433-64719e5c2665.
[2024-09-03 16:25:20,009] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:25:20,020] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:25:20,033] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:25:20,045] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:25:20,045] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO BlockManager: BlockManager stopped
[2024-09-03 16:25:20,049] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:25:20,052] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:25:20,057] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:25:20,057] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:25:20,057] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-6bf1398d-081e-45e5-8bb5-a6c68dc1ff31
[2024-09-03 16:25:20,061] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-3aacf2f4-912a-4bcb-bb8d-0e9930d8f04d
[2024-09-03 16:25:20,063] {spark_submit.py:495} INFO - 24/09/03 16:25:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-3aacf2f4-912a-4bcb-bb8d-0e9930d8f04d/pyspark-23cb32d5-e7ed-4349-9d41-4c674a680e48
[2024-09-03 16:25:20,120] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240903T192511, end_date=20240903T192520
[2024-09-03 16:25:20,135] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:25:20,143] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:42:44,509] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 16:42:44,516] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 16:42:44,516] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:42:44,516] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:42:44,516] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:42:44,525] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 16:42:44,528] {standard_task_runner.py:52} INFO - Started process 296833 to run task
[2024-09-03 16:42:44,530] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '97', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpv07aq6vu', '--error-file', '/tmp/tmp07q50hql']
[2024-09-03 16:42:44,531] {standard_task_runner.py:80} INFO - Job 97: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:42:44,562] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:42:44,600] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 16:42:44,603] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:42:44,604] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 16:42:45,518] {spark_submit.py:495} INFO - 24/09/03 16:42:45 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:42:45,519] {spark_submit.py:495} INFO - 24/09/03 16:42:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:42:45,832] {spark_submit.py:495} INFO - 24/09/03 16:42:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:42:46,335] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:42:46,344] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:42:46,380] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO ResourceUtils: ==============================================================
[2024-09-03 16:42:46,380] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:42:46,381] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO ResourceUtils: ==============================================================
[2024-09-03 16:42:46,381] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:42:46,400] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:42:46,414] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:42:46,415] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:42:46,455] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:42:46,456] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:42:46,456] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:42:46,456] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:42:46,456] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:42:46,602] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO Utils: Successfully started service 'sparkDriver' on port 37955.
[2024-09-03 16:42:46,624] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:42:46,651] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:42:46,667] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:42:46,668] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:42:46,670] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:42:46,681] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-373e351c-406d-4636-92b0-b18b370f2b1c
[2024-09-03 16:42:46,699] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:42:46,715] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:42:46,894] {spark_submit.py:495} INFO - 24/09/03 16:42:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:42:46,899] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:42:46,943] {spark_submit.py:495} INFO - 24/09/03 16:42:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:42:47,133] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:42:47,155] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45907.
[2024-09-03 16:42:47,156] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO NettyBlockTransferService: Server created on 192.168.2.128:45907
[2024-09-03 16:42:47,157] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:42:47,165] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 45907, None)
[2024-09-03 16:42:47,169] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:45907 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 45907, None)
[2024-09-03 16:42:47,173] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 45907, None)
[2024-09-03 16:42:47,174] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 45907, None)
[2024-09-03 16:42:47,606] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:42:47,606] {spark_submit.py:495} INFO - 24/09/03 16:42:47 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:42:48,318] {spark_submit.py:495} INFO - 24/09/03 16:42:48 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 16:42:48,367] {spark_submit.py:495} INFO - 24/09/03 16:42:48 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:42:49,958] {spark_submit.py:495} INFO - 24/09/03 16:42:49 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:42:49,959] {spark_submit.py:495} INFO - 24/09/03 16:42:49 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:42:49,962] {spark_submit.py:495} INFO - 24/09/03 16:42:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:42:50,211] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:42:50,253] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:42:50,255] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:45907 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:42:50,260] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:50,268] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198854 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:42:50,425] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:50,439] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:42:50,440] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:42:50,440] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:42:50,442] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:42:50,452] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:42:50,551] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:42:50,554] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:42:50,555] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:45907 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:42:50,556] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:42:50,567] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:42:50,568] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:42:50,612] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:42:50,628] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:42:50,853] {spark_submit.py:495} INFO - 24/09/03 16:42:50 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-4550, partition values: [empty row]
[2024-09-03 16:42:51,076] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO CodeGenerator: Code generated in 137.190265 ms
[2024-09-03 16:42:51,122] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 16:42:51,134] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 532 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:42:51,139] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:42:51,236] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,672 s
[2024-09-03 16:42:51,245] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:42:51,247] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:42:51,254] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,828606 s
[2024-09-03 16:42:51,714] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:42:51,715] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:42:51,715] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:42:51,785] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:51,785] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:51,786] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:51,884] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO CodeGenerator: Code generated in 32.032767 ms
[2024-09-03 16:42:51,922] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO CodeGenerator: Code generated in 24.461695 ms
[2024-09-03 16:42:51,927] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:42:51,935] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:42:51,936] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:45907 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:42:51,937] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:51,939] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198854 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:42:51,989] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:51,990] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:42:51,990] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:42:51,991] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:42:51,991] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:42:51,991] {spark_submit.py:495} INFO - 24/09/03 16:42:51 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:42:52,030] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:42:52,032] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:42:52,032] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:45907 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:42:52,033] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:42:52,033] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:42:52,034] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:42:52,037] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:42:52,038] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:42:52,097] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:52,098] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:52,099] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:52,190] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO CodeGenerator: Code generated in 45.602974 ms
[2024-09-03 16:42:52,194] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-4550, partition values: [empty row]
[2024-09-03 16:42:52,218] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO CodeGenerator: Code generated in 20.830734 ms
[2024-09-03 16:42:52,239] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO CodeGenerator: Code generated in 4.604543 ms
[2024-09-03 16:42:52,277] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409031642516393433687690868068_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409031642516393433687690868068_0001_m_000000
[2024-09-03 16:42:52,277] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkHadoopMapRedUtil: attempt_202409031642516393433687690868068_0001_m_000000_1: Committed
[2024-09-03 16:42:52,281] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:42:52,282] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 247 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:42:52,283] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:42:52,289] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,291 s
[2024-09-03 16:42:52,289] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:42:52,290] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:42:52,290] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,298356 s
[2024-09-03 16:42:52,326] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileFormatWriter: Write Job ce3ad1cc-eef0-4581-b9dc-3057c0f76aa3 committed.
[2024-09-03 16:42:52,329] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileFormatWriter: Finished processing stats for write job ce3ad1cc-eef0-4581-b9dc-3057c0f76aa3.
[2024-09-03 16:42:52,371] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:42:52,371] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:42:52,372] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:42:52,383] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:52,383] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:52,384] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:52,426] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO CodeGenerator: Code generated in 12.512886 ms
[2024-09-03 16:42:52,432] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:42:52,440] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:42:52,441] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:45907 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:42:52,442] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:52,443] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198854 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:42:52,460] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:52,462] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:42:52,463] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:42:52,463] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:42:52,464] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:42:52,464] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:42:52,488] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:42:52,491] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:42:52,492] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:45907 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:42:52,493] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:42:52,495] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:42:52,495] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:42:52,496] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:42:52,497] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:42:52,515] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:52,516] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:52,516] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:52,556] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO CodeGenerator: Code generated in 19.839052 ms
[2024-09-03 16:42:52,559] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-4550, partition values: [empty row]
[2024-09-03 16:42:52,574] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO CodeGenerator: Code generated in 11.588355 ms
[2024-09-03 16:42:52,586] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409031642522145373768775934821_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409031642522145373768775934821_0002_m_000000
[2024-09-03 16:42:52,587] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkHadoopMapRedUtil: attempt_202409031642522145373768775934821_0002_m_000000_2: Committed
[2024-09-03 16:42:52,589] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:42:52,596] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 100 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:42:52,598] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:42:52,600] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,132 s
[2024-09-03 16:42:52,601] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:42:52,601] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:42:52,602] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,138506 s
[2024-09-03 16:42:52,616] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileFormatWriter: Write Job 7524d437-0645-4f62-8e50-eb4a0d5ccd77 committed.
[2024-09-03 16:42:52,617] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO FileFormatWriter: Finished processing stats for write job 7524d437-0645-4f62-8e50-eb4a0d5ccd77.
[2024-09-03 16:42:52,654] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:42:52,665] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:42:52,678] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:42:52,689] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:42:52,691] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO BlockManager: BlockManager stopped
[2024-09-03 16:42:52,699] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:42:52,703] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:42:52,708] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:42:52,708] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:42:52,708] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-909e14df-3d41-47c7-b620-2fabcb729e86
[2024-09-03 16:42:52,710] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a9cc203-0c5f-47f3-ac49-4a6a86224086
[2024-09-03 16:42:52,715] {spark_submit.py:495} INFO - 24/09/03 16:42:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a9cc203-0c5f-47f3-ac49-4a6a86224086/pyspark-6cd3c1f6-042c-4c2d-9454-5cc9fde60acd
[2024-09-03 16:42:52,766] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240903T194244, end_date=20240903T194252
[2024-09-03 16:42:52,817] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:42:52,828] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:57:04,150] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 17:57:04,155] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 17:57:04,155] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:57:04,155] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:57:04,155] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:57:04,164] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 17:57:04,166] {standard_task_runner.py:52} INFO - Started process 327670 to run task
[2024-09-03 17:57:04,169] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '108', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpwdgn5tev', '--error-file', '/tmp/tmpzx0emvuk']
[2024-09-03 17:57:04,170] {standard_task_runner.py:80} INFO - Job 108: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:57:04,202] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:57:04,240] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 17:57:04,244] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:57:04,244] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 17:57:05,320] {spark_submit.py:495} INFO - 24/09/03 17:57:05 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:57:05,320] {spark_submit.py:495} INFO - 24/09/03 17:57:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:57:05,634] {spark_submit.py:495} INFO - 24/09/03 17:57:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:57:06,137] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:57:06,145] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:57:06,182] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO ResourceUtils: ==============================================================
[2024-09-03 17:57:06,183] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:57:06,183] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO ResourceUtils: ==============================================================
[2024-09-03 17:57:06,184] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:57:06,204] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:57:06,219] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:57:06,220] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:57:06,260] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:57:06,260] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:57:06,260] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:57:06,260] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:57:06,260] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:57:06,406] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO Utils: Successfully started service 'sparkDriver' on port 34281.
[2024-09-03 17:57:06,427] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:57:06,467] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:57:06,482] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:57:06,483] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:57:06,486] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:57:06,497] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f39b1446-6756-4c47-979f-d42d1174f0cf
[2024-09-03 17:57:06,515] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:57:06,532] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:57:06,708] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:57:06,755] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:57:06,948] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:57:06,973] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44421.
[2024-09-03 17:57:06,973] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO NettyBlockTransferService: Server created on 192.168.2.128:44421
[2024-09-03 17:57:06,975] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:57:06,981] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 44421, None)
[2024-09-03 17:57:06,987] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:44421 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 44421, None)
[2024-09-03 17:57:06,988] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 44421, None)
[2024-09-03 17:57:06,989] {spark_submit.py:495} INFO - 24/09/03 17:57:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 44421, None)
[2024-09-03 17:57:07,389] {spark_submit.py:495} INFO - 24/09/03 17:57:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:57:07,390] {spark_submit.py:495} INFO - 24/09/03 17:57:07 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:57:08,169] {spark_submit.py:495} INFO - 24/09/03 17:57:08 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 17:57:08,217] {spark_submit.py:495} INFO - 24/09/03 17:57:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:57:09,805] {spark_submit.py:495} INFO - 24/09/03 17:57:09 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:57:09,805] {spark_submit.py:495} INFO - 24/09/03 17:57:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:57:09,808] {spark_submit.py:495} INFO - 24/09/03 17:57:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:57:10,057] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:57:10,101] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:57:10,104] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:44421 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:57:10,108] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:10,117] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649194 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:57:10,249] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:10,269] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:57:10,269] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:57:10,269] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:57:10,271] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:57:10,280] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:57:10,391] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:57:10,393] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:57:10,394] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:44421 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:57:10,396] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:57:10,408] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:57:10,409] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:57:10,453] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:57:10,472] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:57:10,712] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-454890, partition values: [empty row]
[2024-09-03 17:57:10,928] {spark_submit.py:495} INFO - 24/09/03 17:57:10 INFO CodeGenerator: Code generated in 129.878702 ms
[2024-09-03 17:57:11,056] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 17:57:11,158] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 624 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:57:11,162] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:57:11,169] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,856 s
[2024-09-03 17:57:11,172] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:57:11,173] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:57:11,176] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,926032 s
[2024-09-03 17:57:11,582] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:57:11,584] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:57:11,584] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:57:11,655] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:11,656] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:11,656] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:11,733] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO CodeGenerator: Code generated in 30.41263 ms
[2024-09-03 17:57:11,772] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO CodeGenerator: Code generated in 24.106253 ms
[2024-09-03 17:57:11,778] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:57:11,789] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:57:11,790] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:44421 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:57:11,791] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:11,792] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649194 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:57:11,849] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:11,852] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:57:11,852] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:57:11,852] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:57:11,852] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:57:11,853] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:57:11,906] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:57:11,908] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:57:11,909] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:44421 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:57:11,909] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:57:11,910] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:57:11,910] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:57:11,915] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:57:11,919] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:57:11,966] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:11,966] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:11,967] {spark_submit.py:495} INFO - 24/09/03 17:57:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:12,042] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO CodeGenerator: Code generated in 31.472702 ms
[2024-09-03 17:57:12,045] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-454890, partition values: [empty row]
[2024-09-03 17:57:12,070] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO CodeGenerator: Code generated in 18.815712 ms
[2024-09-03 17:57:12,090] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO CodeGenerator: Code generated in 5.192082 ms
[2024-09-03 17:57:12,202] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409031757113867401514861326383_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409031757113867401514861326383_0001_m_000000
[2024-09-03 17:57:12,203] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkHadoopMapRedUtil: attempt_202409031757113867401514861326383_0001_m_000000_1: Committed
[2024-09-03 17:57:12,208] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:57:12,216] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 305 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:57:12,217] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,363 s
[2024-09-03 17:57:12,218] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:57:12,224] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:57:12,225] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:57:12,225] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,375855 s
[2024-09-03 17:57:12,241] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileFormatWriter: Write Job a91b89d9-14e3-490f-be51-67e88a6c7f8f committed.
[2024-09-03 17:57:12,244] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileFormatWriter: Finished processing stats for write job a91b89d9-14e3-490f-be51-67e88a6c7f8f.
[2024-09-03 17:57:12,298] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:57:12,299] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:57:12,300] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:57:12,309] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:12,310] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:12,310] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:12,336] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO CodeGenerator: Code generated in 10.495096 ms
[2024-09-03 17:57:12,341] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:57:12,349] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:57:12,350] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:44421 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:57:12,352] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:12,353] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649194 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:57:12,367] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:57:12,369] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:57:12,369] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:57:12,370] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:57:12,370] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:57:12,371] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:57:12,390] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:57:12,392] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:57:12,392] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:44421 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:57:12,393] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:57:12,394] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:57:12,394] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:57:12,396] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:57:12,398] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:57:12,412] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:57:12,414] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:57:12,415] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:57:12,452] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO CodeGenerator: Code generated in 9.045669 ms
[2024-09-03 17:57:12,455] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-454890, partition values: [empty row]
[2024-09-03 17:57:12,472] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO CodeGenerator: Code generated in 14.89636 ms
[2024-09-03 17:57:12,507] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409031757124445248217341156263_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409031757124445248217341156263_0002_m_000000
[2024-09-03 17:57:12,507] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkHadoopMapRedUtil: attempt_202409031757124445248217341156263_0002_m_000000_2: Committed
[2024-09-03 17:57:12,509] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:57:12,511] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 116 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:57:12,512] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,140 s
[2024-09-03 17:57:12,514] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:57:12,515] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:57:12,517] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:57:12,517] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,147164 s
[2024-09-03 17:57:12,534] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileFormatWriter: Write Job ec9b08b1-84e5-4448-ace8-6e37a4e5d99e committed.
[2024-09-03 17:57:12,535] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO FileFormatWriter: Finished processing stats for write job ec9b08b1-84e5-4448-ace8-6e37a4e5d99e.
[2024-09-03 17:57:12,578] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:57:12,589] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:57:12,607] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:57:12,620] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:57:12,621] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO BlockManager: BlockManager stopped
[2024-09-03 17:57:12,630] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:57:12,633] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:57:12,641] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:57:12,642] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:57:12,642] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-30947729-25df-4be2-8989-32c12b389b7d
[2024-09-03 17:57:12,647] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-df8f2e11-66de-41cb-8c58-26bfb614f1d7
[2024-09-03 17:57:12,651] {spark_submit.py:495} INFO - 24/09/03 17:57:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-df8f2e11-66de-41cb-8c58-26bfb614f1d7/pyspark-6feb822a-301b-4b7f-9c97-d5f389fc66b5
[2024-09-03 17:57:12,767] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240903T205704, end_date=20240903T205712
[2024-09-03 17:57:12,811] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:57:12,823] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:18:20,420] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 18:18:20,428] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 18:18:20,428] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:18:20,429] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:18:20,429] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:18:20,444] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 18:18:20,446] {standard_task_runner.py:52} INFO - Started process 340153 to run task
[2024-09-03 18:18:20,449] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '108', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmprfi95wkv', '--error-file', '/tmp/tmp4ppdojsg']
[2024-09-03 18:18:20,450] {standard_task_runner.py:80} INFO - Job 108: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:18:20,480] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:18:20,518] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 18:18:20,522] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:18:20,523] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 18:18:21,636] {spark_submit.py:495} INFO - 24/09/03 18:18:21 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:18:21,636] {spark_submit.py:495} INFO - 24/09/03 18:18:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:18:22,003] {spark_submit.py:495} INFO - 24/09/03 18:18:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:18:22,626] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:18:22,645] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:18:22,697] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO ResourceUtils: ==============================================================
[2024-09-03 18:18:22,698] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:18:22,699] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO ResourceUtils: ==============================================================
[2024-09-03 18:18:22,699] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:18:22,729] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:18:22,746] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:18:22,746] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:18:22,795] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:18:22,796] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:18:22,796] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:18:22,797] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:18:22,797] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:18:22,998] {spark_submit.py:495} INFO - 24/09/03 18:18:22 INFO Utils: Successfully started service 'sparkDriver' on port 35889.
[2024-09-03 18:18:23,032] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:18:23,070] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:18:23,090] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:18:23,091] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:18:23,097] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:18:23,113] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b991870c-c4a5-42a3-9dac-b4c493c98e7e
[2024-09-03 18:18:23,138] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:18:23,159] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:18:23,423] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:18:23,491] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:18:23,751] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:18:23,783] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34545.
[2024-09-03 18:18:23,783] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO NettyBlockTransferService: Server created on 192.168.2.128:34545
[2024-09-03 18:18:23,785] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:18:23,791] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34545, None)
[2024-09-03 18:18:23,795] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34545 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34545, None)
[2024-09-03 18:18:23,799] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34545, None)
[2024-09-03 18:18:23,799] {spark_submit.py:495} INFO - 24/09/03 18:18:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34545, None)
[2024-09-03 18:18:24,465] {spark_submit.py:495} INFO - 24/09/03 18:18:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:18:24,466] {spark_submit.py:495} INFO - 24/09/03 18:18:24 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:18:25,472] {spark_submit.py:495} INFO - 24/09/03 18:18:25 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2024-09-03 18:18:25,541] {spark_submit.py:495} INFO - 24/09/03 18:18:25 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2024-09-03 18:18:27,705] {spark_submit.py:495} INFO - 24/09/03 18:18:27 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:18:27,707] {spark_submit.py:495} INFO - 24/09/03 18:18:27 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:18:27,712] {spark_submit.py:495} INFO - 24/09/03 18:18:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:18:28,023] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:18:28,082] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:18:28,084] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34545 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:18:28,098] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:18:28,106] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649605 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:18:28,280] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:18:28,301] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:18:28,301] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:18:28,301] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:18:28,305] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:18:28,324] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:18:28,458] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:18:28,463] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:18:28,464] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34545 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:18:28,465] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:18:28,483] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:18:28,496] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:18:28,558] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:18:28,587] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:18:28,881] {spark_submit.py:495} INFO - 24/09/03 18:18:28 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455301, partition values: [empty row]
[2024-09-03 18:18:29,186] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO CodeGenerator: Code generated in 199.251035 ms
[2024-09-03 18:18:29,314] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 18:18:29,475] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 786 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:18:29,481] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:18:29,490] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,148 s
[2024-09-03 18:18:29,497] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:18:29,497] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:18:29,499] {spark_submit.py:495} INFO - 24/09/03 18:18:29 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,218658 s
[2024-09-03 18:18:30,008] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:18:30,011] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:18:30,012] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:18:30,103] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:18:30,104] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:18:30,105] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:18:30,218] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO CodeGenerator: Code generated in 48.189707 ms
[2024-09-03 18:18:30,262] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO CodeGenerator: Code generated in 25.977981 ms
[2024-09-03 18:18:30,269] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:18:30,279] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:18:30,281] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34545 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:18:30,283] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:18:30,285] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649605 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:18:30,354] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:18:30,356] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:18:30,356] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:18:30,356] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:18:30,356] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:18:30,357] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:18:30,421] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:18:30,427] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:18:30,428] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34545 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:18:30,429] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:18:30,429] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:18:30,429] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:18:30,433] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:18:30,433] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:18:30,498] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:18:30,498] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:18:30,499] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:18:30,615] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO CodeGenerator: Code generated in 51.213958 ms
[2024-09-03 18:18:30,619] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455301, partition values: [empty row]
[2024-09-03 18:18:30,650] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO CodeGenerator: Code generated in 27.409602 ms
[2024-09-03 18:18:30,685] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO CodeGenerator: Code generated in 5.043768 ms
[2024-09-03 18:18:30,866] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileOutputCommitter: Saved output of task 'attempt_202409031818305286991032984445365_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409031818305286991032984445365_0001_m_000000
[2024-09-03 18:18:30,867] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO SparkHadoopMapRedUtil: attempt_202409031818305286991032984445365_0001_m_000000_1: Committed
[2024-09-03 18:18:30,875] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:18:30,884] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 454 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:18:30,884] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:18:30,887] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,527 s
[2024-09-03 18:18:30,894] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:18:30,895] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:18:30,896] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,541910 s
[2024-09-03 18:18:30,916] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileFormatWriter: Write Job c2cb54e0-c887-4083-86c2-d6d38d4b52e8 committed.
[2024-09-03 18:18:30,922] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileFormatWriter: Finished processing stats for write job c2cb54e0-c887-4083-86c2-d6d38d4b52e8.
[2024-09-03 18:18:30,990] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:18:30,990] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:18:30,990] {spark_submit.py:495} INFO - 24/09/03 18:18:30 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:18:31,009] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:18:31,010] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:18:31,011] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:18:31,065] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO CodeGenerator: Code generated in 17.794865 ms
[2024-09-03 18:18:31,071] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:18:31,083] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:18:31,084] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34545 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:18:31,084] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:18:31,085] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649605 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:18:31,113] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:18:31,114] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:18:31,114] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:18:31,114] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:18:31,114] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:18:31,115] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:18:31,138] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:18:31,141] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:18:31,143] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34545 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:18:31,145] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:18:31,146] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:18:31,147] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:18:31,148] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:18:31,149] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:18:31,167] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:18:31,168] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:18:31,168] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:18:31,228] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO CodeGenerator: Code generated in 12.262993 ms
[2024-09-03 18:18:31,232] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455301, partition values: [empty row]
[2024-09-03 18:18:31,246] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO CodeGenerator: Code generated in 10.234713 ms
[2024-09-03 18:18:31,285] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileOutputCommitter: Saved output of task 'attempt_202409031818315838507136124538585_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409031818315838507136124538585_0002_m_000000
[2024-09-03 18:18:31,285] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkHadoopMapRedUtil: attempt_202409031818315838507136124538585_0002_m_000000_2: Committed
[2024-09-03 18:18:31,288] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:18:31,288] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 139 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:18:31,288] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:18:31,293] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,174 s
[2024-09-03 18:18:31,294] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:18:31,294] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:18:31,294] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,177929 s
[2024-09-03 18:18:31,329] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileFormatWriter: Write Job d7d0a881-18d8-499b-8027-e6e68fe57160 committed.
[2024-09-03 18:18:31,329] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO FileFormatWriter: Finished processing stats for write job d7d0a881-18d8-499b-8027-e6e68fe57160.
[2024-09-03 18:18:31,378] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:18:31,388] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:18:31,403] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:18:31,414] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:18:31,414] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO BlockManager: BlockManager stopped
[2024-09-03 18:18:31,423] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:18:31,429] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:18:31,451] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:18:31,451] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:18:31,452] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2f5d69e-f670-49b4-9444-fb9b2ebdf2b3
[2024-09-03 18:18:31,458] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-51f8acd5-5848-49a1-8eaf-22ab11b95e3e
[2024-09-03 18:18:31,462] {spark_submit.py:495} INFO - 24/09/03 18:18:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-51f8acd5-5848-49a1-8eaf-22ab11b95e3e/pyspark-93e52844-0f01-46f9-a45a-da7429899717
[2024-09-03 18:18:31,581] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240903T211820, end_date=20240903T211831
[2024-09-03 18:18:31,624] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:18:31,669] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:24:42,937] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 18:24:42,945] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 18:24:42,945] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:24:42,945] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:24:42,945] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:24:42,956] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 18:24:42,958] {standard_task_runner.py:52} INFO - Started process 346509 to run task
[2024-09-03 18:24:42,961] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '108', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmppv37a1z1', '--error-file', '/tmp/tmp55mpfhq6']
[2024-09-03 18:24:42,962] {standard_task_runner.py:80} INFO - Job 108: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:24:42,992] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:24:43,039] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 18:24:43,043] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:24:43,045] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 18:24:44,195] {spark_submit.py:495} INFO - 24/09/03 18:24:44 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:24:44,196] {spark_submit.py:495} INFO - 24/09/03 18:24:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:24:44,553] {spark_submit.py:495} INFO - 24/09/03 18:24:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:24:45,131] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:24:45,141] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:24:45,181] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO ResourceUtils: ==============================================================
[2024-09-03 18:24:45,182] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:24:45,182] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO ResourceUtils: ==============================================================
[2024-09-03 18:24:45,182] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:24:45,206] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:24:45,220] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:24:45,221] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:24:45,275] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:24:45,275] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:24:45,276] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:24:45,276] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:24:45,276] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:24:45,480] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO Utils: Successfully started service 'sparkDriver' on port 33443.
[2024-09-03 18:24:45,512] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:24:45,542] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:24:45,567] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:24:45,567] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:24:45,568] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:24:45,580] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c56849b4-e51e-4f43-8a1f-2c14c4760de2
[2024-09-03 18:24:45,601] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:24:45,625] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:24:45,915] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:24:45,970] {spark_submit.py:495} INFO - 24/09/03 18:24:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:24:46,217] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:24:46,263] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46421.
[2024-09-03 18:24:46,263] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO NettyBlockTransferService: Server created on 192.168.2.128:46421
[2024-09-03 18:24:46,263] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:24:46,268] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46421, None)
[2024-09-03 18:24:46,277] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46421 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46421, None)
[2024-09-03 18:24:46,291] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46421, None)
[2024-09-03 18:24:46,293] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46421, None)
[2024-09-03 18:24:46,863] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:24:46,864] {spark_submit.py:495} INFO - 24/09/03 18:24:46 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:24:47,693] {spark_submit.py:495} INFO - 24/09/03 18:24:47 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2024-09-03 18:24:47,773] {spark_submit.py:495} INFO - 24/09/03 18:24:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:24:49,748] {spark_submit.py:495} INFO - 24/09/03 18:24:49 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:24:49,750] {spark_submit.py:495} INFO - 24/09/03 18:24:49 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:24:49,755] {spark_submit.py:495} INFO - 24/09/03 18:24:49 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:24:50,044] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:24:50,087] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:24:50,090] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46421 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:24:50,095] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:24:50,102] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649891 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:24:50,256] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:24:50,285] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:24:50,285] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:24:50,286] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:24:50,289] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:24:50,311] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:24:50,457] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:24:50,462] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:24:50,463] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46421 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:24:50,463] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:24:50,480] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:24:50,481] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:24:50,555] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:24:50,578] {spark_submit.py:495} INFO - 24/09/03 18:24:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:24:51,111] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455587, partition values: [empty row]
[2024-09-03 18:24:51,486] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO CodeGenerator: Code generated in 216.682811 ms
[2024-09-03 18:24:51,622] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:24:51,767] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1116 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:24:51,769] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:24:51,772] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,439 s
[2024-09-03 18:24:51,783] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:24:51,784] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:24:51,790] {spark_submit.py:495} INFO - 24/09/03 18:24:51 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,531901 s
[2024-09-03 18:24:52,229] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:24:52,230] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:24:52,231] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:24:52,318] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:24:52,318] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:24:52,319] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:24:52,418] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO CodeGenerator: Code generated in 24.287774 ms
[2024-09-03 18:24:52,469] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO CodeGenerator: Code generated in 34.971432 ms
[2024-09-03 18:24:52,477] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:24:52,484] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:24:52,485] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46421 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:24:52,486] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:24:52,490] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649891 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:24:52,548] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:24:52,550] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:24:52,551] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:24:52,551] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:24:52,551] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:24:52,552] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:24:52,617] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:24:52,620] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:24:52,623] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46421 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:24:52,624] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:24:52,627] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:24:52,627] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:24:52,632] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:24:52,632] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:24:52,686] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:24:52,687] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:24:52,689] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:24:52,757] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO CodeGenerator: Code generated in 24.699266 ms
[2024-09-03 18:24:52,762] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455587, partition values: [empty row]
[2024-09-03 18:24:52,785] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO CodeGenerator: Code generated in 20.08094 ms
[2024-09-03 18:24:52,813] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO CodeGenerator: Code generated in 5.970335 ms
[2024-09-03 18:24:52,995] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO FileOutputCommitter: Saved output of task 'attempt_202409031824523545321152326511371_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409031824523545321152326511371_0001_m_000000
[2024-09-03 18:24:52,996] {spark_submit.py:495} INFO - 24/09/03 18:24:52 INFO SparkHadoopMapRedUtil: attempt_202409031824523545321152326511371_0001_m_000000_1: Committed
[2024-09-03 18:24:53,002] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:24:53,012] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 383 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:24:53,014] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,461 s
[2024-09-03 18:24:53,014] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:24:53,016] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:24:53,017] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:24:53,019] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,469623 s
[2024-09-03 18:24:53,083] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileFormatWriter: Write Job 884f78ac-a267-4757-b4a9-ac627e77f285 committed.
[2024-09-03 18:24:53,090] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileFormatWriter: Finished processing stats for write job 884f78ac-a267-4757-b4a9-ac627e77f285.
[2024-09-03 18:24:53,138] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:24:53,139] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:24:53,139] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:24:53,175] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:24:53,175] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:24:53,177] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:24:53,210] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO CodeGenerator: Code generated in 8.413405 ms
[2024-09-03 18:24:53,218] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:24:53,230] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:24:53,232] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46421 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:24:53,233] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:24:53,234] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649891 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:24:53,257] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:24:53,259] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:24:53,260] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:24:53,260] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:24:53,260] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:24:53,262] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:24:53,289] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:24:53,292] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:24:53,294] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46421 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:24:53,297] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:24:53,298] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:24:53,298] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:24:53,298] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:24:53,298] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:24:53,326] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:24:53,327] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:24:53,328] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:24:53,373] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO CodeGenerator: Code generated in 12.370596 ms
[2024-09-03 18:24:53,378] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455587, partition values: [empty row]
[2024-09-03 18:24:53,407] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO CodeGenerator: Code generated in 21.601932 ms
[2024-09-03 18:24:53,444] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileOutputCommitter: Saved output of task 'attempt_202409031824537352798093469667209_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409031824537352798093469667209_0002_m_000000
[2024-09-03 18:24:53,444] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkHadoopMapRedUtil: attempt_202409031824537352798093469667209_0002_m_000000_2: Committed
[2024-09-03 18:24:53,446] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:24:53,449] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 153 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:24:53,452] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,188 s
[2024-09-03 18:24:53,454] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:24:53,455] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:24:53,456] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:24:53,457] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,198348 s
[2024-09-03 18:24:53,495] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileFormatWriter: Write Job be5a8c83-7ef8-4381-bfdd-5ea869bc86f2 committed.
[2024-09-03 18:24:53,496] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO FileFormatWriter: Finished processing stats for write job be5a8c83-7ef8-4381-bfdd-5ea869bc86f2.
[2024-09-03 18:24:53,550] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:24:53,565] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:24:53,583] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:24:53,596] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:24:53,597] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO BlockManager: BlockManager stopped
[2024-09-03 18:24:53,616] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:24:53,622] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:24:53,634] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:24:53,634] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:24:53,635] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-de100050-df47-4007-b7ae-00d89d279a1c/pyspark-75933e54-45a0-4b47-ae66-d1df2e73076a
[2024-09-03 18:24:53,641] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-de100050-df47-4007-b7ae-00d89d279a1c
[2024-09-03 18:24:53,644] {spark_submit.py:495} INFO - 24/09/03 18:24:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-befe3822-320d-4d02-96a3-dc7cbd61f9ed
[2024-09-03 18:24:53,751] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240903T212442, end_date=20240903T212453
[2024-09-03 18:24:53,796] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:24:53,813] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:54:55,967] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 22:54:55,976] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 22:54:55,976] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:54:55,976] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:54:55,976] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:54:55,986] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 22:54:55,989] {standard_task_runner.py:52} INFO - Started process 454269 to run task
[2024-09-03 22:54:55,993] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '151', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpa2uk2wj2', '--error-file', '/tmp/tmpd4n9xynm']
[2024-09-03 22:54:55,993] {standard_task_runner.py:80} INFO - Job 151: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:54:56,027] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:54:56,073] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 22:54:56,078] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:54:56,079] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 22:54:57,112] {spark_submit.py:495} INFO - 24/09/03 22:54:57 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:54:57,113] {spark_submit.py:495} INFO - 24/09/03 22:54:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:54:57,503] {spark_submit.py:495} INFO - 24/09/03 22:54:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:54:58,035] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:54:58,044] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:54:58,092] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO ResourceUtils: ==============================================================
[2024-09-03 22:54:58,092] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:54:58,092] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO ResourceUtils: ==============================================================
[2024-09-03 22:54:58,093] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:54:58,116] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:54:58,128] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:54:58,128] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:54:58,184] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:54:58,185] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:54:58,185] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:54:58,185] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:54:58,185] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:54:58,356] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO Utils: Successfully started service 'sparkDriver' on port 40585.
[2024-09-03 22:54:58,381] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:54:58,409] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:54:58,426] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:54:58,426] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:54:58,429] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:54:58,440] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c35b3b19-edc7-42f0-82df-215f0fb0705d
[2024-09-03 22:54:58,459] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:54:58,476] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:54:58,679] {spark_submit.py:495} INFO - 24/09/03 22:54:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:54:58,683] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:54:58,741] {spark_submit.py:495} INFO - 24/09/03 22:54:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:54:59,029] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:54:59,060] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38197.
[2024-09-03 22:54:59,060] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO NettyBlockTransferService: Server created on 192.168.2.128:38197
[2024-09-03 22:54:59,063] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:54:59,070] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38197, None)
[2024-09-03 22:54:59,074] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38197 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38197, None)
[2024-09-03 22:54:59,077] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38197, None)
[2024-09-03 22:54:59,079] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38197, None)
[2024-09-03 22:54:59,562] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:54:59,563] {spark_submit.py:495} INFO - 24/09/03 22:54:59 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:55:00,417] {spark_submit.py:495} INFO - 24/09/03 22:55:00 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.
[2024-09-03 22:55:00,478] {spark_submit.py:495} INFO - 24/09/03 22:55:00 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:55:02,382] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:55:02,383] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:55:02,386] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:55:02,670] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:55:02,722] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:55:02,724] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38197 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:55:02,729] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:02,740] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649395 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:55:02,896] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:02,914] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:55:02,915] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:55:02,915] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:55:02,916] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:55:02,924] {spark_submit.py:495} INFO - 24/09/03 22:55:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:55:03,003] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:55:03,005] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:55:03,006] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38197 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:55:03,006] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:55:03,017] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:55:03,019] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:55:03,066] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:55:03,082] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:55:03,371] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455091, partition values: [empty row]
[2024-09-03 22:55:03,616] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO CodeGenerator: Code generated in 144.47355 ms
[2024-09-03 22:55:03,718] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:55:03,833] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 671 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:55:03,837] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:55:03,847] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,906 s
[2024-09-03 22:55:03,850] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:55:03,850] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:55:03,855] {spark_submit.py:495} INFO - 24/09/03 22:55:03 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,957976 s
[2024-09-03 22:55:04,281] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:55:04,282] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:55:04,282] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:55:04,365] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:04,365] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:04,366] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:04,459] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO CodeGenerator: Code generated in 33.690231 ms
[2024-09-03 22:55:04,504] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO CodeGenerator: Code generated in 29.751911 ms
[2024-09-03 22:55:04,509] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:55:04,516] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:55:04,517] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38197 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:55:04,519] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:04,537] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649395 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:55:04,616] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:04,620] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:55:04,621] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:55:04,622] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:55:04,623] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:55:04,625] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:55:04,686] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:55:04,689] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:55:04,690] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38197 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:55:04,690] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:55:04,691] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:55:04,691] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:55:04,695] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:55:04,696] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:55:04,757] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:04,758] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:04,758] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:04,815] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO CodeGenerator: Code generated in 21.901731 ms
[2024-09-03 22:55:04,826] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455091, partition values: [empty row]
[2024-09-03 22:55:04,858] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO CodeGenerator: Code generated in 28.945543 ms
[2024-09-03 22:55:04,883] {spark_submit.py:495} INFO - 24/09/03 22:55:04 INFO CodeGenerator: Code generated in 4.993726 ms
[2024-09-03 22:55:05,082] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409032255041418901509593339461_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409032255041418901509593339461_0001_m_000000
[2024-09-03 22:55:05,083] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkHadoopMapRedUtil: attempt_202409032255041418901509593339461_0001_m_000000_1: Committed
[2024-09-03 22:55:05,084] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:55:05,085] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 391 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:55:05,090] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:55:05,091] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,463 s
[2024-09-03 22:55:05,092] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:55:05,092] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:55:05,092] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,474512 s
[2024-09-03 22:55:05,113] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileFormatWriter: Write Job 03de94ac-e235-406f-ad5a-8e7ffe247f96 committed.
[2024-09-03 22:55:05,116] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileFormatWriter: Finished processing stats for write job 03de94ac-e235-406f-ad5a-8e7ffe247f96.
[2024-09-03 22:55:05,162] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:55:05,163] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:55:05,164] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:55:05,182] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:05,182] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:05,183] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:05,210] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO CodeGenerator: Code generated in 12.714196 ms
[2024-09-03 22:55:05,216] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:55:05,224] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:55:05,227] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38197 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:55:05,228] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:05,229] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649395 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:55:05,246] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:55:05,248] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:55:05,248] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:55:05,248] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:55:05,248] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:55:05,250] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:55:05,268] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:55:05,272] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:55:05,273] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38197 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:55:05,276] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:55:05,277] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:55:05,277] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:55:05,278] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:55:05,282] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:55:05,296] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:55:05,296] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:55:05,297] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:55:05,337] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO CodeGenerator: Code generated in 13.726599 ms
[2024-09-03 22:55:05,340] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-455091, partition values: [empty row]
[2024-09-03 22:55:05,359] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO CodeGenerator: Code generated in 15.665102 ms
[2024-09-03 22:55:05,400] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409032255055575196001114303687_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409032255055575196001114303687_0002_m_000000
[2024-09-03 22:55:05,400] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkHadoopMapRedUtil: attempt_202409032255055575196001114303687_0002_m_000000_2: Committed
[2024-09-03 22:55:05,403] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:55:05,405] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 127 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:55:05,406] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:55:05,409] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,157 s
[2024-09-03 22:55:05,409] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:55:05,410] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:55:05,410] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,160803 s
[2024-09-03 22:55:05,426] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileFormatWriter: Write Job f6bd5a4a-3111-4a52-82b1-5bbd1becd924 committed.
[2024-09-03 22:55:05,427] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO FileFormatWriter: Finished processing stats for write job f6bd5a4a-3111-4a52-82b1-5bbd1becd924.
[2024-09-03 22:55:05,460] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:55:05,468] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:55:05,481] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:55:05,492] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:55:05,493] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO BlockManager: BlockManager stopped
[2024-09-03 22:55:05,501] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:55:05,505] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:55:05,516] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:55:05,517] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:55:05,517] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2a2748a-b0b9-4334-82a4-e619f62df32d
[2024-09-03 22:55:05,520] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-04e79ea7-e6c0-41eb-a6eb-c10e18f9671d
[2024-09-03 22:55:05,523] {spark_submit.py:495} INFO - 24/09/03 22:55:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-04e79ea7-e6c0-41eb-a6eb-c10e18f9671d/pyspark-c7459d49-7f4a-4120-8e77-47e211a5c009
[2024-09-03 22:55:05,586] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240904T015455, end_date=20240904T015505
[2024-09-03 22:55:05,631] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:55:05,670] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 23:34:08,886] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 23:34:08,894] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-09-03 23:34:08,894] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:34:08,894] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 23:34:08,895] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:34:08,908] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-30 00:00:00+00:00
[2024-09-03 23:34:08,912] {standard_task_runner.py:52} INFO - Started process 473757 to run task
[2024-09-03 23:34:08,915] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp97aaf05x', '--error-file', '/tmp/tmpxqaxcqbm']
[2024-09-03 23:34:08,916] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 23:34:08,952] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-30T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 23:34:08,996] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-30T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-30T00:00:00+00:00
[2024-09-03 23:34:09,001] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 23:34:09,002] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240830
[2024-09-03 23:34:10,126] {spark_submit.py:495} INFO - 24/09/03 23:34:10 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 23:34:10,126] {spark_submit.py:495} INFO - 24/09/03 23:34:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 23:34:10,446] {spark_submit.py:495} INFO - 24/09/03 23:34:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 23:34:10,981] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 23:34:10,990] {spark_submit.py:495} INFO - 24/09/03 23:34:10 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 23:34:11,029] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO ResourceUtils: ==============================================================
[2024-09-03 23:34:11,029] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 23:34:11,030] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO ResourceUtils: ==============================================================
[2024-09-03 23:34:11,031] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 23:34:11,052] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 23:34:11,064] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 23:34:11,065] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 23:34:11,106] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 23:34:11,106] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 23:34:11,106] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 23:34:11,106] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 23:34:11,107] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 23:34:11,266] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO Utils: Successfully started service 'sparkDriver' on port 45659.
[2024-09-03 23:34:11,292] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 23:34:11,321] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 23:34:11,337] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 23:34:11,338] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 23:34:11,341] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 23:34:11,353] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3baecc64-26ea-41bf-808f-a3682fcce783
[2024-09-03 23:34:11,375] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 23:34:11,397] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 23:34:11,689] {spark_submit.py:495} INFO - 24/09/03 23:34:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 23:34:11,702] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 23:34:11,807] {spark_submit.py:495} INFO - 24/09/03 23:34:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 23:34:12,020] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 23:34:12,045] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33641.
[2024-09-03 23:34:12,045] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO NettyBlockTransferService: Server created on 192.168.2.128:33641
[2024-09-03 23:34:12,046] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 23:34:12,054] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 33641, None)
[2024-09-03 23:34:12,058] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:33641 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 33641, None)
[2024-09-03 23:34:12,060] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 33641, None)
[2024-09-03 23:34:12,062] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 33641, None)
[2024-09-03 23:34:12,914] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 23:34:12,918] {spark_submit.py:495} INFO - 24/09/03 23:34:12 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 23:34:13,924] {spark_submit.py:495} INFO - 24/09/03 23:34:13 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2024-09-03 23:34:14,011] {spark_submit.py:495} INFO - 24/09/03 23:34:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 23:34:15,910] {spark_submit.py:495} INFO - 24/09/03 23:34:15 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:34:15,910] {spark_submit.py:495} INFO - 24/09/03 23:34:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:34:15,913] {spark_submit.py:495} INFO - 24/09/03 23:34:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 23:34:16,244] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 23:34:16,303] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 23:34:16,305] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:33641 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 23:34:16,313] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:16,323] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198832 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:34:16,485] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:16,501] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:34:16,502] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:34:16,502] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:34:16,504] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:34:16,510] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:34:16,648] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 23:34:16,651] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 23:34:16,652] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:33641 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 23:34:16,652] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:34:16,674] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:34:16,675] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 23:34:16,730] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 23:34:16,754] {spark_submit.py:495} INFO - 24/09/03 23:34:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 23:34:17,039] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-4528, partition values: [empty row]
[2024-09-03 23:34:17,281] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO CodeGenerator: Code generated in 141.548187 ms
[2024-09-03 23:34:17,336] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 23:34:17,353] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 642 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:34:17,456] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 23:34:17,465] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,927 s
[2024-09-03 23:34:17,470] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:34:17,470] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 23:34:17,475] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,986774 s
[2024-09-03 23:34:17,928] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 23:34:17,930] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 23:34:17,931] {spark_submit.py:495} INFO - 24/09/03 23:34:17 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 23:34:18,005] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:18,005] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:18,006] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:18,092] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 25.036965 ms
[2024-09-03 23:34:18,155] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 47.772129 ms
[2024-09-03 23:34:18,162] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 23:34:18,196] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 23:34:18,198] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:33641 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 23:34:18,200] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:18,203] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198832 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:34:18,269] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:18,271] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:34:18,272] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:34:18,273] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:34:18,274] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:34:18,274] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:34:18,313] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 23:34:18,315] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 23:34:18,316] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:33641 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 23:34:18,316] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:34:18,317] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:34:18,317] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 23:34:18,321] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:34:18,321] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 23:34:18,403] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:18,404] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:18,405] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:18,485] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 37.497584 ms
[2024-09-03 23:34:18,488] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-4528, partition values: [empty row]
[2024-09-03 23:34:18,513] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 20.724874 ms
[2024-09-03 23:34:18,541] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 7.749926 ms
[2024-09-03 23:34:18,595] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409032334188902754294555812205_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240830/_temporary/0/task_202409032334188902754294555812205_0001_m_000000
[2024-09-03 23:34:18,597] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkHadoopMapRedUtil: attempt_202409032334188902754294555812205_0001_m_000000_1: Committed
[2024-09-03 23:34:18,606] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 23:34:18,610] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 292 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:34:18,611] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 23:34:18,612] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,336 s
[2024-09-03 23:34:18,612] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:34:18,613] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 23:34:18,614] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,343334 s
[2024-09-03 23:34:18,643] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileFormatWriter: Write Job e3dae377-0cf8-423c-8008-b0f647792ffb committed.
[2024-09-03 23:34:18,651] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileFormatWriter: Finished processing stats for write job e3dae377-0cf8-423c-8008-b0f647792ffb.
[2024-09-03 23:34:18,699] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:34:18,700] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:34:18,701] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 23:34:18,719] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:18,719] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:18,720] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:18,764] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 10.809515 ms
[2024-09-03 23:34:18,771] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 23:34:18,784] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 23:34:18,786] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:33641 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 23:34:18,787] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:18,788] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198832 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:34:18,806] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:34:18,807] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:34:18,807] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:34:18,807] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:34:18,813] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:34:18,814] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:34:18,835] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 23:34:18,839] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 23:34:18,840] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:33641 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 23:34:18,841] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:34:18,842] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:34:18,842] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 23:34:18,844] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:34:18,844] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 23:34:18,858] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:34:18,859] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:34:18,860] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:34:18,905] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 11.063533 ms
[2024-09-03 23:34:18,908] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-30T00:00:00.00Z/data_engineer_20240830.json, range: 0-4528, partition values: [empty row]
[2024-09-03 23:34:18,931] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO CodeGenerator: Code generated in 19.826714 ms
[2024-09-03 23:34:18,943] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409032334188541785398180947051_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240830/_temporary/0/task_202409032334188541785398180947051_0002_m_000000
[2024-09-03 23:34:18,943] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO SparkHadoopMapRedUtil: attempt_202409032334188541785398180947051_0002_m_000000_2: Committed
[2024-09-03 23:34:18,944] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 23:34:18,946] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 103 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:34:18,946] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 23:34:18,950] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,131 s
[2024-09-03 23:34:18,950] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:34:18,950] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 23:34:18,950] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,142668 s
[2024-09-03 23:34:18,988] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileFormatWriter: Write Job 05a74b67-aecc-44cc-b137-11242d6c2092 committed.
[2024-09-03 23:34:18,988] {spark_submit.py:495} INFO - 24/09/03 23:34:18 INFO FileFormatWriter: Finished processing stats for write job 05a74b67-aecc-44cc-b137-11242d6c2092.
[2024-09-03 23:34:19,029] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 23:34:19,038] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 23:34:19,054] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 23:34:19,070] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO MemoryStore: MemoryStore cleared
[2024-09-03 23:34:19,071] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO BlockManager: BlockManager stopped
[2024-09-03 23:34:19,080] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 23:34:19,082] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 23:34:19,097] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 23:34:19,098] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 23:34:19,098] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-df2adfcd-5275-4317-b904-4953a05788e2
[2024-09-03 23:34:19,101] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b4a8a79-6390-4cf3-8240-2ba93bdc0a33/pyspark-bc105549-fc76-4bc4-a8e3-820bb3a7707f
[2024-09-03 23:34:19,103] {spark_submit.py:495} INFO - 24/09/03 23:34:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b4a8a79-6390-4cf3-8240-2ba93bdc0a33
[2024-09-03 23:34:19,176] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240830T000000, start_date=20240904T023408, end_date=20240904T023419
[2024-09-03 23:34:19,219] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 23:34:19,270] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
