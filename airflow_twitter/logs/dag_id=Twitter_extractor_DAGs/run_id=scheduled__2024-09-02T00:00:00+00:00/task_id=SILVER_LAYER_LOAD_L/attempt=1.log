[2024-09-03 16:25:59,322] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 16:25:59,326] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 16:25:59,326] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:25:59,327] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:25:59,327] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:25:59,334] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 16:25:59,336] {standard_task_runner.py:52} INFO - Started process 287569 to run task
[2024-09-03 16:25:59,338] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '103', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp380lx10v', '--error-file', '/tmp/tmp_h53m_6i']
[2024-09-03 16:25:59,339] {standard_task_runner.py:80} INFO - Job 103: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:25:59,367] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:25:59,404] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 16:25:59,407] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:25:59,407] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 16:26:00,438] {spark_submit.py:495} INFO - 24/09/03 16:26:00 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:26:00,438] {spark_submit.py:495} INFO - 24/09/03 16:26:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:26:00,739] {spark_submit.py:495} INFO - 24/09/03 16:26:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:26:01,214] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:26:01,221] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:26:01,253] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO ResourceUtils: ==============================================================
[2024-09-03 16:26:01,253] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:26:01,253] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO ResourceUtils: ==============================================================
[2024-09-03 16:26:01,254] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:26:01,270] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:26:01,281] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:26:01,282] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:26:01,321] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:26:01,321] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:26:01,321] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:26:01,321] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:26:01,321] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:26:01,453] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO Utils: Successfully started service 'sparkDriver' on port 38773.
[2024-09-03 16:26:01,473] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:26:01,496] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:26:01,510] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:26:01,510] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:26:01,514] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:26:01,523] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5b5252ab-55a5-452c-8875-2986ad07dac8
[2024-09-03 16:26:01,540] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:26:01,553] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:26:01,729] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:26:01,780] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:26:01,963] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:26:01,985] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36843.
[2024-09-03 16:26:01,985] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO NettyBlockTransferService: Server created on 192.168.2.128:36843
[2024-09-03 16:26:01,986] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:26:01,990] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36843, None)
[2024-09-03 16:26:01,995] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36843 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36843, None)
[2024-09-03 16:26:01,996] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36843, None)
[2024-09-03 16:26:01,997] {spark_submit.py:495} INFO - 24/09/03 16:26:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36843, None)
[2024-09-03 16:26:02,402] {spark_submit.py:495} INFO - 24/09/03 16:26:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:26:02,402] {spark_submit.py:495} INFO - 24/09/03 16:26:02 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:26:03,081] {spark_submit.py:495} INFO - 24/09/03 16:26:03 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.
[2024-09-03 16:26:03,127] {spark_submit.py:495} INFO - 24/09/03 16:26:03 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:26:04,652] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:26:04,652] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:26:04,655] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:26:04,899] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:26:04,951] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:26:04,951] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36843 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:26:04,973] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:26:04,991] {spark_submit.py:495} INFO - 24/09/03 16:26:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198827 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:26:05,160] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:26:05,178] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:26:05,178] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:26:05,178] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:26:05,179] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:26:05,190] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:26:05,279] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:26:05,282] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:26:05,282] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36843 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:26:05,283] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:26:05,296] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:26:05,297] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:26:05,340] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:26:05,356] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:26:05,617] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4523, partition values: [empty row]
[2024-09-03 16:26:05,822] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO CodeGenerator: Code generated in 127.144114 ms
[2024-09-03 16:26:05,862] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:26:05,869] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 536 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:26:05,873] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:26:05,875] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,666 s
[2024-09-03 16:26:05,968] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:26:05,968] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:26:05,973] {spark_submit.py:495} INFO - 24/09/03 16:26:05 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,808079 s
[2024-09-03 16:26:06,357] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:26:06,358] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:26:06,359] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:26:06,415] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:26:06,416] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:26:06,416] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:26:06,480] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO CodeGenerator: Code generated in 25.644549 ms
[2024-09-03 16:26:06,523] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO CodeGenerator: Code generated in 23.79347 ms
[2024-09-03 16:26:06,529] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:26:06,536] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:26:06,537] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36843 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:26:06,538] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:26:06,542] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198827 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:26:06,600] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:26:06,602] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:26:06,602] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:26:06,602] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:26:06,602] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:26:06,603] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:26:06,638] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:26:06,640] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:26:06,640] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36843 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:26:06,641] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:26:06,641] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:26:06,642] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:26:06,645] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:26:06,645] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:26:06,694] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:26:06,694] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:26:06,695] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:26:06,750] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO CodeGenerator: Code generated in 17.86113 ms
[2024-09-03 16:26:06,753] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4523, partition values: [empty row]
[2024-09-03 16:26:06,771] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO CodeGenerator: Code generated in 15.363305 ms
[2024-09-03 16:26:06,791] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO CodeGenerator: Code generated in 4.416601 ms
[2024-09-03 16:26:06,819] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: Saved output of task 'attempt_202409031626067067930837358538890_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409031626067067930837358538890_0001_m_000000
[2024-09-03 16:26:06,819] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkHadoopMapRedUtil: attempt_202409031626067067930837358538890_0001_m_000000_1: Committed
[2024-09-03 16:26:06,822] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:26:06,824] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 182 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:26:06,824] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:26:06,831] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,220 s
[2024-09-03 16:26:06,831] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:26:06,831] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:26:06,831] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,227922 s
[2024-09-03 16:26:06,855] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileFormatWriter: Write Job 2b173164-90e5-4087-b197-d30d8722cd00 committed.
[2024-09-03 16:26:06,858] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileFormatWriter: Finished processing stats for write job 2b173164-90e5-4087-b197-d30d8722cd00.
[2024-09-03 16:26:06,890] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:26:06,890] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:26:06,890] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:26:06,899] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:26:06,899] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:26:06,899] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:26:06,927] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO CodeGenerator: Code generated in 9.301289 ms
[2024-09-03 16:26:06,930] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:26:06,937] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:26:06,937] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36843 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:26:06,939] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:26:06,940] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198827 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:26:06,956] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:26:06,957] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:26:06,957] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:26:06,957] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:26:06,957] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:26:06,958] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:26:06,976] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:26:06,977] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:26:06,978] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36843 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:26:06,979] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:26:06,979] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:26:06,979] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:26:06,980] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:26:06,980] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:26:06,997] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:26:06,997] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:26:06,997] {spark_submit.py:495} INFO - 24/09/03 16:26:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:26:07,027] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO CodeGenerator: Code generated in 9.801236 ms
[2024-09-03 16:26:07,030] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4523, partition values: [empty row]
[2024-09-03 16:26:07,043] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO CodeGenerator: Code generated in 10.548577 ms
[2024-09-03 16:26:07,051] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO FileOutputCommitter: Saved output of task 'attempt_202409031626068380741178590352788_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409031626068380741178590352788_0002_m_000000
[2024-09-03 16:26:07,051] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO SparkHadoopMapRedUtil: attempt_202409031626068380741178590352788_0002_m_000000_2: Committed
[2024-09-03 16:26:07,052] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:26:07,054] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:26:07,055] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,095 s
[2024-09-03 16:26:07,055] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:26:07,055] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:26:07,055] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:26:07,055] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,098558 s
[2024-09-03 16:26:07,073] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO FileFormatWriter: Write Job 7af5e53a-63e8-4b93-95e9-d9fa2e37eaa9 committed.
[2024-09-03 16:26:07,073] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO FileFormatWriter: Finished processing stats for write job 7af5e53a-63e8-4b93-95e9-d9fa2e37eaa9.
[2024-09-03 16:26:07,109] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:26:07,116] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:26:07,126] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:26:07,133] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:26:07,134] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO BlockManager: BlockManager stopped
[2024-09-03 16:26:07,141] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:26:07,142] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:26:07,146] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:26:07,146] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:26:07,147] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3157a30-c44d-4052-9393-e2ee8a71af00
[2024-09-03 16:26:07,149] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-beebf041-e8d9-47ad-ae25-afdec4942553
[2024-09-03 16:26:07,151] {spark_submit.py:495} INFO - 24/09/03 16:26:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-beebf041-e8d9-47ad-ae25-afdec4942553/pyspark-7f3eade8-6db8-4fd6-8f27-c3ecf0c22449
[2024-09-03 16:26:07,269] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240903T192559, end_date=20240903T192607
[2024-09-03 16:26:07,320] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:26:07,328] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:43:48,322] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 16:43:48,326] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 16:43:48,326] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:43:48,326] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:43:48,326] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:43:48,334] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 16:43:48,337] {standard_task_runner.py:52} INFO - Started process 297965 to run task
[2024-09-03 16:43:48,339] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '103', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_79uu8sn', '--error-file', '/tmp/tmptlsb061v']
[2024-09-03 16:43:48,340] {standard_task_runner.py:80} INFO - Job 103: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:43:48,373] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:43:48,415] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 16:43:48,418] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:43:48,418] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 16:43:49,400] {spark_submit.py:495} INFO - 24/09/03 16:43:49 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:43:49,401] {spark_submit.py:495} INFO - 24/09/03 16:43:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:43:49,720] {spark_submit.py:495} INFO - 24/09/03 16:43:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:43:50,238] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:43:50,250] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:43:50,289] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO ResourceUtils: ==============================================================
[2024-09-03 16:43:50,290] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:43:50,291] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO ResourceUtils: ==============================================================
[2024-09-03 16:43:50,291] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:43:50,308] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:43:50,321] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:43:50,322] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:43:50,359] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:43:50,360] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:43:50,360] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:43:50,360] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:43:50,360] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:43:50,505] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO Utils: Successfully started service 'sparkDriver' on port 38095.
[2024-09-03 16:43:50,530] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:43:50,556] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:43:50,571] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:43:50,572] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:43:50,575] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:43:50,586] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bbd249f9-2e8e-473b-85c9-6c9e89496296
[2024-09-03 16:43:50,604] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:43:50,618] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:43:50,826] {spark_submit.py:495} INFO - 24/09/03 16:43:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:43:50,839] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:43:50,879] {spark_submit.py:495} INFO - 24/09/03 16:43:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:43:51,063] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:43:51,103] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33853.
[2024-09-03 16:43:51,103] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO NettyBlockTransferService: Server created on 192.168.2.128:33853
[2024-09-03 16:43:51,105] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:43:51,111] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 33853, None)
[2024-09-03 16:43:51,118] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:33853 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 33853, None)
[2024-09-03 16:43:51,118] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 33853, None)
[2024-09-03 16:43:51,120] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 33853, None)
[2024-09-03 16:43:51,593] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:43:51,594] {spark_submit.py:495} INFO - 24/09/03 16:43:51 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:43:52,382] {spark_submit.py:495} INFO - 24/09/03 16:43:52 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 16:43:52,442] {spark_submit.py:495} INFO - 24/09/03 16:43:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 16:43:54,084] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:43:54,085] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:43:54,088] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:43:54,345] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:43:54,391] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:43:54,396] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:33853 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:43:54,400] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:54,408] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648761 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:43:54,542] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:54,582] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:43:54,582] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:43:54,582] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:43:54,583] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:43:54,591] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:43:54,670] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:43:54,673] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:43:54,674] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:33853 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:43:54,675] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:43:54,685] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:43:54,686] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:43:54,732] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:43:54,755] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:43:54,964] {spark_submit.py:495} INFO - 24/09/03 16:43:54 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-454457, partition values: [empty row]
[2024-09-03 16:43:55,189] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO CodeGenerator: Code generated in 135.319434 ms
[2024-09-03 16:43:55,269] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 16:43:55,387] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 555 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:43:55,389] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:43:55,401] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,795 s
[2024-09-03 16:43:55,405] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:43:55,406] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:43:55,412] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,866883 s
[2024-09-03 16:43:55,774] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:43:55,775] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:43:55,776] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:43:55,836] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:55,836] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:55,838] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:55,919] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO CodeGenerator: Code generated in 27.975241 ms
[2024-09-03 16:43:55,964] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO CodeGenerator: Code generated in 30.63235 ms
[2024-09-03 16:43:55,970] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:43:55,979] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:43:55,980] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:33853 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:43:55,981] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:55,984] {spark_submit.py:495} INFO - 24/09/03 16:43:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648761 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:43:56,039] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:56,040] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:43:56,041] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:43:56,041] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:43:56,041] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:43:56,045] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:43:56,108] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:43:56,111] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:43:56,113] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:33853 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:43:56,114] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:43:56,116] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:43:56,116] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:43:56,119] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:43:56,119] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:43:56,168] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:56,169] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:56,170] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:56,228] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO CodeGenerator: Code generated in 19.865948 ms
[2024-09-03 16:43:56,231] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-454457, partition values: [empty row]
[2024-09-03 16:43:56,254] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO CodeGenerator: Code generated in 20.530567 ms
[2024-09-03 16:43:56,276] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO CodeGenerator: Code generated in 5.124243 ms
[2024-09-03 16:43:56,390] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: Saved output of task 'attempt_202409031643567161957990415723514_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409031643567161957990415723514_0001_m_000000
[2024-09-03 16:43:56,391] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkHadoopMapRedUtil: attempt_202409031643567161957990415723514_0001_m_000000_1: Committed
[2024-09-03 16:43:56,396] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:43:56,399] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 282 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:43:56,400] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,354 s
[2024-09-03 16:43:56,400] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:43:56,407] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:43:56,407] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:43:56,409] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,369659 s
[2024-09-03 16:43:56,429] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileFormatWriter: Write Job e6b4c903-b1c2-41a6-864d-6e6cc6ca3091 committed.
[2024-09-03 16:43:56,432] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileFormatWriter: Finished processing stats for write job e6b4c903-b1c2-41a6-864d-6e6cc6ca3091.
[2024-09-03 16:43:56,463] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:43:56,464] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:43:56,464] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:43:56,475] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:56,476] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:56,476] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:56,499] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO CodeGenerator: Code generated in 8.409828 ms
[2024-09-03 16:43:56,502] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:43:56,509] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:43:56,510] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:33853 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:43:56,512] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:56,513] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648761 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:43:56,527] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:43:56,529] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:43:56,530] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:43:56,530] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:43:56,531] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:43:56,532] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:43:56,553] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:43:56,554] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:43:56,555] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:33853 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:43:56,555] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:43:56,557] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:43:56,557] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:43:56,560] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:43:56,560] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:43:56,577] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:43:56,577] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:43:56,580] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:43:56,613] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO CodeGenerator: Code generated in 10.259956 ms
[2024-09-03 16:43:56,622] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-454457, partition values: [empty row]
[2024-09-03 16:43:56,637] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO CodeGenerator: Code generated in 11.246066 ms
[2024-09-03 16:43:56,669] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileOutputCommitter: Saved output of task 'attempt_202409031643564832383513081824127_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409031643564832383513081824127_0002_m_000000
[2024-09-03 16:43:56,669] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkHadoopMapRedUtil: attempt_202409031643564832383513081824127_0002_m_000000_2: Committed
[2024-09-03 16:43:56,670] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:43:56,673] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 114 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:43:56,673] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,140 s
[2024-09-03 16:43:56,673] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:43:56,674] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:43:56,675] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:43:56,675] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,147258 s
[2024-09-03 16:43:56,696] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileFormatWriter: Write Job dd9b5a1f-dfc9-417f-aa91-d753d452738f committed.
[2024-09-03 16:43:56,697] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO FileFormatWriter: Finished processing stats for write job dd9b5a1f-dfc9-417f-aa91-d753d452738f.
[2024-09-03 16:43:56,734] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:43:56,742] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:43:56,754] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:43:56,764] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:43:56,764] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO BlockManager: BlockManager stopped
[2024-09-03 16:43:56,769] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:43:56,771] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:43:56,789] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:43:56,791] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:43:56,791] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-840f0a9a-64a0-4997-8f12-03ea9a4784c6/pyspark-ae4c56a0-f362-4cff-a266-1b5cdf6ac310
[2024-09-03 16:43:56,797] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-840f0a9a-64a0-4997-8f12-03ea9a4784c6
[2024-09-03 16:43:56,799] {spark_submit.py:495} INFO - 24/09/03 16:43:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-8cfb2f87-a508-4db6-8591-bbd9975f6d9e
[2024-09-03 16:43:56,859] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240903T194348, end_date=20240903T194356
[2024-09-03 16:43:56,875] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:43:56,883] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:58:15,354] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 17:58:15,360] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 17:58:15,360] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:58:15,360] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:58:15,360] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:58:15,368] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 17:58:15,370] {standard_task_runner.py:52} INFO - Started process 329030 to run task
[2024-09-03 17:58:15,373] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '117', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp9ghveb_p', '--error-file', '/tmp/tmpbnu_8t0h']
[2024-09-03 17:58:15,373] {standard_task_runner.py:80} INFO - Job 117: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:58:15,405] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:58:15,449] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 17:58:15,454] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:58:15,454] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 17:58:16,509] {spark_submit.py:495} INFO - 24/09/03 17:58:16 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:58:16,510] {spark_submit.py:495} INFO - 24/09/03 17:58:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:58:16,806] {spark_submit.py:495} INFO - 24/09/03 17:58:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:58:17,312] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:58:17,321] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:58:17,355] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO ResourceUtils: ==============================================================
[2024-09-03 17:58:17,355] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:58:17,356] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO ResourceUtils: ==============================================================
[2024-09-03 17:58:17,356] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:58:17,373] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:58:17,384] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:58:17,385] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:58:17,422] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:58:17,422] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:58:17,422] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:58:17,422] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:58:17,422] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:58:17,576] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO Utils: Successfully started service 'sparkDriver' on port 33227.
[2024-09-03 17:58:17,599] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:58:17,625] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:58:17,640] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:58:17,640] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:58:17,643] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:58:17,653] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a0fa2d9e-1ac1-4395-ad1d-fba745cfa42d
[2024-09-03 17:58:17,671] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:58:17,685] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:58:17,856] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:58:17,903] {spark_submit.py:495} INFO - 24/09/03 17:58:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:58:18,096] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:58:18,123] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40957.
[2024-09-03 17:58:18,124] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO NettyBlockTransferService: Server created on 192.168.2.128:40957
[2024-09-03 17:58:18,125] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:58:18,130] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40957, None)
[2024-09-03 17:58:18,134] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40957 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40957, None)
[2024-09-03 17:58:18,135] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40957, None)
[2024-09-03 17:58:18,136] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40957, None)
[2024-09-03 17:58:18,656] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:58:18,657] {spark_submit.py:495} INFO - 24/09/03 17:58:18 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:58:19,390] {spark_submit.py:495} INFO - 24/09/03 17:58:19 INFO InMemoryFileIndex: It took 29 ms to list leaf files for 1 paths.
[2024-09-03 17:58:19,440] {spark_submit.py:495} INFO - 24/09/03 17:58:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:58:21,051] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:58:21,052] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:58:21,055] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:58:21,311] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:58:21,356] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:58:21,358] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40957 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:58:21,364] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:58:21,372] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198824 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:58:21,506] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:58:21,526] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:58:21,526] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:58:21,526] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:58:21,527] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:58:21,536] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:58:21,619] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:58:21,623] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:58:21,624] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40957 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:58:21,625] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:58:21,637] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:58:21,639] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:58:21,699] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:58:21,715] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:58:21,965] {spark_submit.py:495} INFO - 24/09/03 17:58:21 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4520, partition values: [empty row]
[2024-09-03 17:58:22,187] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO CodeGenerator: Code generated in 134.306123 ms
[2024-09-03 17:58:22,228] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 17:58:22,239] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 550 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:58:22,244] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,696 s
[2024-09-03 17:58:22,244] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:58:22,341] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:58:22,341] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:58:22,343] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,835617 s
[2024-09-03 17:58:22,711] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:58:22,714] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:58:22,715] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:58:22,776] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:58:22,776] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:58:22,777] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:58:22,845] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO CodeGenerator: Code generated in 24.720232 ms
[2024-09-03 17:58:22,887] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO CodeGenerator: Code generated in 25.376582 ms
[2024-09-03 17:58:22,891] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:58:22,898] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:58:22,899] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40957 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:58:22,901] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:58:22,904] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198824 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:58:22,957] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:58:22,959] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:58:22,959] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:58:22,959] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:58:22,960] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:58:22,962] {spark_submit.py:495} INFO - 24/09/03 17:58:22 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:58:23,024] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 17:58:23,026] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:58:23,028] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40957 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:58:23,029] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:58:23,030] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:58:23,030] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:58:23,034] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:58:23,034] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:58:23,082] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:58:23,082] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:58:23,083] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:58:23,172] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO CodeGenerator: Code generated in 29.423946 ms
[2024-09-03 17:58:23,182] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4520, partition values: [empty row]
[2024-09-03 17:58:23,228] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO CodeGenerator: Code generated in 40.919008 ms
[2024-09-03 17:58:23,253] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO CodeGenerator: Code generated in 6.095406 ms
[2024-09-03 17:58:23,289] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: Saved output of task 'attempt_202409031758228341930197431756111_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409031758228341930197431756111_0001_m_000000
[2024-09-03 17:58:23,290] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkHadoopMapRedUtil: attempt_202409031758228341930197431756111_0001_m_000000_1: Committed
[2024-09-03 17:58:23,293] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:58:23,299] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 267 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:58:23,300] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:58:23,305] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,339 s
[2024-09-03 17:58:23,305] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:58:23,305] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:58:23,306] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,348427 s
[2024-09-03 17:58:23,330] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileFormatWriter: Write Job 68fc2283-a426-4523-b350-e36926285655 committed.
[2024-09-03 17:58:23,335] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileFormatWriter: Finished processing stats for write job 68fc2283-a426-4523-b350-e36926285655.
[2024-09-03 17:58:23,370] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:58:23,370] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:58:23,371] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:58:23,383] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:58:23,383] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:58:23,385] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:58:23,423] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO CodeGenerator: Code generated in 12.46034 ms
[2024-09-03 17:58:23,427] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:58:23,435] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:58:23,436] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40957 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:58:23,437] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:58:23,438] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198824 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:58:23,458] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:58:23,460] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:58:23,460] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:58:23,460] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:58:23,461] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:58:23,464] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:58:23,482] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:58:23,484] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:58:23,485] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40957 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:58:23,487] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:58:23,488] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:58:23,489] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:58:23,490] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:58:23,496] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:58:23,503] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:58:23,507] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:58:23,508] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:58:23,545] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO CodeGenerator: Code generated in 9.97192 ms
[2024-09-03 17:58:23,550] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4520, partition values: [empty row]
[2024-09-03 17:58:23,564] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO CodeGenerator: Code generated in 10.271791 ms
[2024-09-03 17:58:23,574] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileOutputCommitter: Saved output of task 'attempt_202409031758235481732113992338777_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409031758235481732113992338777_0002_m_000000
[2024-09-03 17:58:23,574] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkHadoopMapRedUtil: attempt_202409031758235481732113992338777_0002_m_000000_2: Committed
[2024-09-03 17:58:23,576] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:58:23,579] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 88 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:58:23,579] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:58:23,583] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,115 s
[2024-09-03 17:58:23,583] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:58:23,583] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:58:23,584] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,124216 s
[2024-09-03 17:58:23,604] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileFormatWriter: Write Job da8b8b5c-f470-4a4d-8175-3b7f5d2b9f51 committed.
[2024-09-03 17:58:23,604] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO FileFormatWriter: Finished processing stats for write job da8b8b5c-f470-4a4d-8175-3b7f5d2b9f51.
[2024-09-03 17:58:23,648] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:58:23,661] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:58:23,685] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:58:23,694] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:58:23,696] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO BlockManager: BlockManager stopped
[2024-09-03 17:58:23,704] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:58:23,707] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:58:23,716] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:58:23,716] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:58:23,717] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b6445cc-2446-4bdf-b2b6-cec7da3b98ab/pyspark-2961741a-34e1-437d-9991-3b79b5948993
[2024-09-03 17:58:23,720] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b6445cc-2446-4bdf-b2b6-cec7da3b98ab
[2024-09-03 17:58:23,723] {spark_submit.py:495} INFO - 24/09/03 17:58:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-dd35f0a5-55be-4005-b87c-44ebaf9e992b
[2024-09-03 17:58:23,808] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240903T205815, end_date=20240903T205823
[2024-09-03 17:58:23,837] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:58:23,852] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:25:45,826] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 18:25:45,836] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 18:25:45,836] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:25:45,837] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:25:45,837] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:25:45,852] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 18:25:45,855] {standard_task_runner.py:52} INFO - Started process 347961 to run task
[2024-09-03 18:25:45,860] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '117', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpm0e2qasf', '--error-file', '/tmp/tmpycbvt8xl']
[2024-09-03 18:25:45,861] {standard_task_runner.py:80} INFO - Job 117: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:25:45,907] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:25:45,968] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 18:25:45,972] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:25:45,973] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 18:25:47,637] {spark_submit.py:495} INFO - 24/09/03 18:25:47 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:25:47,639] {spark_submit.py:495} INFO - 24/09/03 18:25:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:25:48,145] {spark_submit.py:495} INFO - 24/09/03 18:25:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:25:49,050] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:25:49,063] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:25:49,121] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO ResourceUtils: ==============================================================
[2024-09-03 18:25:49,122] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:25:49,122] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO ResourceUtils: ==============================================================
[2024-09-03 18:25:49,125] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:25:49,162] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:25:49,184] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:25:49,185] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:25:49,255] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:25:49,257] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:25:49,258] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:25:49,259] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:25:49,259] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:25:49,554] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO Utils: Successfully started service 'sparkDriver' on port 45975.
[2024-09-03 18:25:49,600] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:25:49,688] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:25:49,719] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:25:49,720] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:25:49,725] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:25:49,743] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-28fa2896-1486-4b5b-9b10-69805455a5fd
[2024-09-03 18:25:49,768] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:25:49,789] {spark_submit.py:495} INFO - 24/09/03 18:25:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:25:50,087] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:25:50,164] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:25:50,471] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:25:50,544] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39293.
[2024-09-03 18:25:50,545] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO NettyBlockTransferService: Server created on 192.168.2.128:39293
[2024-09-03 18:25:50,547] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:25:50,561] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 39293, None)
[2024-09-03 18:25:50,567] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:39293 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 39293, None)
[2024-09-03 18:25:50,583] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 39293, None)
[2024-09-03 18:25:50,587] {spark_submit.py:495} INFO - 24/09/03 18:25:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 39293, None)
[2024-09-03 18:25:51,929] {spark_submit.py:495} INFO - 24/09/03 18:25:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:25:51,929] {spark_submit.py:495} INFO - 24/09/03 18:25:51 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:25:53,578] {spark_submit.py:495} INFO - 24/09/03 18:25:53 INFO InMemoryFileIndex: It took 75 ms to list leaf files for 1 paths.
[2024-09-03 18:25:53,683] {spark_submit.py:495} INFO - 24/09/03 18:25:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:25:56,198] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:25:56,199] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:25:56,202] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:25:56,526] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:25:56,582] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:25:56,585] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:39293 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:25:56,597] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:56,607] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198828 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:25:56,782] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:56,802] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:25:56,802] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:25:56,802] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:25:56,804] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:25:56,809] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:25:56,905] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:25:56,908] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:25:56,908] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:39293 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:25:56,909] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:25:56,922] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:25:56,923] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:25:56,973] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:25:56,998] {spark_submit.py:495} INFO - 24/09/03 18:25:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:25:57,335] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4524, partition values: [empty row]
[2024-09-03 18:25:57,634] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO CodeGenerator: Code generated in 173.554127 ms
[2024-09-03 18:25:57,697] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:25:57,717] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 756 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:25:57,722] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:25:57,899] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,076 s
[2024-09-03 18:25:57,905] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:25:57,905] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:25:57,912] {spark_submit.py:495} INFO - 24/09/03 18:25:57 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,124222 s
[2024-09-03 18:25:58,412] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:25:58,417] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:25:58,418] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:25:58,508] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:58,509] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:58,510] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:58,608] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO CodeGenerator: Code generated in 29.82763 ms
[2024-09-03 18:25:58,682] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO CodeGenerator: Code generated in 51.110966 ms
[2024-09-03 18:25:58,688] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:25:58,696] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:25:58,697] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:39293 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:25:58,700] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:58,703] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198828 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:25:58,772] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:58,774] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:25:58,775] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:25:58,775] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:25:58,775] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:25:58,778] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:25:58,825] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:25:58,829] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:25:58,831] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:39293 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:25:58,832] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:25:58,832] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:25:58,833] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:25:58,837] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:25:58,840] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:25:58,900] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:58,900] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:58,901] {spark_submit.py:495} INFO - 24/09/03 18:25:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:59,003] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO CodeGenerator: Code generated in 32.674901 ms
[2024-09-03 18:25:59,008] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4524, partition values: [empty row]
[2024-09-03 18:25:59,056] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO CodeGenerator: Code generated in 40.206097 ms
[2024-09-03 18:25:59,102] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO CodeGenerator: Code generated in 9.653615 ms
[2024-09-03 18:25:59,173] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileOutputCommitter: Saved output of task 'attempt_202409031825588369436019771980435_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409031825588369436019771980435_0001_m_000000
[2024-09-03 18:25:59,175] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkHadoopMapRedUtil: attempt_202409031825588369436019771980435_0001_m_000000_1: Committed
[2024-09-03 18:25:59,184] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:25:59,188] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 354 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:25:59,189] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:25:59,190] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,410 s
[2024-09-03 18:25:59,191] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:25:59,191] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:25:59,192] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,419106 s
[2024-09-03 18:25:59,226] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileFormatWriter: Write Job e2ffa07a-acc5-4aa1-b7b2-5422109297a7 committed.
[2024-09-03 18:25:59,234] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileFormatWriter: Finished processing stats for write job e2ffa07a-acc5-4aa1-b7b2-5422109297a7.
[2024-09-03 18:25:59,323] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:25:59,323] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:25:59,324] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:25:59,337] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:59,338] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:59,340] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:59,386] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO CodeGenerator: Code generated in 18.433799 ms
[2024-09-03 18:25:59,391] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:25:59,401] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:25:59,402] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:39293 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:25:59,403] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:59,404] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198828 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:25:59,436] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:25:59,438] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:25:59,438] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:25:59,438] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:25:59,438] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:25:59,440] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:25:59,472] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:25:59,476] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:25:59,479] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:39293 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:25:59,480] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:25:59,485] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:25:59,485] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:25:59,485] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:25:59,485] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:25:59,507] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:25:59,507] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:25:59,507] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:25:59,565] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO CodeGenerator: Code generated in 13.794172 ms
[2024-09-03 18:25:59,568] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4524, partition values: [empty row]
[2024-09-03 18:25:59,587] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO CodeGenerator: Code generated in 16.165002 ms
[2024-09-03 18:25:59,600] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileOutputCommitter: Saved output of task 'attempt_202409031825597925367010406167434_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409031825597925367010406167434_0002_m_000000
[2024-09-03 18:25:59,600] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkHadoopMapRedUtil: attempt_202409031825597925367010406167434_0002_m_000000_2: Committed
[2024-09-03 18:25:59,602] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:25:59,606] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 123 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:25:59,606] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:25:59,609] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,166 s
[2024-09-03 18:25:59,610] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:25:59,613] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:25:59,613] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,173438 s
[2024-09-03 18:25:59,637] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileFormatWriter: Write Job 56748584-f657-4898-9c99-e0bc8df8a009 committed.
[2024-09-03 18:25:59,639] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO FileFormatWriter: Finished processing stats for write job 56748584-f657-4898-9c99-e0bc8df8a009.
[2024-09-03 18:25:59,689] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:25:59,704] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:25:59,722] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:25:59,733] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:25:59,734] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO BlockManager: BlockManager stopped
[2024-09-03 18:25:59,743] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:25:59,746] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:25:59,756] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:25:59,756] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:25:59,757] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-082e01a6-f412-4a99-bddf-e407d84c84cc/pyspark-2729321d-a040-4edf-acfe-b4d5d37e16f3
[2024-09-03 18:25:59,759] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-082e01a6-f412-4a99-bddf-e407d84c84cc
[2024-09-03 18:25:59,764] {spark_submit.py:495} INFO - 24/09/03 18:25:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-087804ca-c0d9-4dfa-92b1-97ebcc64e924
[2024-09-03 18:25:59,834] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240903T212545, end_date=20240903T212559
[2024-09-03 18:25:59,850] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:25:59,865] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:45:04,777] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 21:45:04,783] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 21:45:04,784] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:45:04,784] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:45:04,784] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:45:04,796] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 21:45:04,799] {standard_task_runner.py:52} INFO - Started process 419402 to run task
[2024-09-03 21:45:04,802] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpw13su1tq', '--error-file', '/tmp/tmpjsop1qp2']
[2024-09-03 21:45:04,803] {standard_task_runner.py:80} INFO - Job 76: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:45:04,841] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:45:04,885] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 21:45:04,890] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:45:04,891] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 21:45:05,840] {spark_submit.py:495} INFO - 24/09/03 21:45:05 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:45:05,840] {spark_submit.py:495} INFO - 24/09/03 21:45:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:45:06,193] {spark_submit.py:495} INFO - 24/09/03 21:45:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:45:06,723] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:45:06,734] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:45:06,769] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO ResourceUtils: ==============================================================
[2024-09-03 21:45:06,769] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:45:06,770] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO ResourceUtils: ==============================================================
[2024-09-03 21:45:06,770] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:45:06,789] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:45:06,801] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:45:06,802] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:45:06,845] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:45:06,845] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:45:06,845] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:45:06,845] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:45:06,845] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:45:06,994] {spark_submit.py:495} INFO - 24/09/03 21:45:06 INFO Utils: Successfully started service 'sparkDriver' on port 43675.
[2024-09-03 21:45:07,021] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:45:07,055] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:45:07,072] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:45:07,073] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:45:07,077] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:45:07,088] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-548fac27-a2da-4af2-acd8-008ebf2f1f10
[2024-09-03 21:45:07,106] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:45:07,119] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:45:07,306] {spark_submit.py:495} INFO - 24/09/03 21:45:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:45:07,314] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:45:07,358] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:45:07,521] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:45:07,542] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46857.
[2024-09-03 21:45:07,542] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO NettyBlockTransferService: Server created on 192.168.2.128:46857
[2024-09-03 21:45:07,543] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:45:07,550] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46857, None)
[2024-09-03 21:45:07,553] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46857 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46857, None)
[2024-09-03 21:45:07,555] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46857, None)
[2024-09-03 21:45:07,556] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46857, None)
[2024-09-03 21:45:07,999] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:45:07,999] {spark_submit.py:495} INFO - 24/09/03 21:45:07 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:45:08,776] {spark_submit.py:495} INFO - 24/09/03 21:45:08 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 21:45:08,827] {spark_submit.py:495} INFO - 24/09/03 21:45:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:45:10,560] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:45:10,561] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:45:10,564] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:45:10,806] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:45:10,850] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:45:10,852] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46857 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:45:10,859] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:10,865] {spark_submit.py:495} INFO - 24/09/03 21:45:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198838 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:45:11,045] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:11,061] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:45:11,062] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:45:11,062] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:45:11,063] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:45:11,072] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:45:11,146] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:45:11,148] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:45:11,149] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46857 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:45:11,150] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:45:11,162] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:45:11,162] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:45:11,218] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:45:11,237] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:45:11,474] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4534, partition values: [empty row]
[2024-09-03 21:45:11,694] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO CodeGenerator: Code generated in 129.563704 ms
[2024-09-03 21:45:11,735] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 21:45:11,754] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 547 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:45:11,756] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:45:11,861] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,774 s
[2024-09-03 21:45:11,864] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:45:11,865] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:45:11,867] {spark_submit.py:495} INFO - 24/09/03 21:45:11 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,822052 s
[2024-09-03 21:45:12,236] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:45:12,237] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:45:12,238] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:45:12,302] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:12,302] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:12,303] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:12,398] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO CodeGenerator: Code generated in 37.598912 ms
[2024-09-03 21:45:12,443] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO CodeGenerator: Code generated in 27.874258 ms
[2024-09-03 21:45:12,449] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:45:12,459] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:45:12,460] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46857 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:45:12,461] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:12,469] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198838 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:45:12,528] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:12,530] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:45:12,530] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:45:12,530] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:45:12,530] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:45:12,543] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:45:12,600] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:45:12,603] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:45:12,603] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46857 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:45:12,604] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:45:12,606] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:45:12,606] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:45:12,611] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:45:12,612] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:45:12,660] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:12,660] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:12,661] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:12,724] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO CodeGenerator: Code generated in 20.811862 ms
[2024-09-03 21:45:12,727] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4534, partition values: [empty row]
[2024-09-03 21:45:12,769] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO CodeGenerator: Code generated in 33.646967 ms
[2024-09-03 21:45:12,791] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO CodeGenerator: Code generated in 9.128734 ms
[2024-09-03 21:45:12,833] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409032145125019873867906013052_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032145125019873867906013052_0001_m_000000
[2024-09-03 21:45:12,833] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SparkHadoopMapRedUtil: attempt_202409032145125019873867906013052_0001_m_000000_1: Committed
[2024-09-03 21:45:12,843] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:45:12,845] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 238 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:45:12,846] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,302 s
[2024-09-03 21:45:12,846] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:45:12,852] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:45:12,853] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:45:12,853] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,321782 s
[2024-09-03 21:45:12,865] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileFormatWriter: Write Job 8acb7fd3-cb18-40a0-b820-46890f9b9885 committed.
[2024-09-03 21:45:12,868] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileFormatWriter: Finished processing stats for write job 8acb7fd3-cb18-40a0-b820-46890f9b9885.
[2024-09-03 21:45:12,918] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:45:12,918] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:45:12,918] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:45:12,926] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:12,926] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:12,926] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:12,956] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO CodeGenerator: Code generated in 10.144535 ms
[2024-09-03 21:45:12,959] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:45:12,966] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:45:12,966] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46857 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:45:12,968] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:12,968] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198838 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:45:12,986] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:45:12,988] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:45:12,988] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:45:12,988] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:45:12,989] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:45:12,990] {spark_submit.py:495} INFO - 24/09/03 21:45:12 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:45:13,018] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:45:13,021] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:45:13,022] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46857 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:45:13,025] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:45:13,026] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:45:13,027] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:45:13,029] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:45:13,029] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:45:13,042] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:45:13,042] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:45:13,042] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:45:13,075] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO CodeGenerator: Code generated in 12.856464 ms
[2024-09-03 21:45:13,077] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4534, partition values: [empty row]
[2024-09-03 21:45:13,091] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO CodeGenerator: Code generated in 11.133104 ms
[2024-09-03 21:45:13,099] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO FileOutputCommitter: Saved output of task 'attempt_202409032145121598792486686095747_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032145121598792486686095747_0002_m_000000
[2024-09-03 21:45:13,099] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO SparkHadoopMapRedUtil: attempt_202409032145121598792486686095747_0002_m_000000_2: Committed
[2024-09-03 21:45:13,101] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:45:13,104] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 76 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:45:13,106] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,115 s
[2024-09-03 21:45:13,106] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:45:13,106] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:45:13,111] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:45:13,111] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,119852 s
[2024-09-03 21:45:13,143] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO FileFormatWriter: Write Job 17179040-e388-4a1d-b3ef-0cdb6ef4b308 committed.
[2024-09-03 21:45:13,143] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO FileFormatWriter: Finished processing stats for write job 17179040-e388-4a1d-b3ef-0cdb6ef4b308.
[2024-09-03 21:45:13,171] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:45:13,179] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:45:13,190] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:45:13,198] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:45:13,199] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO BlockManager: BlockManager stopped
[2024-09-03 21:45:13,205] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:45:13,207] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:45:13,212] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:45:13,213] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:45:13,214] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-890a6c32-72ff-4e79-8205-48064f65b97f/pyspark-a522fc08-1068-48b0-a5ec-6d996d5ebb19
[2024-09-03 21:45:13,217] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-87f04e5a-8079-405e-bad2-4c3a340b5fa8
[2024-09-03 21:45:13,220] {spark_submit.py:495} INFO - 24/09/03 21:45:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-890a6c32-72ff-4e79-8205-48064f65b97f
[2024-09-03 21:45:13,279] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T004504, end_date=20240904T004513
[2024-09-03 21:45:13,318] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:45:13,342] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:50:45,297] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 21:50:45,303] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 21:50:45,303] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:50:45,304] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:50:45,304] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:50:45,317] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 21:50:45,319] {standard_task_runner.py:52} INFO - Started process 420910 to run task
[2024-09-03 21:50:45,323] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpzs9gaj8t', '--error-file', '/tmp/tmpyboy4jcp']
[2024-09-03 21:50:45,323] {standard_task_runner.py:80} INFO - Job 76: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:50:45,354] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:50:45,394] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 21:50:45,398] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:50:45,399] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 21:50:46,423] {spark_submit.py:495} INFO - 24/09/03 21:50:46 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:50:46,423] {spark_submit.py:495} INFO - 24/09/03 21:50:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:50:48,741] {spark_submit.py:495} INFO - 24/09/03 21:50:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:50:49,370] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:50:49,385] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:50:49,447] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO ResourceUtils: ==============================================================
[2024-09-03 21:50:49,448] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:50:49,448] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO ResourceUtils: ==============================================================
[2024-09-03 21:50:49,449] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:50:49,476] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:50:49,490] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:50:49,490] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:50:49,546] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:50:49,546] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:50:49,547] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:50:49,547] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:50:49,547] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:50:49,708] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO Utils: Successfully started service 'sparkDriver' on port 45805.
[2024-09-03 21:50:49,732] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:50:49,761] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:50:49,777] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:50:49,777] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:50:49,782] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:50:49,793] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-156f55a9-bd56-4d73-8a71-acb39e662dea
[2024-09-03 21:50:49,813] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:50:49,832] {spark_submit.py:495} INFO - 24/09/03 21:50:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:50:50,083] {spark_submit.py:495} INFO - 24/09/03 21:50:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:50:50,100] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:50:50,191] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:50:50,390] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:50:50,412] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38313.
[2024-09-03 21:50:50,413] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO NettyBlockTransferService: Server created on 192.168.2.128:38313
[2024-09-03 21:50:50,415] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:50:50,423] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38313, None)
[2024-09-03 21:50:50,428] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38313 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38313, None)
[2024-09-03 21:50:50,432] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38313, None)
[2024-09-03 21:50:50,434] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38313, None)
[2024-09-03 21:50:50,940] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:50:50,941] {spark_submit.py:495} INFO - 24/09/03 21:50:50 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:50:51,709] {spark_submit.py:495} INFO - 24/09/03 21:50:51 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.
[2024-09-03 21:50:51,764] {spark_submit.py:495} INFO - 24/09/03 21:50:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 21:50:53,452] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:50:53,453] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:50:53,457] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:50:53,709] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:50:53,755] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:50:53,759] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38313 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:50:53,766] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:50:53,773] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198920 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:50:53,942] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:50:53,971] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:50:53,972] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:50:53,972] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:50:53,973] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:50:53,978] {spark_submit.py:495} INFO - 24/09/03 21:50:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:50:54,123] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:50:54,129] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:50:54,130] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38313 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:50:54,135] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:50:54,155] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:50:54,156] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:50:54,228] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:50:54,253] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:50:54,526] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4616, partition values: [empty row]
[2024-09-03 21:50:54,758] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO CodeGenerator: Code generated in 130.770239 ms
[2024-09-03 21:50:54,802] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 21:50:54,811] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 601 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:50:54,814] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:50:54,911] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,915 s
[2024-09-03 21:50:54,914] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:50:54,920] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:50:54,923] {spark_submit.py:495} INFO - 24/09/03 21:50:54 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,980160 s
[2024-09-03 21:50:55,281] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:50:55,282] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:50:55,283] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:50:55,349] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:50:55,349] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:50:55,350] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:50:55,446] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO CodeGenerator: Code generated in 33.437728 ms
[2024-09-03 21:50:55,487] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO CodeGenerator: Code generated in 24.227114 ms
[2024-09-03 21:50:55,495] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:50:55,507] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:50:55,508] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38313 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:50:55,510] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:50:55,512] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198920 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:50:55,561] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:50:55,562] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:50:55,563] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:50:55,563] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:50:55,563] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:50:55,565] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:50:55,605] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:50:55,608] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:50:55,608] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38313 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:50:55,608] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:50:55,609] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:50:55,609] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:50:55,613] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:50:55,615] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:50:55,668] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:50:55,669] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:50:55,669] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:50:55,762] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO CodeGenerator: Code generated in 48.456784 ms
[2024-09-03 21:50:55,767] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4616, partition values: [empty row]
[2024-09-03 21:50:55,789] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO CodeGenerator: Code generated in 18.3104 ms
[2024-09-03 21:50:55,811] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO CodeGenerator: Code generated in 4.770624 ms
[2024-09-03 21:50:55,842] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: Saved output of task 'attempt_202409032150555716024535330762063_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032150555716024535330762063_0001_m_000000
[2024-09-03 21:50:55,843] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SparkHadoopMapRedUtil: attempt_202409032150555716024535330762063_0001_m_000000_1: Committed
[2024-09-03 21:50:55,849] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:50:55,860] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 249 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:50:55,865] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,298 s
[2024-09-03 21:50:55,865] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:50:55,866] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:50:55,867] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:50:55,867] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,306191 s
[2024-09-03 21:50:55,884] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileFormatWriter: Write Job 25b403dc-a3e8-4a6c-b1cc-957322cdcf20 committed.
[2024-09-03 21:50:55,886] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileFormatWriter: Finished processing stats for write job 25b403dc-a3e8-4a6c-b1cc-957322cdcf20.
[2024-09-03 21:50:55,950] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:50:55,950] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:50:55,951] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:50:55,972] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:50:55,972] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:50:55,973] {spark_submit.py:495} INFO - 24/09/03 21:50:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:50:56,007] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO CodeGenerator: Code generated in 13.477209 ms
[2024-09-03 21:50:56,011] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:50:56,021] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:50:56,022] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38313 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:50:56,023] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:50:56,023] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198920 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:50:56,044] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:50:56,045] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:50:56,045] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:50:56,045] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:50:56,045] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:50:56,048] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:50:56,070] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:50:56,073] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:50:56,074] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38313 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:50:56,075] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:50:56,076] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:50:56,076] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:50:56,078] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:50:56,079] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:50:56,091] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:50:56,092] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:50:56,092] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:50:56,125] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO CodeGenerator: Code generated in 8.506875 ms
[2024-09-03 21:50:56,127] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4616, partition values: [empty row]
[2024-09-03 21:50:56,140] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO CodeGenerator: Code generated in 11.464748 ms
[2024-09-03 21:50:56,150] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileOutputCommitter: Saved output of task 'attempt_202409032150565151781517186081036_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032150565151781517186081036_0002_m_000000
[2024-09-03 21:50:56,150] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkHadoopMapRedUtil: attempt_202409032150565151781517186081036_0002_m_000000_2: Committed
[2024-09-03 21:50:56,151] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:50:56,153] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 76 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:50:56,153] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:50:56,155] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,108 s
[2024-09-03 21:50:56,155] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:50:56,156] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:50:56,156] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,111338 s
[2024-09-03 21:50:56,168] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileFormatWriter: Write Job 395ebabc-efad-42ad-914c-16dd2bcdcc84 committed.
[2024-09-03 21:50:56,168] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO FileFormatWriter: Finished processing stats for write job 395ebabc-efad-42ad-914c-16dd2bcdcc84.
[2024-09-03 21:50:56,221] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:50:56,236] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:50:56,258] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:50:56,285] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:50:56,286] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO BlockManager: BlockManager stopped
[2024-09-03 21:50:56,295] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:50:56,300] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:50:56,331] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:50:56,331] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:50:56,331] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-51b9a20c-e778-4a8f-a4be-b43c76cacb95
[2024-09-03 21:50:56,332] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab00a0c1-00f0-4cff-8005-9f7333a6b01f/pyspark-dab401ea-7cb7-4a30-acf2-b9540fb63d6b
[2024-09-03 21:50:56,339] {spark_submit.py:495} INFO - 24/09/03 21:50:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab00a0c1-00f0-4cff-8005-9f7333a6b01f
[2024-09-03 21:50:56,408] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T005045, end_date=20240904T005056
[2024-09-03 21:50:56,449] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:50:56,464] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:57:23,229] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 21:57:23,235] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 21:57:23,235] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:57:23,235] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:57:23,235] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:57:23,244] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 21:57:23,246] {standard_task_runner.py:52} INFO - Started process 423042 to run task
[2024-09-03 21:57:23,249] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp16fs_yck', '--error-file', '/tmp/tmprmdqqunf']
[2024-09-03 21:57:23,249] {standard_task_runner.py:80} INFO - Job 76: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:57:23,282] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:57:23,320] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 21:57:23,324] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:57:23,325] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 21:57:24,326] {spark_submit.py:495} INFO - 24/09/03 21:57:24 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:57:24,327] {spark_submit.py:495} INFO - 24/09/03 21:57:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:57:24,619] {spark_submit.py:495} INFO - 24/09/03 21:57:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:57:25,123] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:57:25,130] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:57:25,166] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO ResourceUtils: ==============================================================
[2024-09-03 21:57:25,166] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:57:25,167] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO ResourceUtils: ==============================================================
[2024-09-03 21:57:25,167] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:57:25,184] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:57:25,197] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:57:25,197] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:57:25,232] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:57:25,232] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:57:25,232] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:57:25,233] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:57:25,233] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:57:25,405] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO Utils: Successfully started service 'sparkDriver' on port 38875.
[2024-09-03 21:57:25,447] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:57:25,479] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:57:25,495] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:57:25,495] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:57:25,498] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:57:25,510] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-777900b0-d08e-499d-9c48-06d199f8f2d7
[2024-09-03 21:57:25,528] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:57:25,543] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:57:25,725] {spark_submit.py:495} INFO - 24/09/03 21:57:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:57:25,731] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:57:25,773] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:57:25,947] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:57:25,968] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34487.
[2024-09-03 21:57:25,969] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO NettyBlockTransferService: Server created on 192.168.2.128:34487
[2024-09-03 21:57:25,970] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:57:25,975] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34487, None)
[2024-09-03 21:57:25,981] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34487 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34487, None)
[2024-09-03 21:57:25,981] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34487, None)
[2024-09-03 21:57:25,981] {spark_submit.py:495} INFO - 24/09/03 21:57:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34487, None)
[2024-09-03 21:57:26,455] {spark_submit.py:495} INFO - 24/09/03 21:57:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:57:26,455] {spark_submit.py:495} INFO - 24/09/03 21:57:26 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:57:27,268] {spark_submit.py:495} INFO - 24/09/03 21:57:27 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2024-09-03 21:57:27,333] {spark_submit.py:495} INFO - 24/09/03 21:57:27 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:57:29,051] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:57:29,053] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:57:29,055] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:57:29,295] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:57:29,342] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:57:29,345] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34487 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:57:29,350] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:29,358] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198784 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:57:29,508] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:29,525] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:57:29,525] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:57:29,525] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:57:29,526] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:57:29,530] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:57:29,625] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:57:29,629] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:57:29,630] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34487 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:57:29,630] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:57:29,644] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:57:29,645] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:57:29,689] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:57:29,714] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:57:29,985] {spark_submit.py:495} INFO - 24/09/03 21:57:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4480, partition values: [empty row]
[2024-09-03 21:57:30,205] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO CodeGenerator: Code generated in 131.510017 ms
[2024-09-03 21:57:30,246] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 21:57:30,256] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 576 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:57:30,258] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:57:30,360] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,808 s
[2024-09-03 21:57:30,365] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:57:30,369] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:57:30,372] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,864040 s
[2024-09-03 21:57:30,742] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:57:30,743] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:57:30,744] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:57:30,807] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:30,807] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:30,808] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:30,878] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO CodeGenerator: Code generated in 24.632096 ms
[2024-09-03 21:57:30,922] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO CodeGenerator: Code generated in 30.597207 ms
[2024-09-03 21:57:30,927] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:57:30,934] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:57:30,937] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34487 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:57:30,937] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:30,940] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198784 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:57:30,995] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:30,996] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:57:30,996] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:57:30,996] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:57:30,996] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:57:30,999] {spark_submit.py:495} INFO - 24/09/03 21:57:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:57:31,043] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:57:31,045] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:57:31,046] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34487 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:57:31,047] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:57:31,047] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:57:31,048] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:57:31,051] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:57:31,053] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:57:31,098] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:31,098] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:31,099] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:31,164] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO CodeGenerator: Code generated in 20.01047 ms
[2024-09-03 21:57:31,167] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4480, partition values: [empty row]
[2024-09-03 21:57:31,192] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO CodeGenerator: Code generated in 22.124133 ms
[2024-09-03 21:57:31,222] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO CodeGenerator: Code generated in 6.343956 ms
[2024-09-03 21:57:31,259] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: Saved output of task 'attempt_20240903215730288034857835763730_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_20240903215730288034857835763730_0001_m_000000
[2024-09-03 21:57:31,260] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkHadoopMapRedUtil: attempt_20240903215730288034857835763730_0001_m_000000_1: Committed
[2024-09-03 21:57:31,264] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:57:31,270] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 219 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:57:31,271] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:57:31,271] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,272 s
[2024-09-03 21:57:31,271] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:57:31,271] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:57:31,274] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,278221 s
[2024-09-03 21:57:31,306] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileFormatWriter: Write Job 2c0ddc9e-c025-4fd7-8d29-4b1cef68056c committed.
[2024-09-03 21:57:31,309] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileFormatWriter: Finished processing stats for write job 2c0ddc9e-c025-4fd7-8d29-4b1cef68056c.
[2024-09-03 21:57:31,347] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:57:31,348] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:57:31,348] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:57:31,358] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:31,359] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:31,359] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:31,393] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO CodeGenerator: Code generated in 13.650636 ms
[2024-09-03 21:57:31,398] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:57:31,406] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:57:31,407] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34487 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:57:31,408] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:31,409] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198784 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:57:31,429] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:57:31,431] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:57:31,431] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:57:31,431] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:57:31,431] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:57:31,433] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:57:31,454] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:57:31,456] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:57:31,457] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34487 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:57:31,457] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:57:31,458] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:57:31,458] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:57:31,460] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:57:31,463] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:57:31,475] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:57:31,476] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:57:31,476] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:57:31,516] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO CodeGenerator: Code generated in 10.079272 ms
[2024-09-03 21:57:31,519] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4480, partition values: [empty row]
[2024-09-03 21:57:31,534] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO CodeGenerator: Code generated in 13.224537 ms
[2024-09-03 21:57:31,543] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileOutputCommitter: Saved output of task 'attempt_202409032157311953158821720454715_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032157311953158821720454715_0002_m_000000
[2024-09-03 21:57:31,544] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkHadoopMapRedUtil: attempt_202409032157311953158821720454715_0002_m_000000_2: Committed
[2024-09-03 21:57:31,545] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:57:31,546] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 87 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:57:31,547] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,113 s
[2024-09-03 21:57:31,547] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:57:31,548] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:57:31,548] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:57:31,549] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,118641 s
[2024-09-03 21:57:31,584] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileFormatWriter: Write Job 69b80634-c27a-4baa-8609-8b39106a9588 committed.
[2024-09-03 21:57:31,584] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO FileFormatWriter: Finished processing stats for write job 69b80634-c27a-4baa-8609-8b39106a9588.
[2024-09-03 21:57:31,628] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:57:31,636] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:57:31,651] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:57:31,660] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:57:31,660] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO BlockManager: BlockManager stopped
[2024-09-03 21:57:31,666] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:57:31,670] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:57:31,680] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:57:31,680] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:57:31,681] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-cf484cfb-5423-460a-81c5-08d24118b5ec
[2024-09-03 21:57:31,684] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-38d28ddf-1930-41bb-baef-6f327ea71d31
[2024-09-03 21:57:31,687] {spark_submit.py:495} INFO - 24/09/03 21:57:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-cf484cfb-5423-460a-81c5-08d24118b5ec/pyspark-641353f4-c189-4f61-a7d6-99b71c005aa3
[2024-09-03 21:57:31,769] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T005723, end_date=20240904T005731
[2024-09-03 21:57:31,781] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:57:31,795] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:01:12,705] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:01:12,716] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:01:12,716] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:01:12,716] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:01:12,716] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:01:12,725] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 22:01:12,727] {standard_task_runner.py:52} INFO - Started process 425065 to run task
[2024-09-03 22:01:12,731] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpuc1o1nax', '--error-file', '/tmp/tmp1rxxcmbu']
[2024-09-03 22:01:12,731] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:01:12,766] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:01:12,811] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 22:01:12,815] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:01:12,816] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 22:01:13,849] {spark_submit.py:495} INFO - 24/09/03 22:01:13 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:01:13,850] {spark_submit.py:495} INFO - 24/09/03 22:01:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:01:14,167] {spark_submit.py:495} INFO - 24/09/03 22:01:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:01:14,655] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:01:14,661] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:01:14,695] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO ResourceUtils: ==============================================================
[2024-09-03 22:01:14,695] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:01:14,695] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO ResourceUtils: ==============================================================
[2024-09-03 22:01:14,696] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:01:14,715] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:01:14,728] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:01:14,728] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:01:14,766] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:01:14,766] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:01:14,766] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:01:14,767] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:01:14,767] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:01:14,912] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO Utils: Successfully started service 'sparkDriver' on port 46221.
[2024-09-03 22:01:14,934] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:01:14,960] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:01:14,975] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:01:14,976] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:01:14,979] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:01:14,989] {spark_submit.py:495} INFO - 24/09/03 22:01:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-46a2558e-3e07-4f69-a77f-3c02fc0bb2f1
[2024-09-03 22:01:15,010] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:01:15,030] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:01:15,233] {spark_submit.py:495} INFO - 24/09/03 22:01:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:01:15,240] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:01:15,287] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:01:15,459] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:01:15,482] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35205.
[2024-09-03 22:01:15,483] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO NettyBlockTransferService: Server created on 192.168.2.128:35205
[2024-09-03 22:01:15,484] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:01:15,490] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 35205, None)
[2024-09-03 22:01:15,493] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:35205 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 35205, None)
[2024-09-03 22:01:15,494] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 35205, None)
[2024-09-03 22:01:15,495] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 35205, None)
[2024-09-03 22:01:15,979] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:01:15,980] {spark_submit.py:495} INFO - 24/09/03 22:01:15 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:01:16,824] {spark_submit.py:495} INFO - 24/09/03 22:01:16 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 22:01:16,875] {spark_submit.py:495} INFO - 24/09/03 22:01:16 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:01:18,559] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:01:18,561] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:01:18,564] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:01:18,815] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:01:18,858] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:01:18,861] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:35205 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:01:18,867] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:18,875] {spark_submit.py:495} INFO - 24/09/03 22:01:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650021 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:01:19,018] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:19,032] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:01:19,034] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:01:19,035] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:01:19,037] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:01:19,047] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:01:19,161] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:01:19,165] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:01:19,166] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:35205 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:01:19,166] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:01:19,176] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:01:19,177] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:01:19,224] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:01:19,238] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:01:19,448] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455717, partition values: [empty row]
[2024-09-03 22:01:19,731] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO CodeGenerator: Code generated in 177.646757 ms
[2024-09-03 22:01:19,811] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 22:01:19,919] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 610 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:01:19,925] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:01:19,930] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,867 s
[2024-09-03 22:01:19,930] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:01:19,932] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:01:19,936] {spark_submit.py:495} INFO - 24/09/03 22:01:19 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,914067 s
[2024-09-03 22:01:20,355] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:01:20,356] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:01:20,357] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:01:20,434] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:20,434] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:20,436] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:20,529] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO CodeGenerator: Code generated in 32.277685 ms
[2024-09-03 22:01:20,572] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO CodeGenerator: Code generated in 28.927311 ms
[2024-09-03 22:01:20,577] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:01:20,585] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:01:20,586] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:35205 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:01:20,587] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:20,591] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650021 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:01:20,647] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:20,648] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:01:20,649] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:01:20,649] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:01:20,649] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:01:20,651] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:01:20,700] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:01:20,702] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:01:20,702] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:35205 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:01:20,707] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:01:20,707] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:01:20,707] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:01:20,711] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:01:20,712] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:01:20,769] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:20,769] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:20,770] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:20,828] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO CodeGenerator: Code generated in 22.784953 ms
[2024-09-03 22:01:20,833] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455717, partition values: [empty row]
[2024-09-03 22:01:20,854] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO CodeGenerator: Code generated in 18.610923 ms
[2024-09-03 22:01:20,878] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO CodeGenerator: Code generated in 5.212486 ms
[2024-09-03 22:01:20,984] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO FileOutputCommitter: Saved output of task 'attempt_202409032201208015093731247321309_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032201208015093731247321309_0001_m_000000
[2024-09-03 22:01:20,985] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO SparkHadoopMapRedUtil: attempt_202409032201208015093731247321309_0001_m_000000_1: Committed
[2024-09-03 22:01:20,989] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:01:20,994] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 286 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:01:20,995] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,344 s
[2024-09-03 22:01:20,995] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:01:20,997] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:01:20,998] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:01:20,999] {spark_submit.py:495} INFO - 24/09/03 22:01:20 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,352066 s
[2024-09-03 22:01:21,014] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileFormatWriter: Write Job 87e86315-b6a3-416f-ab2e-716cc1e64e78 committed.
[2024-09-03 22:01:21,017] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileFormatWriter: Finished processing stats for write job 87e86315-b6a3-416f-ab2e-716cc1e64e78.
[2024-09-03 22:01:21,060] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:01:21,060] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:01:21,060] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:01:21,092] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:21,092] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:21,092] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:21,122] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO CodeGenerator: Code generated in 9.386893 ms
[2024-09-03 22:01:21,127] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:01:21,134] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:01:21,136] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:35205 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:01:21,136] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:21,137] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650021 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:01:21,153] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:01:21,155] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:01:21,155] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:01:21,155] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:01:21,155] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:01:21,155] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:01:21,172] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:01:21,175] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:01:21,175] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:35205 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:01:21,176] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:01:21,176] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:01:21,176] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:01:21,177] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:01:21,177] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:01:21,197] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:01:21,198] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:01:21,198] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:01:21,240] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO CodeGenerator: Code generated in 8.80686 ms
[2024-09-03 22:01:21,248] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455717, partition values: [empty row]
[2024-09-03 22:01:21,265] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO CodeGenerator: Code generated in 14.406018 ms
[2024-09-03 22:01:21,302] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileOutputCommitter: Saved output of task 'attempt_202409032201211913357066897926344_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032201211913357066897926344_0002_m_000000
[2024-09-03 22:01:21,302] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkHadoopMapRedUtil: attempt_202409032201211913357066897926344_0002_m_000000_2: Committed
[2024-09-03 22:01:21,303] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:01:21,305] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 127 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:01:21,305] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:01:21,306] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,149 s
[2024-09-03 22:01:21,306] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:01:21,306] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:01:21,307] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,153426 s
[2024-09-03 22:01:21,332] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileFormatWriter: Write Job 094db15b-f326-43e5-a73d-606e64c0d0e1 committed.
[2024-09-03 22:01:21,332] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO FileFormatWriter: Finished processing stats for write job 094db15b-f326-43e5-a73d-606e64c0d0e1.
[2024-09-03 22:01:21,367] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:01:21,375] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:01:21,387] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:01:21,396] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:01:21,396] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO BlockManager: BlockManager stopped
[2024-09-03 22:01:21,403] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:01:21,405] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:01:21,410] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:01:21,411] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:01:21,411] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9bb41660-0f4b-4d61-8b75-ea0838585ac3/pyspark-a4d0a40e-6b88-4651-8e50-5d151de99fc8
[2024-09-03 22:01:21,413] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce1f394d-4064-4d59-9212-a56109aca5ed
[2024-09-03 22:01:21,417] {spark_submit.py:495} INFO - 24/09/03 22:01:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9bb41660-0f4b-4d61-8b75-ea0838585ac3
[2024-09-03 22:01:21,499] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T010112, end_date=20240904T010121
[2024-09-03 22:01:21,524] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:01:21,541] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:04:33,212] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:04:33,217] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:04:33,217] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:04:33,217] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:04:33,217] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:04:33,225] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 22:04:33,227] {standard_task_runner.py:52} INFO - Started process 426858 to run task
[2024-09-03 22:04:33,230] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmprwqfrcls', '--error-file', '/tmp/tmpi36dtx3h']
[2024-09-03 22:04:33,231] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:04:33,266] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:04:33,304] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 22:04:33,308] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:04:33,309] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 22:04:34,427] {spark_submit.py:495} INFO - 24/09/03 22:04:34 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:04:34,428] {spark_submit.py:495} INFO - 24/09/03 22:04:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:04:34,762] {spark_submit.py:495} INFO - 24/09/03 22:04:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:04:35,315] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:04:35,323] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:04:35,362] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO ResourceUtils: ==============================================================
[2024-09-03 22:04:35,363] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:04:35,363] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO ResourceUtils: ==============================================================
[2024-09-03 22:04:35,363] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:04:35,383] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:04:35,400] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:04:35,400] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:04:35,442] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:04:35,442] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:04:35,443] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:04:35,443] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:04:35,443] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:04:35,639] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO Utils: Successfully started service 'sparkDriver' on port 34155.
[2024-09-03 22:04:35,664] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:04:35,693] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:04:35,710] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:04:35,711] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:04:35,714] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:04:35,725] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9ff1777b-2870-49cf-800e-459c809e52dd
[2024-09-03 22:04:35,745] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:04:35,760] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:04:35,975] {spark_submit.py:495} INFO - 24/09/03 22:04:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:04:35,982] {spark_submit.py:495} INFO - 24/09/03 22:04:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:04:36,048] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:04:36,237] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:04:36,264] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38071.
[2024-09-03 22:04:36,265] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO NettyBlockTransferService: Server created on 192.168.2.128:38071
[2024-09-03 22:04:36,267] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:04:36,272] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38071, None)
[2024-09-03 22:04:36,274] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38071 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38071, None)
[2024-09-03 22:04:36,276] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38071, None)
[2024-09-03 22:04:36,277] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38071, None)
[2024-09-03 22:04:36,713] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:04:36,713] {spark_submit.py:495} INFO - 24/09/03 22:04:36 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:04:37,666] {spark_submit.py:495} INFO - 24/09/03 22:04:37 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
[2024-09-03 22:04:37,770] {spark_submit.py:495} INFO - 24/09/03 22:04:37 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2024-09-03 22:04:39,759] {spark_submit.py:495} INFO - 24/09/03 22:04:39 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:04:39,760] {spark_submit.py:495} INFO - 24/09/03 22:04:39 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:04:39,763] {spark_submit.py:495} INFO - 24/09/03 22:04:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:04:40,040] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:04:40,085] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:04:40,087] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38071 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:04:40,096] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:40,106] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198749 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:04:40,287] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:40,302] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:04:40,303] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:04:40,304] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:04:40,306] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:04:40,316] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:04:40,418] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:04:40,443] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:04:40,444] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38071 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:04:40,447] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:04:40,480] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:04:40,490] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:04:40,541] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:04:40,566] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:04:40,823] {spark_submit.py:495} INFO - 24/09/03 22:04:40 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4445, partition values: [empty row]
[2024-09-03 22:04:41,076] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO CodeGenerator: Code generated in 139.362924 ms
[2024-09-03 22:04:41,136] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 22:04:41,146] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 614 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:04:41,242] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:04:41,252] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,918 s
[2024-09-03 22:04:41,255] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:04:41,255] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:04:41,259] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,970764 s
[2024-09-03 22:04:41,750] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:04:41,752] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:04:41,753] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:04:41,832] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:41,833] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:41,834] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:41,910] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO CodeGenerator: Code generated in 25.961507 ms
[2024-09-03 22:04:41,950] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO CodeGenerator: Code generated in 27.670886 ms
[2024-09-03 22:04:41,960] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:04:41,970] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:04:41,971] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38071 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:04:41,972] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:41,977] {spark_submit.py:495} INFO - 24/09/03 22:04:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198749 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:04:42,042] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:42,045] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:04:42,046] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:04:42,046] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:04:42,047] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:04:42,055] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:04:42,100] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:04:42,104] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:04:42,110] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38071 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:04:42,124] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:04:42,135] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:04:42,136] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:04:42,137] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:04:42,138] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:04:42,225] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:42,226] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:42,227] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:42,298] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO CodeGenerator: Code generated in 23.449423 ms
[2024-09-03 22:04:42,302] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4445, partition values: [empty row]
[2024-09-03 22:04:42,323] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO CodeGenerator: Code generated in 18.138579 ms
[2024-09-03 22:04:42,352] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO CodeGenerator: Code generated in 5.227099 ms
[2024-09-03 22:04:42,391] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: Saved output of task 'attempt_202409032204427105934920295259607_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032204427105934920295259607_0001_m_000000
[2024-09-03 22:04:42,392] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkHadoopMapRedUtil: attempt_202409032204427105934920295259607_0001_m_000000_1: Committed
[2024-09-03 22:04:42,399] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:04:42,401] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 268 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:04:42,402] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,346 s
[2024-09-03 22:04:42,402] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:04:42,402] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:04:42,402] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:04:42,403] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,359876 s
[2024-09-03 22:04:42,424] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileFormatWriter: Write Job 15fb4174-ef34-43cd-99c5-34b4cefb5ea2 committed.
[2024-09-03 22:04:42,431] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileFormatWriter: Finished processing stats for write job 15fb4174-ef34-43cd-99c5-34b4cefb5ea2.
[2024-09-03 22:04:42,475] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:04:42,475] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:04:42,476] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:04:42,513] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:42,513] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:42,513] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:42,551] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO CodeGenerator: Code generated in 10.876881 ms
[2024-09-03 22:04:42,556] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:04:42,566] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:04:42,568] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38071 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:04:42,568] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:42,570] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198749 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:04:42,591] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:04:42,592] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:04:42,592] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:04:42,593] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:04:42,593] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:04:42,594] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:04:42,613] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:04:42,616] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:04:42,617] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38071 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:04:42,618] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:04:42,618] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:04:42,619] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:04:42,621] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:04:42,623] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:04:42,636] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:04:42,637] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:04:42,637] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:04:42,672] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO CodeGenerator: Code generated in 12.349536 ms
[2024-09-03 22:04:42,674] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4445, partition values: [empty row]
[2024-09-03 22:04:42,690] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO CodeGenerator: Code generated in 13.635064 ms
[2024-09-03 22:04:42,700] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileOutputCommitter: Saved output of task 'attempt_202409032204425662994687608327345_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032204425662994687608327345_0002_m_000000
[2024-09-03 22:04:42,701] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkHadoopMapRedUtil: attempt_202409032204425662994687608327345_0002_m_000000_2: Committed
[2024-09-03 22:04:42,701] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:04:42,702] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 83 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:04:42,703] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,107 s
[2024-09-03 22:04:42,703] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:04:42,704] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:04:42,704] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:04:42,704] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,112403 s
[2024-09-03 22:04:42,720] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileFormatWriter: Write Job f468a2c3-3ea1-4afb-9702-1d08c0535065 committed.
[2024-09-03 22:04:42,720] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO FileFormatWriter: Finished processing stats for write job f468a2c3-3ea1-4afb-9702-1d08c0535065.
[2024-09-03 22:04:42,755] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:04:42,764] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:04:42,777] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:04:42,784] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:04:42,784] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO BlockManager: BlockManager stopped
[2024-09-03 22:04:42,789] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:04:42,792] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:04:42,798] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:04:42,799] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:04:42,800] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-87bcc4a9-a7e5-47ef-b504-effdabae6fb8
[2024-09-03 22:04:42,801] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-13244d0a-4b0e-4300-b7da-e82e342f264f
[2024-09-03 22:04:42,803] {spark_submit.py:495} INFO - 24/09/03 22:04:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-87bcc4a9-a7e5-47ef-b504-effdabae6fb8/pyspark-cafb620d-7ca7-4c9b-a76d-f0f13f1552d1
[2024-09-03 22:04:42,854] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T010433, end_date=20240904T010442
[2024-09-03 22:04:42,896] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:04:42,914] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:15:27,354] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:15:27,361] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:15:27,361] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:15:27,361] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:15:27,361] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:15:27,370] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 22:15:27,372] {standard_task_runner.py:52} INFO - Started process 428676 to run task
[2024-09-03 22:15:27,376] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpc98x2r7e', '--error-file', '/tmp/tmp7t3dyw_g']
[2024-09-03 22:15:27,377] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:15:27,410] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:15:27,450] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 22:15:27,455] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:15:27,456] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 22:15:28,472] {spark_submit.py:495} INFO - 24/09/03 22:15:28 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:15:28,473] {spark_submit.py:495} INFO - 24/09/03 22:15:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:15:28,796] {spark_submit.py:495} INFO - 24/09/03 22:15:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:15:29,365] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:15:29,376] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:15:29,415] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO ResourceUtils: ==============================================================
[2024-09-03 22:15:29,416] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:15:29,416] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO ResourceUtils: ==============================================================
[2024-09-03 22:15:29,417] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:15:29,437] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:15:29,451] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:15:29,453] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:15:29,498] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:15:29,498] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:15:29,499] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:15:29,499] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:15:29,499] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:15:29,666] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO Utils: Successfully started service 'sparkDriver' on port 37019.
[2024-09-03 22:15:29,700] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:15:29,773] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:15:29,798] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:15:29,800] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:15:29,806] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:15:29,828] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-69895563-9a44-4bf8-b146-2b09ca80da9c
[2024-09-03 22:15:29,870] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:15:29,900] {spark_submit.py:495} INFO - 24/09/03 22:15:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:15:30,192] {spark_submit.py:495} INFO - 24/09/03 22:15:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:15:30,200] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:15:30,275] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:15:30,462] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:15:30,488] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45725.
[2024-09-03 22:15:30,489] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO NettyBlockTransferService: Server created on 192.168.2.128:45725
[2024-09-03 22:15:30,491] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:15:30,497] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 45725, None)
[2024-09-03 22:15:30,500] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:45725 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 45725, None)
[2024-09-03 22:15:30,504] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 45725, None)
[2024-09-03 22:15:30,505] {spark_submit.py:495} INFO - 24/09/03 22:15:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 45725, None)
[2024-09-03 22:15:31,025] {spark_submit.py:495} INFO - 24/09/03 22:15:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:15:31,025] {spark_submit.py:495} INFO - 24/09/03 22:15:31 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:15:31,859] {spark_submit.py:495} INFO - 24/09/03 22:15:31 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 22:15:31,919] {spark_submit.py:495} INFO - 24/09/03 22:15:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 22:15:33,724] {spark_submit.py:495} INFO - 24/09/03 22:15:33 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:15:33,725] {spark_submit.py:495} INFO - 24/09/03 22:15:33 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:15:33,728] {spark_submit.py:495} INFO - 24/09/03 22:15:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:15:34,003] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:15:34,060] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:15:34,063] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:45725 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:15:34,071] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:34,080] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198888 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:15:34,236] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:34,259] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:15:34,260] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:15:34,261] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:15:34,263] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:15:34,272] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:15:34,386] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:15:34,389] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:15:34,389] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:45725 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:15:34,391] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:15:34,403] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:15:34,404] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:15:34,457] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:15:34,478] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:15:34,744] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4584, partition values: [empty row]
[2024-09-03 22:15:34,988] {spark_submit.py:495} INFO - 24/09/03 22:15:34 INFO CodeGenerator: Code generated in 139.46449 ms
[2024-09-03 22:15:35,032] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 22:15:35,042] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 600 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:15:35,143] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:15:35,148] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,831 s
[2024-09-03 22:15:35,156] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:15:35,156] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:15:35,160] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,922543 s
[2024-09-03 22:15:35,563] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:15:35,564] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:15:35,565] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:15:35,635] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:35,635] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:35,636] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:35,711] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO CodeGenerator: Code generated in 26.395305 ms
[2024-09-03 22:15:35,757] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO CodeGenerator: Code generated in 31.52022 ms
[2024-09-03 22:15:35,763] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:15:35,770] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:15:35,771] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:45725 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:15:35,773] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:35,776] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198888 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:15:35,836] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:35,838] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:15:35,838] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:15:35,838] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:15:35,838] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:15:35,839] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:15:35,903] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:15:35,906] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:15:35,908] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:45725 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:15:35,908] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:15:35,909] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:15:35,909] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:15:35,913] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:15:35,914] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:15:35,989] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:35,989] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:35,990] {spark_submit.py:495} INFO - 24/09/03 22:15:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:36,057] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO CodeGenerator: Code generated in 26.132975 ms
[2024-09-03 22:15:36,061] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4584, partition values: [empty row]
[2024-09-03 22:15:36,090] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO CodeGenerator: Code generated in 25.803238 ms
[2024-09-03 22:15:36,112] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO CodeGenerator: Code generated in 4.605173 ms
[2024-09-03 22:15:36,149] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileOutputCommitter: Saved output of task 'attempt_2024090322153576386021143354504_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_2024090322153576386021143354504_0001_m_000000
[2024-09-03 22:15:36,150] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkHadoopMapRedUtil: attempt_2024090322153576386021143354504_0001_m_000000_1: Committed
[2024-09-03 22:15:36,155] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:15:36,167] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 256 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:15:36,167] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:15:36,168] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,327 s
[2024-09-03 22:15:36,168] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:15:36,168] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:15:36,168] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,332232 s
[2024-09-03 22:15:36,184] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileFormatWriter: Write Job 91d57850-f274-4183-80a7-dcaf3b27b6eb committed.
[2024-09-03 22:15:36,188] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileFormatWriter: Finished processing stats for write job 91d57850-f274-4183-80a7-dcaf3b27b6eb.
[2024-09-03 22:15:36,223] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:15:36,224] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:15:36,225] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:15:36,236] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:36,237] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:36,238] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:36,267] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO CodeGenerator: Code generated in 10.317441 ms
[2024-09-03 22:15:36,273] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:15:36,283] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:15:36,287] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:45725 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:15:36,288] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:36,290] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198888 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:15:36,306] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:15:36,308] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:15:36,308] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:15:36,309] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:15:36,309] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:15:36,309] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:15:36,326] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:15:36,328] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:15:36,329] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:45725 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:15:36,330] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:15:36,331] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:15:36,331] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:15:36,333] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:15:36,334] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:15:36,346] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:15:36,347] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:15:36,348] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:15:36,384] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO CodeGenerator: Code generated in 8.864815 ms
[2024-09-03 22:15:36,388] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-4584, partition values: [empty row]
[2024-09-03 22:15:36,402] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO CodeGenerator: Code generated in 10.31294 ms
[2024-09-03 22:15:36,412] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileOutputCommitter: Saved output of task 'attempt_202409032215367755473206780369829_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032215367755473206780369829_0002_m_000000
[2024-09-03 22:15:36,413] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkHadoopMapRedUtil: attempt_202409032215367755473206780369829_0002_m_000000_2: Committed
[2024-09-03 22:15:36,414] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:15:36,416] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 84 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:15:36,416] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:15:36,417] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,108 s
[2024-09-03 22:15:36,418] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:15:36,418] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:15:36,419] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,110930 s
[2024-09-03 22:15:36,446] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileFormatWriter: Write Job 70e3f6d8-36cc-4fb2-8dd3-9a172d85fc50 committed.
[2024-09-03 22:15:36,447] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO FileFormatWriter: Finished processing stats for write job 70e3f6d8-36cc-4fb2-8dd3-9a172d85fc50.
[2024-09-03 22:15:36,492] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:15:36,501] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:15:36,517] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:15:36,529] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:15:36,530] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO BlockManager: BlockManager stopped
[2024-09-03 22:15:36,539] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:15:36,544] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:15:36,550] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:15:36,550] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:15:36,551] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-560edad6-2e3f-4bb2-a8ba-013933db6c98/pyspark-17a1aa68-e951-4d01-8248-476b3226e5e3
[2024-09-03 22:15:36,554] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-30386de7-c9a2-4c26-88ca-aade19c473eb
[2024-09-03 22:15:36,558] {spark_submit.py:495} INFO - 24/09/03 22:15:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-560edad6-2e3f-4bb2-a8ba-013933db6c98
[2024-09-03 22:15:36,640] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T011527, end_date=20240904T011536
[2024-09-03 22:15:36,664] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:15:36,708] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:29:21,206] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:29:21,213] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:29:21,213] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:29:21,213] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:29:21,213] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:29:21,222] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 22:29:21,224] {standard_task_runner.py:52} INFO - Started process 432406 to run task
[2024-09-03 22:29:21,226] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpev4vm56w', '--error-file', '/tmp/tmprko7ihlw']
[2024-09-03 22:29:21,227] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:29:21,261] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:29:21,307] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 22:29:21,310] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:29:21,311] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 22:29:22,494] {spark_submit.py:495} INFO - 24/09/03 22:29:22 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:29:22,496] {spark_submit.py:495} INFO - 24/09/03 22:29:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:29:22,864] {spark_submit.py:495} INFO - 24/09/03 22:29:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:29:23,483] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:29:23,494] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:29:23,568] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO ResourceUtils: ==============================================================
[2024-09-03 22:29:23,569] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:29:23,571] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO ResourceUtils: ==============================================================
[2024-09-03 22:29:23,573] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:29:23,605] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:29:23,624] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:29:23,625] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:29:23,687] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:29:23,687] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:29:23,688] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:29:23,688] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:29:23,688] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:29:23,896] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO Utils: Successfully started service 'sparkDriver' on port 38201.
[2024-09-03 22:29:23,924] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:29:23,960] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:29:23,978] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:29:23,979] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:29:23,983] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:29:23,997] {spark_submit.py:495} INFO - 24/09/03 22:29:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4bcba265-1c7f-444b-ada1-5e6fcb4c250a
[2024-09-03 22:29:24,020] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:29:24,040] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:29:24,261] {spark_submit.py:495} INFO - 24/09/03 22:29:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:29:24,270] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:29:24,323] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:29:24,548] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:29:24,574] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37931.
[2024-09-03 22:29:24,574] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO NettyBlockTransferService: Server created on 192.168.2.128:37931
[2024-09-03 22:29:24,576] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:29:24,582] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 37931, None)
[2024-09-03 22:29:24,587] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:37931 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 37931, None)
[2024-09-03 22:29:24,589] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 37931, None)
[2024-09-03 22:29:24,591] {spark_submit.py:495} INFO - 24/09/03 22:29:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 37931, None)
[2024-09-03 22:29:25,151] {spark_submit.py:495} INFO - 24/09/03 22:29:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:29:25,152] {spark_submit.py:495} INFO - 24/09/03 22:29:25 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:29:26,250] {spark_submit.py:495} INFO - 24/09/03 22:29:26 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.
[2024-09-03 22:29:26,325] {spark_submit.py:495} INFO - 24/09/03 22:29:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 22:29:28,421] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:29:28,422] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:29:28,426] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:29:28,765] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:29:28,828] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:29:28,832] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:37931 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:29:28,839] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:28,853] {spark_submit.py:495} INFO - 24/09/03 22:29:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648944 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:29:29,103] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:29,124] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:29:29,124] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:29:29,124] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:29:29,126] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:29:29,133] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:29:29,275] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:29:29,280] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:29:29,281] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:37931 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:29:29,282] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:29:29,298] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:29:29,299] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:29:29,358] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:29:29,377] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:29:29,659] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-454640, partition values: [empty row]
[2024-09-03 22:29:29,940] {spark_submit.py:495} INFO - 24/09/03 22:29:29 INFO CodeGenerator: Code generated in 162.195914 ms
[2024-09-03 22:29:30,091] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:29:30,205] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 859 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:29:30,214] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:29:30,218] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,066 s
[2024-09-03 22:29:30,222] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:29:30,223] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:29:30,227] {spark_submit.py:495} INFO - 24/09/03 22:29:30 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,122684 s
[2024-09-03 22:29:31,043] {spark_submit.py:495} INFO - 24/09/03 22:29:31 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:29:31,045] {spark_submit.py:495} INFO - 24/09/03 22:29:31 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:29:31,045] {spark_submit.py:495} INFO - 24/09/03 22:29:31 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:29:31,685] {spark_submit.py:495} INFO - 24/09/03 22:29:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:31,685] {spark_submit.py:495} INFO - 24/09/03 22:29:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:31,687] {spark_submit.py:495} INFO - 24/09/03 22:29:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:32,924] {spark_submit.py:495} INFO - 24/09/03 22:29:32 INFO CodeGenerator: Code generated in 87.289228 ms
[2024-09-03 22:29:33,017] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO CodeGenerator: Code generated in 72.955657 ms
[2024-09-03 22:29:33,029] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:29:33,042] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:29:33,043] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:37931 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:29:33,044] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:33,051] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648944 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:29:33,150] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:33,168] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:29:33,168] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:29:33,168] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:29:33,168] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:29:33,172] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:29:33,414] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:29:33,417] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:29:33,434] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:37931 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:29:33,436] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:29:33,437] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:29:33,438] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:29:33,442] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:29:33,457] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:29:33,691] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:33,692] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:33,692] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:33,857] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO CodeGenerator: Code generated in 23.911279 ms
[2024-09-03 22:29:33,871] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-454640, partition values: [empty row]
[2024-09-03 22:29:33,928] {spark_submit.py:495} INFO - 24/09/03 22:29:33 INFO CodeGenerator: Code generated in 50.874107 ms
[2024-09-03 22:29:34,078] {spark_submit.py:495} INFO - 24/09/03 22:29:34 INFO CodeGenerator: Code generated in 50.383163 ms
[2024-09-03 22:29:34,399] {spark_submit.py:495} INFO - 24/09/03 22:29:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409032229333981509799938857510_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032229333981509799938857510_0001_m_000000
[2024-09-03 22:29:34,400] {spark_submit.py:495} INFO - 24/09/03 22:29:34 INFO SparkHadoopMapRedUtil: attempt_202409032229333981509799938857510_0001_m_000000_1: Committed
[2024-09-03 22:29:34,404] {spark_submit.py:495} INFO - 24/09/03 22:29:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:29:35,021] {spark_submit.py:495} INFO - 24/09/03 22:29:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1582 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:29:35,026] {spark_submit.py:495} INFO - 24/09/03 22:29:35 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 1,848 s
[2024-09-03 22:29:35,027] {spark_submit.py:495} INFO - 24/09/03 22:29:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:29:35,029] {spark_submit.py:495} INFO - 24/09/03 22:29:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:29:35,030] {spark_submit.py:495} INFO - 24/09/03 22:29:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:29:35,031] {spark_submit.py:495} INFO - 24/09/03 22:29:35 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 1,880579 s
[2024-09-03 22:29:36,373] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileFormatWriter: Write Job e92089d3-ccfd-431b-940e-cbb027c7fde1 committed.
[2024-09-03 22:29:36,378] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileFormatWriter: Finished processing stats for write job e92089d3-ccfd-431b-940e-cbb027c7fde1.
[2024-09-03 22:29:36,428] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:29:36,428] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:29:36,429] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:29:36,467] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:36,467] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:36,468] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:36,518] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO CodeGenerator: Code generated in 12.628215 ms
[2024-09-03 22:29:36,523] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:29:36,538] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:29:36,542] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:37931 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:29:36,543] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:36,544] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648944 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:29:36,568] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:29:36,569] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:29:36,569] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:29:36,569] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:29:36,570] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:29:36,571] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:29:36,610] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:29:36,615] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:29:36,616] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:37931 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:29:36,617] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:29:36,618] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:29:36,618] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:29:36,621] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:29:36,622] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:29:36,650] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:29:36,650] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:29:36,651] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:29:36,695] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO CodeGenerator: Code generated in 11.416496 ms
[2024-09-03 22:29:36,699] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-454640, partition values: [empty row]
[2024-09-03 22:29:36,719] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO CodeGenerator: Code generated in 18.164063 ms
[2024-09-03 22:29:36,767] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileOutputCommitter: Saved output of task 'attempt_202409032229361766400660059806208_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032229361766400660059806208_0002_m_000000
[2024-09-03 22:29:36,768] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SparkHadoopMapRedUtil: attempt_202409032229361766400660059806208_0002_m_000000_2: Committed
[2024-09-03 22:29:36,770] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:29:36,776] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 155 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:29:36,777] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:29:36,783] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,205 s
[2024-09-03 22:29:36,785] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:29:36,785] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:29:36,786] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,215503 s
[2024-09-03 22:29:36,813] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileFormatWriter: Write Job 25a4b930-5f25-4615-84e2-095f6ed9adb4 committed.
[2024-09-03 22:29:36,813] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO FileFormatWriter: Finished processing stats for write job 25a4b930-5f25-4615-84e2-095f6ed9adb4.
[2024-09-03 22:29:36,868] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:29:36,886] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:29:36,983] {spark_submit.py:495} INFO - 24/09/03 22:29:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:29:37,018] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:29:37,019] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO BlockManager: BlockManager stopped
[2024-09-03 22:29:37,040] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:29:37,069] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:29:37,146] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:29:37,229] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:29:37,229] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd2b300b-67dc-48ee-bf89-849076239964
[2024-09-03 22:29:37,253] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-875fea5f-e3fc-4900-8a43-9198ed9882ab
[2024-09-03 22:29:37,257] {spark_submit.py:495} INFO - 24/09/03 22:29:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd2b300b-67dc-48ee-bf89-849076239964/pyspark-e2293b78-e39b-4109-8f1e-a0e692eaed60
[2024-09-03 22:29:37,406] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T012921, end_date=20240904T012937
[2024-09-03 22:29:37,460] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:29:37,474] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:36:39,095] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:36:39,102] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:36:39,102] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:36:39,102] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:36:39,102] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:36:39,111] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 22:36:39,113] {standard_task_runner.py:52} INFO - Started process 435940 to run task
[2024-09-03 22:36:39,116] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpik2a8syw', '--error-file', '/tmp/tmp4ddjnx16']
[2024-09-03 22:36:39,117] {standard_task_runner.py:80} INFO - Job 78: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:36:39,153] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:36:39,196] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 22:36:39,200] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:36:39,201] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 22:36:40,239] {spark_submit.py:495} INFO - 24/09/03 22:36:40 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:36:40,240] {spark_submit.py:495} INFO - 24/09/03 22:36:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:36:40,579] {spark_submit.py:495} INFO - 24/09/03 22:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:36:41,114] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:36:41,124] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:36:41,163] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO ResourceUtils: ==============================================================
[2024-09-03 22:36:41,164] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:36:41,165] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO ResourceUtils: ==============================================================
[2024-09-03 22:36:41,166] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:36:41,188] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:36:41,202] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:36:41,203] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:36:41,244] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:36:41,244] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:36:41,245] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:36:41,245] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:36:41,245] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:36:41,424] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO Utils: Successfully started service 'sparkDriver' on port 35149.
[2024-09-03 22:36:41,451] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:36:41,483] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:36:41,504] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:36:41,505] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:36:41,510] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:36:41,523] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-552289cc-7ef5-44e0-a6a4-529276132243
[2024-09-03 22:36:41,546] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:36:41,564] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:36:41,762] {spark_submit.py:495} INFO - 24/09/03 22:36:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:36:41,770] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:36:41,823] {spark_submit.py:495} INFO - 24/09/03 22:36:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:36:42,036] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:36:42,058] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36163.
[2024-09-03 22:36:42,058] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO NettyBlockTransferService: Server created on 192.168.2.128:36163
[2024-09-03 22:36:42,060] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:36:42,067] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36163, None)
[2024-09-03 22:36:42,070] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36163 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36163, None)
[2024-09-03 22:36:42,073] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36163, None)
[2024-09-03 22:36:42,074] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36163, None)
[2024-09-03 22:36:42,491] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:36:42,491] {spark_submit.py:495} INFO - 24/09/03 22:36:42 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:36:43,306] {spark_submit.py:495} INFO - 24/09/03 22:36:43 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 22:36:43,361] {spark_submit.py:495} INFO - 24/09/03 22:36:43 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:36:45,137] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:36:45,138] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:36:45,141] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:36:45,396] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:36:45,441] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:36:45,443] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36163 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:36:45,451] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:45,459] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649693 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:36:45,610] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:45,626] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:36:45,627] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:36:45,627] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:36:45,628] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:36:45,633] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:36:45,713] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:36:45,715] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:36:45,716] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36163 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:36:45,716] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:36:45,728] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:36:45,728] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:36:45,792] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:36:45,807] {spark_submit.py:495} INFO - 24/09/03 22:36:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:36:46,059] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455389, partition values: [empty row]
[2024-09-03 22:36:46,283] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO CodeGenerator: Code generated in 130.750211 ms
[2024-09-03 22:36:46,370] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:36:46,390] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 614 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:36:46,392] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:36:46,501] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,853 s
[2024-09-03 22:36:46,505] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:36:46,505] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:36:46,512] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,896964 s
[2024-09-03 22:36:46,904] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:36:46,905] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:36:46,905] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:36:46,967] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:46,968] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:46,968] {spark_submit.py:495} INFO - 24/09/03 22:36:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:47,042] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 26.627138 ms
[2024-09-03 22:36:47,079] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 24.12241 ms
[2024-09-03 22:36:47,085] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:36:47,093] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:36:47,094] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36163 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:36:47,094] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:47,097] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649693 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:36:47,152] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:47,154] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:36:47,154] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:36:47,154] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:36:47,155] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:36:47,155] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:36:47,199] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:36:47,202] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:36:47,202] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36163 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:36:47,203] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:36:47,204] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:36:47,204] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:36:47,207] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:36:47,208] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:36:47,250] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:47,250] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:47,250] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:47,313] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 17.6804 ms
[2024-09-03 22:36:47,316] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455389, partition values: [empty row]
[2024-09-03 22:36:47,337] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 16.656444 ms
[2024-09-03 22:36:47,363] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 6.633567 ms
[2024-09-03 22:36:47,524] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: Saved output of task 'attempt_20240903223647839736361018629568_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_20240903223647839736361018629568_0001_m_000000
[2024-09-03 22:36:47,525] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkHadoopMapRedUtil: attempt_20240903223647839736361018629568_0001_m_000000_1: Committed
[2024-09-03 22:36:47,530] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:36:47,533] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 328 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:36:47,533] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,377 s
[2024-09-03 22:36:47,534] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:36:47,539] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:36:47,541] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:36:47,546] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,389217 s
[2024-09-03 22:36:47,570] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileFormatWriter: Write Job be4a7b45-29a2-425d-a9bf-df3932f24c56 committed.
[2024-09-03 22:36:47,575] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileFormatWriter: Finished processing stats for write job be4a7b45-29a2-425d-a9bf-df3932f24c56.
[2024-09-03 22:36:47,622] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:36:47,623] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:36:47,624] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:36:47,641] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:47,641] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:47,641] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:47,685] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 13.928728 ms
[2024-09-03 22:36:47,689] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:36:47,699] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:36:47,702] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36163 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:36:47,703] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:47,704] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649693 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:36:47,724] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:36:47,726] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:36:47,726] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:36:47,726] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:36:47,726] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:36:47,727] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:36:47,768] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:36:47,771] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:36:47,775] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36163 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:36:47,775] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:36:47,775] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:36:47,775] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:36:47,775] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:36:47,775] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:36:47,794] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:36:47,794] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:36:47,794] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:36:47,830] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 10.896746 ms
[2024-09-03 22:36:47,839] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455389, partition values: [empty row]
[2024-09-03 22:36:47,853] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO CodeGenerator: Code generated in 13.595905 ms
[2024-09-03 22:36:47,889] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileOutputCommitter: Saved output of task 'attempt_20240903223647457182342124536002_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_20240903223647457182342124536002_0002_m_000000
[2024-09-03 22:36:47,890] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkHadoopMapRedUtil: attempt_20240903223647457182342124536002_0002_m_000000_2: Committed
[2024-09-03 22:36:47,891] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:36:47,892] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 118 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:36:47,892] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:36:47,894] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,166 s
[2024-09-03 22:36:47,894] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:36:47,895] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:36:47,896] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,171078 s
[2024-09-03 22:36:47,933] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileFormatWriter: Write Job d13f0292-f859-4c94-9b21-f86e3b6ba6c5 committed.
[2024-09-03 22:36:47,933] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO FileFormatWriter: Finished processing stats for write job d13f0292-f859-4c94-9b21-f86e3b6ba6c5.
[2024-09-03 22:36:47,982] {spark_submit.py:495} INFO - 24/09/03 22:36:47 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:36:48,001] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:36:48,014] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:36:48,026] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:36:48,026] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO BlockManager: BlockManager stopped
[2024-09-03 22:36:48,036] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:36:48,039] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:36:48,057] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:36:48,057] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:36:48,058] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-238e6f5a-615f-4766-9d0c-2104ab4ae66f
[2024-09-03 22:36:48,062] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-07dd803e-125d-4d28-8ee7-01fa971f2ef6/pyspark-1c25811c-a23b-4a33-b736-7b91e5420189
[2024-09-03 22:36:48,065] {spark_submit.py:495} INFO - 24/09/03 22:36:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-07dd803e-125d-4d28-8ee7-01fa971f2ef6
[2024-09-03 22:36:48,199] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T013639, end_date=20240904T013648
[2024-09-03 22:36:48,235] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:36:48,251] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:57:12,811] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:57:12,818] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 22:57:12,818] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:57:12,818] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:57:12,818] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:57:12,827] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 22:57:12,829] {standard_task_runner.py:52} INFO - Started process 456768 to run task
[2024-09-03 22:57:12,832] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '162', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_z4t4ufr', '--error-file', '/tmp/tmpqfa8xgo2']
[2024-09-03 22:57:12,833] {standard_task_runner.py:80} INFO - Job 162: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:57:12,864] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:57:12,904] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 22:57:12,907] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:57:12,908] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 22:57:13,968] {spark_submit.py:495} INFO - 24/09/03 22:57:13 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:57:13,969] {spark_submit.py:495} INFO - 24/09/03 22:57:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:57:14,268] {spark_submit.py:495} INFO - 24/09/03 22:57:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:57:14,723] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:57:14,731] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:57:14,763] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO ResourceUtils: ==============================================================
[2024-09-03 22:57:14,763] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:57:14,763] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO ResourceUtils: ==============================================================
[2024-09-03 22:57:14,764] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:57:14,781] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:57:14,793] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:57:14,793] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:57:14,827] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:57:14,828] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:57:14,828] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:57:14,828] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:57:14,828] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:57:14,963] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO Utils: Successfully started service 'sparkDriver' on port 37275.
[2024-09-03 22:57:14,983] {spark_submit.py:495} INFO - 24/09/03 22:57:14 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:57:15,005] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:57:15,019] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:57:15,019] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:57:15,022] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:57:15,032] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ab70aaa4-c971-4561-b066-012344984c6f
[2024-09-03 22:57:15,065] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:57:15,082] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:57:15,252] {spark_submit.py:495} INFO - 24/09/03 22:57:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:57:15,261] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:57:15,301] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:57:15,468] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:57:15,492] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39527.
[2024-09-03 22:57:15,492] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO NettyBlockTransferService: Server created on 192.168.2.128:39527
[2024-09-03 22:57:15,493] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:57:15,498] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 39527, None)
[2024-09-03 22:57:15,500] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:39527 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 39527, None)
[2024-09-03 22:57:15,502] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 39527, None)
[2024-09-03 22:57:15,503] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 39527, None)
[2024-09-03 22:57:15,902] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:57:15,902] {spark_submit.py:495} INFO - 24/09/03 22:57:15 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:57:16,554] {spark_submit.py:495} INFO - 24/09/03 22:57:16 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.
[2024-09-03 22:57:16,600] {spark_submit.py:495} INFO - 24/09/03 22:57:16 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:57:18,069] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:57:18,070] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:57:18,073] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:57:18,314] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:57:18,359] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:57:18,362] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:39527 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:57:18,366] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:18,374] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649927 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:57:18,497] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:18,516] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:57:18,516] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:57:18,517] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:57:18,517] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:57:18,528] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:57:18,600] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:57:18,602] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:57:18,603] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:39527 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:57:18,603] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:57:18,614] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:57:18,614] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:57:18,659] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:57:18,676] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:57:18,935] {spark_submit.py:495} INFO - 24/09/03 22:57:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455623, partition values: [empty row]
[2024-09-03 22:57:19,135] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO CodeGenerator: Code generated in 118.728254 ms
[2024-09-03 22:57:19,215] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:57:19,227] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 578 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:57:19,323] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:57:19,328] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,784 s
[2024-09-03 22:57:19,332] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:57:19,332] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:57:19,335] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,837027 s
[2024-09-03 22:57:19,681] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:57:19,682] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:57:19,682] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:57:19,740] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:19,740] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:19,740] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:19,828] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO CodeGenerator: Code generated in 27.364435 ms
[2024-09-03 22:57:19,865] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO CodeGenerator: Code generated in 23.112646 ms
[2024-09-03 22:57:19,870] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:57:19,878] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:57:19,879] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:39527 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:57:19,880] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:19,882] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649927 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:57:19,937] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:19,938] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:57:19,939] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:57:19,939] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:57:19,939] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:57:19,940] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:57:19,977] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:57:19,979] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:57:19,979] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:39527 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:57:19,979] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:57:19,980] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:57:19,980] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:57:19,983] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:57:19,984] {spark_submit.py:495} INFO - 24/09/03 22:57:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:57:20,026] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:20,026] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:20,027] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:20,093] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO CodeGenerator: Code generated in 28.143482 ms
[2024-09-03 22:57:20,097] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455623, partition values: [empty row]
[2024-09-03 22:57:20,120] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO CodeGenerator: Code generated in 19.997489 ms
[2024-09-03 22:57:20,143] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO CodeGenerator: Code generated in 5.817954 ms
[2024-09-03 22:57:20,243] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: Saved output of task 'attempt_202409032257191983811192553286755_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032257191983811192553286755_0001_m_000000
[2024-09-03 22:57:20,244] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkHadoopMapRedUtil: attempt_202409032257191983811192553286755_0001_m_000000_1: Committed
[2024-09-03 22:57:20,249] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:57:20,254] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 273 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:57:20,254] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:57:20,255] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,314 s
[2024-09-03 22:57:20,255] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:57:20,256] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:57:20,258] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,318763 s
[2024-09-03 22:57:20,275] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileFormatWriter: Write Job 7691bb62-098e-4aa7-ac64-9dc663ffeb47 committed.
[2024-09-03 22:57:20,276] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileFormatWriter: Finished processing stats for write job 7691bb62-098e-4aa7-ac64-9dc663ffeb47.
[2024-09-03 22:57:20,328] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:57:20,329] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:57:20,329] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:57:20,339] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:20,340] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:20,340] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:20,368] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO CodeGenerator: Code generated in 11.943814 ms
[2024-09-03 22:57:20,372] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:57:20,379] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:57:20,380] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:39527 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:57:20,381] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:20,382] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649927 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:57:20,398] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:57:20,399] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:57:20,400] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:57:20,400] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:57:20,400] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:57:20,401] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:57:20,416] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:57:20,418] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:57:20,418] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:39527 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:57:20,419] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:57:20,420] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:57:20,420] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:57:20,422] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:57:20,425] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:57:20,434] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:57:20,435] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:57:20,435] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:57:20,468] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO CodeGenerator: Code generated in 7.541063 ms
[2024-09-03 22:57:20,472] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455623, partition values: [empty row]
[2024-09-03 22:57:20,485] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO CodeGenerator: Code generated in 10.774857 ms
[2024-09-03 22:57:20,513] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileOutputCommitter: Saved output of task 'attempt_202409032257203005129212951962490_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032257203005129212951962490_0002_m_000000
[2024-09-03 22:57:20,514] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkHadoopMapRedUtil: attempt_202409032257203005129212951962490_0002_m_000000_2: Committed
[2024-09-03 22:57:20,516] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:57:20,518] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 97 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:57:20,519] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,118 s
[2024-09-03 22:57:20,519] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:57:20,524] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:57:20,524] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:57:20,524] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,122905 s
[2024-09-03 22:57:20,531] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileFormatWriter: Write Job 15b3c6d4-f738-4f05-a4ca-232b660b606f committed.
[2024-09-03 22:57:20,532] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO FileFormatWriter: Finished processing stats for write job 15b3c6d4-f738-4f05-a4ca-232b660b606f.
[2024-09-03 22:57:20,565] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:57:20,572] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:57:20,583] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:57:20,591] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:57:20,591] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO BlockManager: BlockManager stopped
[2024-09-03 22:57:20,598] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:57:20,601] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:57:20,605] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:57:20,606] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:57:20,607] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-e963937c-2412-44f8-86d2-ce5739fa8a74/pyspark-f6ba547e-4ef6-4fd1-8ee9-0764e10c3cd3
[2024-09-03 22:57:20,609] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-e963937c-2412-44f8-86d2-ce5739fa8a74
[2024-09-03 22:57:20,612] {spark_submit.py:495} INFO - 24/09/03 22:57:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0bf780a-9de1-47d0-b244-15bb42bde6c6
[2024-09-03 22:57:20,727] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T015712, end_date=20240904T015720
[2024-09-03 22:57:20,769] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:57:20,786] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 23:35:44,854] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 23:35:44,861] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [queued]>
[2024-09-03 23:35:44,861] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:35:44,861] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 23:35:44,861] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 23:35:44,870] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-09-02 00:00:00+00:00
[2024-09-03 23:35:44,872] {standard_task_runner.py:52} INFO - Started process 475562 to run task
[2024-09-03 23:35:44,875] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-09-02T00:00:00+00:00', '--job-id', '86', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpznbsno70', '--error-file', '/tmp/tmp208092q7']
[2024-09-03 23:35:44,876] {standard_task_runner.py:80} INFO - Job 86: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 23:35:44,910] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-09-02T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 23:35:44,952] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-09-02T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-09-02T00:00:00+00:00
[2024-09-03 23:35:44,956] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 23:35:44,957] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240902
[2024-09-03 23:35:46,074] {spark_submit.py:495} INFO - 24/09/03 23:35:46 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 23:35:46,074] {spark_submit.py:495} INFO - 24/09/03 23:35:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 23:35:46,433] {spark_submit.py:495} INFO - 24/09/03 23:35:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 23:35:46,976] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 23:35:46,985] {spark_submit.py:495} INFO - 24/09/03 23:35:46 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 23:35:47,021] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO ResourceUtils: ==============================================================
[2024-09-03 23:35:47,022] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 23:35:47,022] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO ResourceUtils: ==============================================================
[2024-09-03 23:35:47,022] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 23:35:47,042] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 23:35:47,053] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 23:35:47,054] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 23:35:47,094] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 23:35:47,094] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 23:35:47,094] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 23:35:47,095] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 23:35:47,095] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 23:35:47,262] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO Utils: Successfully started service 'sparkDriver' on port 43125.
[2024-09-03 23:35:47,288] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 23:35:47,313] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 23:35:47,330] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 23:35:47,331] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 23:35:47,334] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 23:35:47,345] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1f82e704-5700-4c55-bceb-de7088498fe6
[2024-09-03 23:35:47,366] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 23:35:47,383] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 23:35:47,586] {spark_submit.py:495} INFO - 24/09/03 23:35:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 23:35:47,593] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 23:35:47,648] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 23:35:47,854] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 23:35:47,878] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34797.
[2024-09-03 23:35:47,878] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO NettyBlockTransferService: Server created on 192.168.2.128:34797
[2024-09-03 23:35:47,880] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 23:35:47,885] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34797, None)
[2024-09-03 23:35:47,888] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34797 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34797, None)
[2024-09-03 23:35:47,890] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34797, None)
[2024-09-03 23:35:47,892] {spark_submit.py:495} INFO - 24/09/03 23:35:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34797, None)
[2024-09-03 23:35:48,342] {spark_submit.py:495} INFO - 24/09/03 23:35:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 23:35:48,342] {spark_submit.py:495} INFO - 24/09/03 23:35:48 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 23:35:49,234] {spark_submit.py:495} INFO - 24/09/03 23:35:49 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.
[2024-09-03 23:35:49,290] {spark_submit.py:495} INFO - 24/09/03 23:35:49 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 23:35:51,067] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:35:51,068] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:35:51,071] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 23:35:51,336] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 23:35:51,384] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 23:35:51,387] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34797 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 23:35:51,392] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:35:51,401] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649363 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:35:51,562] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:35:51,581] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:35:51,583] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:35:51,584] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:35:51,584] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:35:51,593] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:35:51,701] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 23:35:51,704] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 23:35:51,705] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34797 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 23:35:51,706] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:35:51,717] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:35:51,718] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 23:35:51,802] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 23:35:51,824] {spark_submit.py:495} INFO - 24/09/03 23:35:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 23:35:52,103] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455059, partition values: [empty row]
[2024-09-03 23:35:52,336] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO CodeGenerator: Code generated in 141.674074 ms
[2024-09-03 23:35:52,419] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 23:35:52,523] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 641 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:35:52,532] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,923 s
[2024-09-03 23:35:52,533] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 23:35:52,537] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:35:52,538] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 23:35:52,542] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,978127 s
[2024-09-03 23:35:52,945] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 23:35:52,947] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 23:35:52,947] {spark_submit.py:495} INFO - 24/09/03 23:35:52 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 23:35:53,012] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:35:53,012] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:35:53,014] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:35:53,094] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 26.466767 ms
[2024-09-03 23:35:53,137] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 26.492716 ms
[2024-09-03 23:35:53,144] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 23:35:53,151] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 23:35:53,153] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34797 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 23:35:53,155] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:35:53,157] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649363 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:35:53,209] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:35:53,211] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:35:53,211] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:35:53,211] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:35:53,211] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:35:53,214] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:35:53,272] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 23:35:53,274] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 23:35:53,275] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34797 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 23:35:53,275] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:35:53,276] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:35:53,276] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 23:35:53,280] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:35:53,281] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 23:35:53,342] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:35:53,342] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:35:53,343] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:35:53,403] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 23.830705 ms
[2024-09-03 23:35:53,407] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455059, partition values: [empty row]
[2024-09-03 23:35:53,436] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 25.05946 ms
[2024-09-03 23:35:53,460] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 4.737257 ms
[2024-09-03 23:35:53,570] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: Saved output of task 'attempt_202409032335535460346956815031510_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240902/_temporary/0/task_202409032335535460346956815031510_0001_m_000000
[2024-09-03 23:35:53,570] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkHadoopMapRedUtil: attempt_202409032335535460346956815031510_0001_m_000000_1: Committed
[2024-09-03 23:35:53,574] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 23:35:53,576] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 299 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:35:53,577] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,364 s
[2024-09-03 23:35:53,578] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:35:53,578] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 23:35:53,578] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 23:35:53,580] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,370598 s
[2024-09-03 23:35:53,606] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileFormatWriter: Write Job 01f88ae3-ac17-428c-9ac0-59587b5c5355 committed.
[2024-09-03 23:35:53,610] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileFormatWriter: Finished processing stats for write job 01f88ae3-ac17-428c-9ac0-59587b5c5355.
[2024-09-03 23:35:53,648] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 23:35:53,649] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 23:35:53,650] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 23:35:53,661] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:35:53,661] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:35:53,662] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:35:53,694] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 14.085539 ms
[2024-09-03 23:35:53,698] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 23:35:53,708] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 23:35:53,709] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34797 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 23:35:53,709] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:35:53,710] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649363 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 23:35:53,729] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 23:35:53,730] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 23:35:53,731] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 23:35:53,731] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 23:35:53,731] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 23:35:53,732] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 23:35:53,752] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 23:35:53,754] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 23:35:53,755] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34797 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 23:35:53,755] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 23:35:53,756] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 23:35:53,757] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 23:35:53,758] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 23:35:53,758] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 23:35:53,772] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 23:35:53,772] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 23:35:53,772] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 23:35:53,814] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 9.487659 ms
[2024-09-03 23:35:53,818] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-09-02T00:00:00.00Z/data_engineer_20240902.json, range: 0-455059, partition values: [empty row]
[2024-09-03 23:35:53,831] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO CodeGenerator: Code generated in 10.50636 ms
[2024-09-03 23:35:53,868] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileOutputCommitter: Saved output of task 'attempt_202409032335538465679131432091017_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240902/_temporary/0/task_202409032335538465679131432091017_0002_m_000000
[2024-09-03 23:35:53,868] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkHadoopMapRedUtil: attempt_202409032335538465679131432091017_0002_m_000000_2: Committed
[2024-09-03 23:35:53,869] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 23:35:53,871] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 114 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 23:35:53,872] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,140 s
[2024-09-03 23:35:53,873] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 23:35:53,873] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 23:35:53,873] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 23:35:53,873] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,143316 s
[2024-09-03 23:35:53,894] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileFormatWriter: Write Job 57d74760-f8ec-4556-903f-c058bde6c1e8 committed.
[2024-09-03 23:35:53,895] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO FileFormatWriter: Finished processing stats for write job 57d74760-f8ec-4556-903f-c058bde6c1e8.
[2024-09-03 23:35:53,937] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 23:35:53,945] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 23:35:53,959] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 23:35:53,972] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO MemoryStore: MemoryStore cleared
[2024-09-03 23:35:53,973] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO BlockManager: BlockManager stopped
[2024-09-03 23:35:53,983] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 23:35:53,987] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 23:35:53,994] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 23:35:53,995] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 23:35:53,996] {spark_submit.py:495} INFO - 24/09/03 23:35:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-bbf29075-218a-45e4-b54c-4b338c876cd6
[2024-09-03 23:35:54,009] {spark_submit.py:495} INFO - 24/09/03 23:35:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-902228ad-4cc9-487d-82dc-37642013f32a
[2024-09-03 23:35:54,012] {spark_submit.py:495} INFO - 24/09/03 23:35:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-902228ad-4cc9-487d-82dc-37642013f32a/pyspark-228008f8-35b3-49c8-8f37-9a7654605310
[2024-09-03 23:35:54,098] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240902T000000, start_date=20240904T023544, end_date=20240904T023554
[2024-09-03 23:35:54,135] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 23:35:54,149] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
