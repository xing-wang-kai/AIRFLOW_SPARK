[2024-09-03 16:23:37,154] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 16:23:37,161] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 16:23:37,161] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:23:37,162] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:23:37,162] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:23:37,170] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-27 00:00:00+00:00
[2024-09-03 16:23:37,172] {standard_task_runner.py:52} INFO - Started process 284956 to run task
[2024-09-03 16:23:37,175] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-27T00:00:00+00:00', '--job-id', '91', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmph1muh_3i', '--error-file', '/tmp/tmpmi_9c4rm']
[2024-09-03 16:23:37,175] {standard_task_runner.py:80} INFO - Job 91: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:23:37,207] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:23:37,247] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-27T00:00:00+00:00
[2024-09-03 16:23:37,251] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:23:37,252] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827
[2024-09-03 16:23:38,356] {spark_submit.py:495} INFO - 24/09/03 16:23:38 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:23:38,356] {spark_submit.py:495} INFO - 24/09/03 16:23:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:23:38,689] {spark_submit.py:495} INFO - 24/09/03 16:23:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:23:39,179] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:23:39,186] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:23:39,219] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO ResourceUtils: ==============================================================
[2024-09-03 16:23:39,219] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:23:39,220] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO ResourceUtils: ==============================================================
[2024-09-03 16:23:39,220] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:23:39,237] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:23:39,248] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:23:39,249] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:23:39,286] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:23:39,286] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:23:39,287] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:23:39,287] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:23:39,287] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:23:39,427] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO Utils: Successfully started service 'sparkDriver' on port 39159.
[2024-09-03 16:23:39,449] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:23:39,478] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:23:39,493] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:23:39,493] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:23:39,497] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:23:39,508] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3bf39f93-c0d4-4b34-9fc9-8aa1fb6170cd
[2024-09-03 16:23:39,528] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:23:39,556] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:23:39,731] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:23:39,771] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:23:39,961] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:23:39,984] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40501.
[2024-09-03 16:23:39,984] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO NettyBlockTransferService: Server created on 192.168.2.128:40501
[2024-09-03 16:23:39,986] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:23:39,990] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40501, None)
[2024-09-03 16:23:39,993] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40501 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40501, None)
[2024-09-03 16:23:39,995] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40501, None)
[2024-09-03 16:23:39,996] {spark_submit.py:495} INFO - 24/09/03 16:23:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40501, None)
[2024-09-03 16:23:40,381] {spark_submit.py:495} INFO - 24/09/03 16:23:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:23:40,381] {spark_submit.py:495} INFO - 24/09/03 16:23:40 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:23:41,017] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 76, in <module>
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - define_extration(spark,
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/src/notebooks/transforms_func.py", line 48, in define_extration
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - df = spark.read.json(src)
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 372, in json
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2024-09-03 16:23:41,020] {spark_submit.py:495} INFO - File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-3.1.3-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2024-09-03 16:23:41,025] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z
[2024-09-03 16:23:41,050] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:23:41,058] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:23:41,068] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:23:41,076] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:23:41,076] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO BlockManager: BlockManager stopped
[2024-09-03 16:23:41,081] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:23:41,084] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:23:41,088] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:23:41,088] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:23:41,088] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3c1e338-0314-4560-a22c-ea9692006582
[2024-09-03 16:23:41,090] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2614919-4e22-48df-a34f-cef1c27f2964
[2024-09-03 16:23:41,091] {spark_submit.py:495} INFO - 24/09/03 16:23:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2614919-4e22-48df-a34f-cef1c27f2964/pyspark-7b07780a-d867-4655-a691-03a95b87ef4b
[2024-09-03 16:23:41,124] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/kai/Documents/PROJETOS/twitter_extractor_airflow/venv/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827. Error code is: 1.
[2024-09-03 16:23:41,126] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240827T000000, start_date=20240903T192337, end_date=20240903T192341
[2024-09-03 16:23:41,132] {standard_task_runner.py:92} ERROR - Failed to execute job 91 for task SILVER_LAYER_LOAD_L (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827. Error code is: 1.; 284956)
[2024-09-03 16:23:41,179] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-09-03 16:23:41,197] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:41:56,729] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 16:41:56,737] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 16:41:56,737] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:41:56,737] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:41:56,737] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:41:56,746] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-27 00:00:00+00:00
[2024-09-03 16:41:56,749] {standard_task_runner.py:52} INFO - Started process 295847 to run task
[2024-09-03 16:41:56,754] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-27T00:00:00+00:00', '--job-id', '91', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnqvikk7i', '--error-file', '/tmp/tmpg0id30kc']
[2024-09-03 16:41:56,754] {standard_task_runner.py:80} INFO - Job 91: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:41:56,790] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:41:56,835] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-27T00:00:00+00:00
[2024-09-03 16:41:56,842] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:41:56,844] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827
[2024-09-03 16:41:57,818] {spark_submit.py:495} INFO - 24/09/03 16:41:57 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:41:57,819] {spark_submit.py:495} INFO - 24/09/03 16:41:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:41:58,110] {spark_submit.py:495} INFO - 24/09/03 16:41:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:41:58,664] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:41:58,674] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:41:58,708] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO ResourceUtils: ==============================================================
[2024-09-03 16:41:58,709] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:41:58,710] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO ResourceUtils: ==============================================================
[2024-09-03 16:41:58,710] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:41:58,728] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:41:58,741] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:41:58,742] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:41:58,786] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:41:58,786] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:41:58,786] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:41:58,786] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:41:58,787] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:41:58,930] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO Utils: Successfully started service 'sparkDriver' on port 44329.
[2024-09-03 16:41:58,951] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:41:58,975] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:41:58,990] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:41:58,991] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:41:58,996] {spark_submit.py:495} INFO - 24/09/03 16:41:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:41:59,008] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-41578d00-cb95-425a-b9af-b90994d322f3
[2024-09-03 16:41:59,027] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:41:59,039] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:41:59,203] {spark_submit.py:495} INFO - 24/09/03 16:41:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:41:59,213] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:41:59,273] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:41:59,453] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:41:59,474] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36001.
[2024-09-03 16:41:59,474] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO NettyBlockTransferService: Server created on 192.168.2.128:36001
[2024-09-03 16:41:59,476] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:41:59,481] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36001, None)
[2024-09-03 16:41:59,484] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36001 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36001, None)
[2024-09-03 16:41:59,486] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36001, None)
[2024-09-03 16:41:59,487] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36001, None)
[2024-09-03 16:41:59,871] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:41:59,871] {spark_submit.py:495} INFO - 24/09/03 16:41:59 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:42:00,620] {spark_submit.py:495} INFO - 24/09/03 16:42:00 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 16:42:00,671] {spark_submit.py:495} INFO - 24/09/03 16:42:00 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:42:02,286] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:42:02,286] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:42:02,297] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:42:02,529] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:42:02,585] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:42:02,587] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36001 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:42:02,592] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:02,601] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649940 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:42:02,733] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:02,748] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:42:02,748] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:42:02,749] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:42:02,752] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:42:02,758] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:42:02,834] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:42:02,845] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:42:02,845] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36001 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:42:02,845] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:42:02,852] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:42:02,852] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:42:02,901] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:42:02,921] {spark_submit.py:495} INFO - 24/09/03 16:42:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:42:03,181] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-455636, partition values: [empty row]
[2024-09-03 16:42:03,395] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO CodeGenerator: Code generated in 129.113922 ms
[2024-09-03 16:42:03,492] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 16:42:03,504] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 615 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:42:03,605] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:42:03,612] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,839 s
[2024-09-03 16:42:03,615] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:42:03,617] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:42:03,622] {spark_submit.py:495} INFO - 24/09/03 16:42:03 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,887708 s
[2024-09-03 16:42:04,005] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:42:04,005] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:42:04,006] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:42:04,069] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:04,069] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:04,071] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:04,155] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 39.135811 ms
[2024-09-03 16:42:04,192] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 21.047487 ms
[2024-09-03 16:42:04,199] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:42:04,218] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:42:04,223] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36001 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:42:04,225] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:04,228] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649940 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:42:04,297] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:04,298] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:42:04,298] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:42:04,298] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:42:04,298] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:42:04,306] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:42:04,363] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:42:04,365] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:42:04,365] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36001 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:42:04,366] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:42:04,367] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:42:04,367] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:42:04,371] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:42:04,371] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:42:04,431] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:04,431] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:04,432] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:04,492] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 23.423044 ms
[2024-09-03 16:42:04,497] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-455636, partition values: [empty row]
[2024-09-03 16:42:04,521] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 21.744225 ms
[2024-09-03 16:42:04,543] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 5.508683 ms
[2024-09-03 16:42:04,650] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: Saved output of task 'attempt_202409031642047059348921002573022_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240827/_temporary/0/task_202409031642047059348921002573022_0001_m_000000
[2024-09-03 16:42:04,651] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkHadoopMapRedUtil: attempt_202409031642047059348921002573022_0001_m_000000_1: Committed
[2024-09-03 16:42:04,655] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:42:04,660] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 292 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:42:04,662] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,358 s
[2024-09-03 16:42:04,662] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:42:04,665] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:42:04,669] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:42:04,669] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,370141 s
[2024-09-03 16:42:04,683] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileFormatWriter: Write Job 7ad2dcf9-b486-487e-b113-efd2fe4f9117 committed.
[2024-09-03 16:42:04,688] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileFormatWriter: Finished processing stats for write job 7ad2dcf9-b486-487e-b113-efd2fe4f9117.
[2024-09-03 16:42:04,726] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:42:04,727] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:42:04,727] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:42:04,737] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:04,738] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:04,738] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:04,765] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 9.454424 ms
[2024-09-03 16:42:04,768] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:42:04,775] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:42:04,775] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36001 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:42:04,776] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:04,777] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649940 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:42:04,795] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:42:04,795] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:42:04,795] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:42:04,795] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:42:04,795] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:42:04,797] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:42:04,813] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:42:04,816] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:42:04,817] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36001 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:42:04,818] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:42:04,819] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:42:04,819] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:42:04,820] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:42:04,821] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:42:04,835] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:42:04,835] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:42:04,836] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:42:04,865] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 8.041823 ms
[2024-09-03 16:42:04,868] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-455636, partition values: [empty row]
[2024-09-03 16:42:04,882] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO CodeGenerator: Code generated in 11.537592 ms
[2024-09-03 16:42:04,911] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileOutputCommitter: Saved output of task 'attempt_202409031642043812651559930546814_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240827/_temporary/0/task_202409031642043812651559930546814_0002_m_000000
[2024-09-03 16:42:04,911] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkHadoopMapRedUtil: attempt_202409031642043812651559930546814_0002_m_000000_2: Committed
[2024-09-03 16:42:04,912] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:42:04,915] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 94 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:42:04,916] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,118 s
[2024-09-03 16:42:04,917] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:42:04,918] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:42:04,918] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:42:04,918] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,124913 s
[2024-09-03 16:42:04,938] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileFormatWriter: Write Job 3bde2926-4707-4674-9e32-bfcf04f1a7f4 committed.
[2024-09-03 16:42:04,939] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO FileFormatWriter: Finished processing stats for write job 3bde2926-4707-4674-9e32-bfcf04f1a7f4.
[2024-09-03 16:42:04,982] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:42:04,991] {spark_submit.py:495} INFO - 24/09/03 16:42:04 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:42:05,002] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:42:05,012] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:42:05,013] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO BlockManager: BlockManager stopped
[2024-09-03 16:42:05,021] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:42:05,024] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:42:05,028] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:42:05,029] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:42:05,029] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-75588416-3a97-4586-873e-10f90ef79d7a
[2024-09-03 16:42:05,032] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-75588416-3a97-4586-873e-10f90ef79d7a/pyspark-d5dca138-9de5-42ff-b1b3-762e613ef289
[2024-09-03 16:42:05,035] {spark_submit.py:495} INFO - 24/09/03 16:42:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7ed478b-6e74-4c61-adce-73f59e75c4d6
[2024-09-03 16:42:05,100] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240827T000000, start_date=20240903T194156, end_date=20240903T194205
[2024-09-03 16:42:05,146] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:42:05,154] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:55:51,347] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 17:55:51,353] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 17:55:51,353] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:55:51,353] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:55:51,353] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:55:51,363] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-27 00:00:00+00:00
[2024-09-03 17:55:51,366] {standard_task_runner.py:52} INFO - Started process 326207 to run task
[2024-09-03 17:55:51,369] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-27T00:00:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpv8xg0e54', '--error-file', '/tmp/tmpegvdji7p']
[2024-09-03 17:55:51,370] {standard_task_runner.py:80} INFO - Job 99: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:55:51,401] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:55:51,442] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-27T00:00:00+00:00
[2024-09-03 17:55:51,445] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:55:51,446] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827
[2024-09-03 17:55:52,500] {spark_submit.py:495} INFO - 24/09/03 17:55:52 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:55:52,501] {spark_submit.py:495} INFO - 24/09/03 17:55:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:55:52,806] {spark_submit.py:495} INFO - 24/09/03 17:55:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:55:53,293] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:55:53,301] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:55:53,335] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO ResourceUtils: ==============================================================
[2024-09-03 17:55:53,336] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:55:53,336] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO ResourceUtils: ==============================================================
[2024-09-03 17:55:53,337] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:55:53,353] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:55:53,365] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:55:53,366] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:55:53,403] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:55:53,404] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:55:53,405] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:55:53,406] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:55:53,406] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:55:53,560] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO Utils: Successfully started service 'sparkDriver' on port 36459.
[2024-09-03 17:55:53,583] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:55:53,608] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:55:53,625] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:55:53,626] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:55:53,630] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:55:53,640] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-050bab9d-23b5-4db6-8143-2ad06a0d2ba6
[2024-09-03 17:55:53,660] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:55:53,676] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:55:53,877] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:55:53,923] {spark_submit.py:495} INFO - 24/09/03 17:55:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:55:54,098] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:55:54,120] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46139.
[2024-09-03 17:55:54,120] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO NettyBlockTransferService: Server created on 192.168.2.128:46139
[2024-09-03 17:55:54,122] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:55:54,126] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46139, None)
[2024-09-03 17:55:54,131] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46139 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46139, None)
[2024-09-03 17:55:54,134] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46139, None)
[2024-09-03 17:55:54,135] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46139, None)
[2024-09-03 17:55:54,615] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:55:54,616] {spark_submit.py:495} INFO - 24/09/03 17:55:54 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:55:55,366] {spark_submit.py:495} INFO - 24/09/03 17:55:55 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 17:55:55,423] {spark_submit.py:495} INFO - 24/09/03 17:55:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:55:57,037] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:55:57,038] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:55:57,042] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:55:57,279] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:55:57,321] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:55:57,323] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46139 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:55:57,328] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:57,336] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650632 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:57,476] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:57,494] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:57,494] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:57,494] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:57,497] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:57,512] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:57,604] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:55:57,641] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:55:57,645] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46139 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:55:57,648] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:57,665] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:57,666] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:55:57,737] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:57,756] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:55:57,979] {spark_submit.py:495} INFO - 24/09/03 17:55:57 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-456328, partition values: [empty row]
[2024-09-03 17:55:58,215] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO CodeGenerator: Code generated in 132.359143 ms
[2024-09-03 17:55:58,297] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 17:55:58,421] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 583 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:58,424] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:55:58,432] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,897 s
[2024-09-03 17:55:58,434] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:58,435] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:55:58,438] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,960902 s
[2024-09-03 17:55:58,823] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:55:58,824] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:55:58,825] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:55:58,891] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:58,892] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:58,892] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:58,967] {spark_submit.py:495} INFO - 24/09/03 17:55:58 INFO CodeGenerator: Code generated in 29.181682 ms
[2024-09-03 17:55:59,020] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 36.327385 ms
[2024-09-03 17:55:59,025] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:55:59,035] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:55:59,035] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46139 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:55:59,036] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:59,040] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650632 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:59,095] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:59,097] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:59,097] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:59,098] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:59,098] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:59,099] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:59,136] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:55:59,138] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:55:59,138] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46139 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:55:59,139] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:59,139] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:59,139] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:55:59,142] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:59,143] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:55:59,187] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:59,187] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:59,188] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:59,240] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 16.37464 ms
[2024-09-03 17:55:59,244] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-456328, partition values: [empty row]
[2024-09-03 17:55:59,268] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 19.285877 ms
[2024-09-03 17:55:59,292] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 5.650548 ms
[2024-09-03 17:55:59,410] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_20240903175559457659101926409621_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240827/_temporary/0/task_20240903175559457659101926409621_0001_m_000000
[2024-09-03 17:55:59,412] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkHadoopMapRedUtil: attempt_20240903175559457659101926409621_0001_m_000000_1: Committed
[2024-09-03 17:55:59,418] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:55:59,424] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 283 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:59,424] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,324 s
[2024-09-03 17:55:59,425] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:59,434] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:55:59,435] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:55:59,437] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,341480 s
[2024-09-03 17:55:59,457] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileFormatWriter: Write Job dc7aa89f-5b2d-45a0-9fc3-c3cf98f0027c committed.
[2024-09-03 17:55:59,462] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileFormatWriter: Finished processing stats for write job dc7aa89f-5b2d-45a0-9fc3-c3cf98f0027c.
[2024-09-03 17:55:59,513] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:55:59,515] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:55:59,516] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:55:59,550] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:59,551] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:59,551] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:59,582] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 11.833704 ms
[2024-09-03 17:55:59,587] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:55:59,594] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:55:59,598] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46139 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:55:59,599] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:59,599] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650632 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:59,617] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:59,618] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:59,618] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:59,618] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:59,619] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:59,619] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:59,644] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:55:59,648] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:55:59,648] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46139 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:55:59,649] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:59,649] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:59,649] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:55:59,650] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:59,650] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:55:59,661] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:59,662] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:59,662] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:59,713] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 11.851489 ms
[2024-09-03 17:55:59,716] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-456328, partition values: [empty row]
[2024-09-03 17:55:59,731] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO CodeGenerator: Code generated in 12.268346 ms
[2024-09-03 17:55:59,759] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_202409031755594092965630390704233_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240827/_temporary/0/task_202409031755594092965630390704233_0002_m_000000
[2024-09-03 17:55:59,759] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkHadoopMapRedUtil: attempt_202409031755594092965630390704233_0002_m_000000_2: Committed
[2024-09-03 17:55:59,760] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:55:59,762] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 112 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:59,763] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:55:59,763] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,143 s
[2024-09-03 17:55:59,764] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:59,764] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:55:59,764] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,147299 s
[2024-09-03 17:55:59,797] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileFormatWriter: Write Job d5d48b90-957a-4611-9da2-f460258c0aa0 committed.
[2024-09-03 17:55:59,797] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO FileFormatWriter: Finished processing stats for write job d5d48b90-957a-4611-9da2-f460258c0aa0.
[2024-09-03 17:55:59,824] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:55:59,832] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:55:59,848] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:55:59,862] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:55:59,863] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO BlockManager: BlockManager stopped
[2024-09-03 17:55:59,869] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:55:59,877] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:55:59,882] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:55:59,882] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:55:59,883] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-99f505cf-61f6-47e2-af2c-91e488cc3a6c/pyspark-bc9e3ae7-ce15-4ef9-9254-632f97803df0
[2024-09-03 17:55:59,885] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-8268c62b-9b46-4c5f-b7aa-6048ae7d3d5d
[2024-09-03 17:55:59,887] {spark_submit.py:495} INFO - 24/09/03 17:55:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-99f505cf-61f6-47e2-af2c-91e488cc3a6c
[2024-09-03 17:55:59,988] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240827T000000, start_date=20240903T205551, end_date=20240903T205559
[2024-09-03 17:56:00,014] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:56:00,056] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:16:41,056] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 18:16:41,063] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 18:16:41,064] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:16:41,064] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:16:41,064] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:16:41,079] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-27 00:00:00+00:00
[2024-09-03 18:16:41,081] {standard_task_runner.py:52} INFO - Started process 338529 to run task
[2024-09-03 18:16:41,087] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-27T00:00:00+00:00', '--job-id', '98', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpip181g0w', '--error-file', '/tmp/tmpk1lauky5']
[2024-09-03 18:16:41,088] {standard_task_runner.py:80} INFO - Job 98: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:16:41,133] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:16:41,184] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-27T00:00:00+00:00
[2024-09-03 18:16:41,190] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:16:41,191] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827
[2024-09-03 18:16:42,273] {spark_submit.py:495} INFO - 24/09/03 18:16:42 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:16:42,273] {spark_submit.py:495} INFO - 24/09/03 18:16:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:16:42,642] {spark_submit.py:495} INFO - 24/09/03 18:16:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:16:43,282] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:16:43,302] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:16:43,362] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO ResourceUtils: ==============================================================
[2024-09-03 18:16:43,363] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:16:43,363] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO ResourceUtils: ==============================================================
[2024-09-03 18:16:43,364] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:16:43,396] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:16:43,413] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:16:43,414] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:16:43,460] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:16:43,460] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:16:43,461] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:16:43,461] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:16:43,461] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:16:43,651] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO Utils: Successfully started service 'sparkDriver' on port 41995.
[2024-09-03 18:16:43,679] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:16:43,713] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:16:43,743] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:16:43,743] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:16:43,746] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:16:43,758] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-218c6084-06fe-42fb-a9cc-545948a9ac6b
[2024-09-03 18:16:43,782] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:16:43,803] {spark_submit.py:495} INFO - 24/09/03 18:16:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:16:44,052] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:16:44,120] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:16:44,345] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:16:44,373] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38383.
[2024-09-03 18:16:44,373] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO NettyBlockTransferService: Server created on 192.168.2.128:38383
[2024-09-03 18:16:44,374] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:16:44,381] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38383, None)
[2024-09-03 18:16:44,385] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38383 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38383, None)
[2024-09-03 18:16:44,387] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38383, None)
[2024-09-03 18:16:44,388] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38383, None)
[2024-09-03 18:16:44,978] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:16:44,979] {spark_submit.py:495} INFO - 24/09/03 18:16:44 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:16:45,756] {spark_submit.py:495} INFO - 24/09/03 18:16:45 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 18:16:45,815] {spark_submit.py:495} INFO - 24/09/03 18:16:45 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:16:47,806] {spark_submit.py:495} INFO - 24/09/03 18:16:47 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:16:47,807] {spark_submit.py:495} INFO - 24/09/03 18:16:47 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:16:47,810] {spark_submit.py:495} INFO - 24/09/03 18:16:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:16:48,112] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:16:48,158] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:16:48,160] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38383 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:16:48,165] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:48,174] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198856 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:48,336] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:48,358] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:48,358] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:48,359] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:48,361] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:48,369] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:48,459] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:16:48,462] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:16:48,465] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38383 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:16:48,469] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:48,482] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:48,483] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:16:48,559] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:48,585] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:16:48,921] {spark_submit.py:495} INFO - 24/09/03 18:16:48 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-4552, partition values: [empty row]
[2024-09-03 18:16:49,236] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO CodeGenerator: Code generated in 188.432615 ms
[2024-09-03 18:16:49,318] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:16:49,343] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 812 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:49,346] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:16:49,509] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,123 s
[2024-09-03 18:16:49,512] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:49,514] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:16:49,529] {spark_submit.py:495} INFO - 24/09/03 18:16:49 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,192595 s
[2024-09-03 18:16:50,031] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:16:50,034] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:16:50,034] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:16:50,133] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:50,133] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:50,134] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:50,233] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO CodeGenerator: Code generated in 29.842049 ms
[2024-09-03 18:16:50,280] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO CodeGenerator: Code generated in 26.403026 ms
[2024-09-03 18:16:50,281] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:16:50,290] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:16:50,291] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38383 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:16:50,292] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:50,294] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198856 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:50,380] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:50,388] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:50,388] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:50,388] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:50,391] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:50,413] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:50,504] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:16:50,508] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:16:50,510] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38383 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:16:50,511] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:50,512] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:50,513] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:16:50,519] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:50,520] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:16:50,580] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:50,581] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:50,583] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:50,658] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO CodeGenerator: Code generated in 22.27141 ms
[2024-09-03 18:16:50,660] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-4552, partition values: [empty row]
[2024-09-03 18:16:50,683] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO CodeGenerator: Code generated in 19.95172 ms
[2024-09-03 18:16:50,708] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO CodeGenerator: Code generated in 6.289499 ms
[2024-09-03 18:16:50,753] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: Saved output of task 'attempt_202409031816506856448593623669495_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240827/_temporary/0/task_202409031816506856448593623669495_0001_m_000000
[2024-09-03 18:16:50,753] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SparkHadoopMapRedUtil: attempt_202409031816506856448593623669495_0001_m_000000_1: Committed
[2024-09-03 18:16:50,758] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:16:50,763] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 246 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:50,764] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:16:50,764] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,353 s
[2024-09-03 18:16:50,765] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:50,765] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:16:50,766] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,381904 s
[2024-09-03 18:16:50,802] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileFormatWriter: Write Job 5aee9ff7-f70d-4a05-bcf1-86fa27781ea3 committed.
[2024-09-03 18:16:50,810] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileFormatWriter: Finished processing stats for write job 5aee9ff7-f70d-4a05-bcf1-86fa27781ea3.
[2024-09-03 18:16:50,856] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:16:50,856] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:16:50,858] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:16:50,870] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:50,871] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:50,872] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:50,923] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO CodeGenerator: Code generated in 13.932558 ms
[2024-09-03 18:16:50,927] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:16:50,937] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:16:50,940] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38383 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:16:50,942] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:50,943] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198856 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:50,965] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:50,967] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:50,967] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:50,967] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:50,967] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:50,970] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:50,991] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:16:50,993] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:16:50,994] {spark_submit.py:495} INFO - 24/09/03 18:16:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38383 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:16:51,003] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:51,008] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:51,009] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:16:51,019] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:51,022] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:16:51,041] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:51,041] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:51,041] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:51,095] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO CodeGenerator: Code generated in 11.681297 ms
[2024-09-03 18:16:51,099] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-4552, partition values: [empty row]
[2024-09-03 18:16:51,122] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO CodeGenerator: Code generated in 19.743648 ms
[2024-09-03 18:16:51,137] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO FileOutputCommitter: Saved output of task 'attempt_20240903181650519895483580732333_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240827/_temporary/0/task_20240903181650519895483580732333_0002_m_000000
[2024-09-03 18:16:51,138] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO SparkHadoopMapRedUtil: attempt_20240903181650519895483580732333_0002_m_000000_2: Committed
[2024-09-03 18:16:51,138] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:16:51,139] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 121 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:51,142] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:16:51,142] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,171 s
[2024-09-03 18:16:51,143] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:51,144] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:16:51,144] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,177005 s
[2024-09-03 18:16:51,196] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO FileFormatWriter: Write Job d6c4c48b-5a50-4a6a-b6b4-d554ae05a3d1 committed.
[2024-09-03 18:16:51,196] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO FileFormatWriter: Finished processing stats for write job d6c4c48b-5a50-4a6a-b6b4-d554ae05a3d1.
[2024-09-03 18:16:51,262] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:16:51,307] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:16:51,326] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:16:51,358] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:16:51,358] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO BlockManager: BlockManager stopped
[2024-09-03 18:16:51,374] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:16:51,376] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:16:51,400] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:16:51,401] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:16:51,401] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2a3f295-de87-4245-86de-acf7fe02a15f
[2024-09-03 18:16:51,407] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-cdfb79da-db33-4e42-9acf-1839973836e9
[2024-09-03 18:16:51,411] {spark_submit.py:495} INFO - 24/09/03 18:16:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-cdfb79da-db33-4e42-9acf-1839973836e9/pyspark-1f5c76a6-fd27-4677-b141-53abdeecff72
[2024-09-03 18:16:51,523] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240827T000000, start_date=20240903T211641, end_date=20240903T211651
[2024-09-03 18:16:51,553] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:16:51,581] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:23:01,852] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 18:23:01,860] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 18:23:01,860] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:23:01,860] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:23:01,860] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:23:01,872] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-27 00:00:00+00:00
[2024-09-03 18:23:01,875] {standard_task_runner.py:52} INFO - Started process 344951 to run task
[2024-09-03 18:23:01,878] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-27T00:00:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpgsa8yfqq', '--error-file', '/tmp/tmpb41gy5z1']
[2024-09-03 18:23:01,879] {standard_task_runner.py:80} INFO - Job 99: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:23:01,913] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:23:01,961] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-27T00:00:00+00:00
[2024-09-03 18:23:01,967] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:23:01,968] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827
[2024-09-03 18:23:03,384] {spark_submit.py:495} INFO - 24/09/03 18:23:03 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:23:03,384] {spark_submit.py:495} INFO - 24/09/03 18:23:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:23:03,894] {spark_submit.py:495} INFO - 24/09/03 18:23:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:23:04,584] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:23:04,597] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:23:04,656] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO ResourceUtils: ==============================================================
[2024-09-03 18:23:04,657] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:23:04,657] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO ResourceUtils: ==============================================================
[2024-09-03 18:23:04,658] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:23:04,686] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:23:04,704] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:23:04,704] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:23:04,756] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:23:04,756] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:23:04,756] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:23:04,757] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:23:04,757] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:23:04,934] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO Utils: Successfully started service 'sparkDriver' on port 44171.
[2024-09-03 18:23:04,959] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:23:04,990] {spark_submit.py:495} INFO - 24/09/03 18:23:04 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:23:05,008] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:23:05,009] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:23:05,014] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:23:05,027] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1e1e2599-957b-4bc8-ae45-c27d2af231cd
[2024-09-03 18:23:05,051] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:23:05,071] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:23:05,304] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:23:05,354] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:23:05,562] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:23:05,587] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35717.
[2024-09-03 18:23:05,587] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO NettyBlockTransferService: Server created on 192.168.2.128:35717
[2024-09-03 18:23:05,589] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:23:05,594] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 35717, None)
[2024-09-03 18:23:05,598] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:35717 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 35717, None)
[2024-09-03 18:23:05,599] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 35717, None)
[2024-09-03 18:23:05,600] {spark_submit.py:495} INFO - 24/09/03 18:23:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 35717, None)
[2024-09-03 18:23:06,071] {spark_submit.py:495} INFO - 24/09/03 18:23:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:23:06,072] {spark_submit.py:495} INFO - 24/09/03 18:23:06 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:23:06,989] {spark_submit.py:495} INFO - 24/09/03 18:23:06 INFO InMemoryFileIndex: It took 53 ms to list leaf files for 1 paths.
[2024-09-03 18:23:07,069] {spark_submit.py:495} INFO - 24/09/03 18:23:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:23:09,203] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:23:09,204] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:23:09,207] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:23:09,633] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:23:09,693] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:23:09,702] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:35717 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:23:09,706] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:23:09,719] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:23:09,969] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:23:10,000] {spark_submit.py:495} INFO - 24/09/03 18:23:09 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:23:10,000] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:23:10,000] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:23:10,001] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:23:10,009] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:23:10,118] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:23:10,121] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:23:10,121] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:35717 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:23:10,122] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:23:10,140] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:23:10,141] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:23:10,212] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:23:10,237] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:23:10,850] {spark_submit.py:495} INFO - 24/09/03 18:23:10 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-456536, partition values: [empty row]
[2024-09-03 18:23:11,160] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO CodeGenerator: Code generated in 184.250396 ms
[2024-09-03 18:23:11,269] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:23:11,399] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1097 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:23:11,402] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:23:11,420] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,395 s
[2024-09-03 18:23:11,425] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:23:11,426] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:23:11,434] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,463342 s
[2024-09-03 18:23:11,877] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:23:11,879] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:23:11,880] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:23:11,959] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:23:11,960] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:23:11,961] {spark_submit.py:495} INFO - 24/09/03 18:23:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:23:12,048] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 30.431666 ms
[2024-09-03 18:23:12,094] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 29.165847 ms
[2024-09-03 18:23:12,102] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:23:12,112] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:23:12,114] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:35717 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:23:12,115] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:23:12,118] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:23:12,183] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:23:12,185] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:23:12,185] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:23:12,185] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:23:12,185] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:23:12,189] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:23:12,237] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:23:12,239] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:23:12,239] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:35717 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:23:12,240] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:23:12,240] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:23:12,240] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:23:12,250] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:23:12,251] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:23:12,303] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:23:12,304] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:23:12,304] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:23:12,396] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 41.337401 ms
[2024-09-03 18:23:12,401] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-456536, partition values: [empty row]
[2024-09-03 18:23:12,431] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 25.884721 ms
[2024-09-03 18:23:12,456] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 5.781764 ms
[2024-09-03 18:23:12,565] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409031823121602660661633891489_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240827/_temporary/0/task_202409031823121602660661633891489_0001_m_000000
[2024-09-03 18:23:12,566] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkHadoopMapRedUtil: attempt_202409031823121602660661633891489_0001_m_000000_1: Committed
[2024-09-03 18:23:12,570] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:23:12,581] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 334 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:23:12,581] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:23:12,581] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,389 s
[2024-09-03 18:23:12,581] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:23:12,582] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:23:12,585] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,393855 s
[2024-09-03 18:23:12,621] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileFormatWriter: Write Job aae708f2-f25e-4baa-b962-dcda068274c8 committed.
[2024-09-03 18:23:12,626] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileFormatWriter: Finished processing stats for write job aae708f2-f25e-4baa-b962-dcda068274c8.
[2024-09-03 18:23:12,676] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:23:12,676] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:23:12,676] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:23:12,687] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:23:12,687] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:23:12,688] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:23:12,725] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 9.859536 ms
[2024-09-03 18:23:12,730] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:23:12,738] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:23:12,740] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:35717 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:23:12,741] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:23:12,742] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4650840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:23:12,759] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:23:12,759] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:23:12,760] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:23:12,762] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:23:12,762] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:23:12,763] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:23:12,778] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:23:12,781] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:23:12,782] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:35717 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:23:12,784] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:23:12,785] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:23:12,785] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:23:12,787] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:23:12,787] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:23:12,800] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:23:12,800] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:23:12,801] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:23:12,836] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 11.435852 ms
[2024-09-03 18:23:12,847] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-456536, partition values: [empty row]
[2024-09-03 18:23:12,863] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO CodeGenerator: Code generated in 12.421972 ms
[2024-09-03 18:23:12,896] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileOutputCommitter: Saved output of task 'attempt_202409031823125585519041922997201_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240827/_temporary/0/task_202409031823125585519041922997201_0002_m_000000
[2024-09-03 18:23:12,896] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkHadoopMapRedUtil: attempt_202409031823125585519041922997201_0002_m_000000_2: Committed
[2024-09-03 18:23:12,897] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:23:12,901] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 115 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:23:12,902] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:23:12,915] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,148 s
[2024-09-03 18:23:12,915] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:23:12,915] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:23:12,915] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,151917 s
[2024-09-03 18:23:12,936] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileFormatWriter: Write Job 2a474b51-a806-4769-9169-c28143108a34 committed.
[2024-09-03 18:23:12,938] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO FileFormatWriter: Finished processing stats for write job 2a474b51-a806-4769-9169-c28143108a34.
[2024-09-03 18:23:12,971] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:23:12,985] {spark_submit.py:495} INFO - 24/09/03 18:23:12 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:23:13,001] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:23:13,015] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:23:13,016] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO BlockManager: BlockManager stopped
[2024-09-03 18:23:13,023] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:23:13,027] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:23:13,035] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:23:13,035] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:23:13,035] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-12097dc7-9ba2-4c02-aa33-71ed11b59125
[2024-09-03 18:23:13,037] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-12097dc7-9ba2-4c02-aa33-71ed11b59125/pyspark-7c002ab6-a9e2-43ca-8396-8726176c0ad6
[2024-09-03 18:23:13,040] {spark_submit.py:495} INFO - 24/09/03 18:23:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-d65262a8-02c7-4e10-810c-99a460004e18
[2024-09-03 18:23:13,128] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240827T000000, start_date=20240903T212301, end_date=20240903T212313
[2024-09-03 18:23:13,170] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:23:13,186] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:53:09,842] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 22:53:09,849] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [queued]>
[2024-09-03 22:53:09,849] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:53:09,849] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:53:09,849] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:53:09,858] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-27 00:00:00+00:00
[2024-09-03 22:53:09,860] {standard_task_runner.py:52} INFO - Started process 452393 to run task
[2024-09-03 22:53:09,864] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-27T00:00:00+00:00', '--job-id', '143', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_b3g_iv3', '--error-file', '/tmp/tmp2uofgy96']
[2024-09-03 22:53:09,865] {standard_task_runner.py:80} INFO - Job 143: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:53:09,897] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-27T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:53:09,943] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-27T00:00:00+00:00
[2024-09-03 22:53:09,946] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:53:09,949] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240827
[2024-09-03 22:53:10,978] {spark_submit.py:495} INFO - 24/09/03 22:53:10 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:53:10,979] {spark_submit.py:495} INFO - 24/09/03 22:53:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:53:11,290] {spark_submit.py:495} INFO - 24/09/03 22:53:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:53:11,809] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:53:11,816] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:53:11,853] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO ResourceUtils: ==============================================================
[2024-09-03 22:53:11,854] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:53:11,854] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO ResourceUtils: ==============================================================
[2024-09-03 22:53:11,855] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:53:11,876] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:53:11,887] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:53:11,888] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:53:11,928] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:53:11,928] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:53:11,929] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:53:11,929] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:53:11,929] {spark_submit.py:495} INFO - 24/09/03 22:53:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:53:12,074] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO Utils: Successfully started service 'sparkDriver' on port 36403.
[2024-09-03 22:53:12,095] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:53:12,120] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:53:12,137] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:53:12,137] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:53:12,142] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:53:12,152] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2d261eb3-f8b4-4cba-9340-8164c2fbf8ea
[2024-09-03 22:53:12,172] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:53:12,187] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:53:12,369] {spark_submit.py:495} INFO - 24/09/03 22:53:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:53:12,375] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:53:12,434] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:53:12,630] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:53:12,660] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42287.
[2024-09-03 22:53:12,661] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO NettyBlockTransferService: Server created on 192.168.2.128:42287
[2024-09-03 22:53:12,662] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:53:12,667] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 42287, None)
[2024-09-03 22:53:12,673] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:42287 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 42287, None)
[2024-09-03 22:53:12,676] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 42287, None)
[2024-09-03 22:53:12,677] {spark_submit.py:495} INFO - 24/09/03 22:53:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 42287, None)
[2024-09-03 22:53:13,143] {spark_submit.py:495} INFO - 24/09/03 22:53:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:53:13,144] {spark_submit.py:495} INFO - 24/09/03 22:53:13 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:53:13,992] {spark_submit.py:495} INFO - 24/09/03 22:53:13 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.
[2024-09-03 22:53:14,044] {spark_submit.py:495} INFO - 24/09/03 22:53:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:53:15,741] {spark_submit.py:495} INFO - 24/09/03 22:53:15 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:53:15,742] {spark_submit.py:495} INFO - 24/09/03 22:53:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:53:15,745] {spark_submit.py:495} INFO - 24/09/03 22:53:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:53:15,992] {spark_submit.py:495} INFO - 24/09/03 22:53:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:53:16,037] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:53:16,040] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:42287 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:53:16,044] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:53:16,052] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198771 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:53:16,199] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:53:16,216] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:53:16,216] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:53:16,216] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:53:16,217] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:53:16,222] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:53:16,302] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:53:16,306] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:53:16,306] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:42287 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:53:16,307] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:53:16,317] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:53:16,317] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:53:16,372] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:53:16,394] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:53:16,670] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-4467, partition values: [empty row]
[2024-09-03 22:53:16,928] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO CodeGenerator: Code generated in 163.042498 ms
[2024-09-03 22:53:16,981] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 22:53:16,992] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 634 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:53:16,994] {spark_submit.py:495} INFO - 24/09/03 22:53:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:53:17,001] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,767 s
[2024-09-03 22:53:17,094] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:53:17,096] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:53:17,098] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,899187 s
[2024-09-03 22:53:17,507] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:53:17,508] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:53:17,508] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:53:17,574] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:53:17,575] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:53:17,575] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:53:17,663] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO CodeGenerator: Code generated in 37.25545 ms
[2024-09-03 22:53:17,708] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO CodeGenerator: Code generated in 28.846689 ms
[2024-09-03 22:53:17,713] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:53:17,724] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:53:17,724] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:42287 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:53:17,725] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:53:17,727] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198771 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:53:17,783] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:53:17,785] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:53:17,786] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:53:17,786] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:53:17,787] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:53:17,788] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:53:17,830] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:53:17,834] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:53:17,836] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:42287 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:53:17,837] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:53:17,839] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:53:17,839] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:53:17,843] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:53:17,844] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:53:17,895] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:53:17,896] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:53:17,896] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:53:17,946] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO CodeGenerator: Code generated in 16.058338 ms
[2024-09-03 22:53:17,950] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-4467, partition values: [empty row]
[2024-09-03 22:53:17,975] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO CodeGenerator: Code generated in 21.897108 ms
[2024-09-03 22:53:17,998] {spark_submit.py:495} INFO - 24/09/03 22:53:17 INFO CodeGenerator: Code generated in 5.285572 ms
[2024-09-03 22:53:18,029] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409032253173779774075156975513_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240827/_temporary/0/task_202409032253173779774075156975513_0001_m_000000
[2024-09-03 22:53:18,030] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkHadoopMapRedUtil: attempt_202409032253173779774075156975513_0001_m_000000_1: Committed
[2024-09-03 22:53:18,035] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:53:18,042] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 203 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:53:18,042] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:53:18,043] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,254 s
[2024-09-03 22:53:18,044] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:53:18,044] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:53:18,047] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,260638 s
[2024-09-03 22:53:18,060] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileFormatWriter: Write Job 0b69b4ef-5b1d-4b87-8118-b094a42257f1 committed.
[2024-09-03 22:53:18,063] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileFormatWriter: Finished processing stats for write job 0b69b4ef-5b1d-4b87-8118-b094a42257f1.
[2024-09-03 22:53:18,109] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:53:18,110] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:53:18,110] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:53:18,122] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:53:18,122] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:53:18,123] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:53:18,156] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO CodeGenerator: Code generated in 11.182472 ms
[2024-09-03 22:53:18,159] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:53:18,167] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:53:18,168] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:42287 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:53:18,170] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:53:18,171] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198771 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:53:18,188] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:53:18,188] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:53:18,188] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:53:18,188] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:53:18,189] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:53:18,191] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:53:18,208] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:53:18,210] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:53:18,210] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:42287 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:53:18,211] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:53:18,211] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:53:18,211] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:53:18,214] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:53:18,214] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:53:18,227] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:53:18,228] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:53:18,228] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:53:18,263] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO CodeGenerator: Code generated in 9.946117 ms
[2024-09-03 22:53:18,267] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-27T00:00:00.00Z/data_engineer_20240827.json, range: 0-4467, partition values: [empty row]
[2024-09-03 22:53:18,282] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO CodeGenerator: Code generated in 12.167821 ms
[2024-09-03 22:53:18,291] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileOutputCommitter: Saved output of task 'attempt_202409032253187874227059978214800_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240827/_temporary/0/task_202409032253187874227059978214800_0002_m_000000
[2024-09-03 22:53:18,291] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkHadoopMapRedUtil: attempt_202409032253187874227059978214800_0002_m_000000_2: Committed
[2024-09-03 22:53:18,292] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:53:18,294] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 80 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:53:18,294] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:53:18,296] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,104 s
[2024-09-03 22:53:18,297] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:53:18,297] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:53:18,297] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,108390 s
[2024-09-03 22:53:18,314] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileFormatWriter: Write Job c1841ace-f4ba-4310-92c9-19358d0ded57 committed.
[2024-09-03 22:53:18,314] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO FileFormatWriter: Finished processing stats for write job c1841ace-f4ba-4310-92c9-19358d0ded57.
[2024-09-03 22:53:18,349] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:53:18,369] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:53:18,382] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:53:18,396] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:53:18,396] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO BlockManager: BlockManager stopped
[2024-09-03 22:53:18,405] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:53:18,407] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:53:18,412] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:53:18,412] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:53:18,413] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3677df53-cb6a-416d-bdba-5559915801a5
[2024-09-03 22:53:18,415] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3677df53-cb6a-416d-bdba-5559915801a5/pyspark-79e6c81d-292a-4695-8ad2-84084f80f030
[2024-09-03 22:53:18,418] {spark_submit.py:495} INFO - 24/09/03 22:53:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-efdf4857-0e14-42a4-b635-9ca6eafffde8
[2024-09-03 22:53:18,509] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240827T000000, start_date=20240904T015309, end_date=20240904T015318
[2024-09-03 22:53:18,559] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:53:18,585] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
