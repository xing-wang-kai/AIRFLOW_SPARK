[2024-09-03 16:23:26,981] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 16:23:26,989] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 16:23:26,989] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:23:26,989] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:23:26,989] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:23:26,999] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 16:23:27,002] {standard_task_runner.py:52} INFO - Started process 284708 to run task
[2024-09-03 16:23:27,005] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '90', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpmgijr0zp', '--error-file', '/tmp/tmpbj2b1s13']
[2024-09-03 16:23:27,005] {standard_task_runner.py:80} INFO - Job 90: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:23:27,040] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:23:27,084] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 16:23:27,087] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:23:27,088] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/ --process-data 20240826
[2024-09-03 16:23:28,105] {spark_submit.py:495} INFO - 24/09/03 16:23:28 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:23:28,107] {spark_submit.py:495} INFO - 24/09/03 16:23:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:23:28,499] {spark_submit.py:495} INFO - 24/09/03 16:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:23:29,047] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:23:29,075] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:23:29,183] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO ResourceUtils: ==============================================================
[2024-09-03 16:23:29,183] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:23:29,185] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO ResourceUtils: ==============================================================
[2024-09-03 16:23:29,185] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:23:29,235] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:23:29,252] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:23:29,252] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:23:29,305] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:23:29,305] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:23:29,305] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:23:29,305] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:23:29,305] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:23:29,474] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO Utils: Successfully started service 'sparkDriver' on port 36891.
[2024-09-03 16:23:29,505] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:23:29,537] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:23:29,555] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:23:29,555] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:23:29,558] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:23:29,570] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-beab9962-34eb-49a5-a0eb-65c75580fd7e
[2024-09-03 16:23:29,590] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:23:29,605] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:23:29,776] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:23:29,838] {spark_submit.py:495} INFO - 24/09/03 16:23:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:23:30,049] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:23:30,073] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38331.
[2024-09-03 16:23:30,073] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO NettyBlockTransferService: Server created on 192.168.2.128:38331
[2024-09-03 16:23:30,075] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:23:30,080] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38331, None)
[2024-09-03 16:23:30,083] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38331 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38331, None)
[2024-09-03 16:23:30,085] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38331, None)
[2024-09-03 16:23:30,086] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38331, None)
[2024-09-03 16:23:30,768] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:23:30,768] {spark_submit.py:495} INFO - 24/09/03 16:23:30 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:23:31,522] {spark_submit.py:495} INFO - 24/09/03 16:23:31 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2024-09-03 16:23:31,576] {spark_submit.py:495} INFO - 24/09/03 16:23:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:23:33,253] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:23:33,254] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:23:33,256] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:23:33,492] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:23:33,535] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:23:33,539] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38331 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:23:33,545] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:33,552] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198804 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:23:33,688] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:33,703] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:23:33,704] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:23:33,705] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:23:33,706] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:23:33,715] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:23:33,782] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:23:33,784] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:23:33,785] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38331 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:23:33,786] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:23:33,802] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:23:33,803] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:23:33,845] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4967 bytes) taskResourceAssignments Map()
[2024-09-03 16:23:33,861] {spark_submit.py:495} INFO - 24/09/03 16:23:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:23:34,121] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4500, partition values: [empty row]
[2024-09-03 16:23:34,341] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO CodeGenerator: Code generated in 130.997442 ms
[2024-09-03 16:23:34,384] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:23:34,394] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 557 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:23:34,396] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:23:34,405] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,678 s
[2024-09-03 16:23:34,507] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:23:34,508] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:23:34,510] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,820926 s
[2024-09-03 16:23:34,933] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:23:34,935] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:23:34,937] {spark_submit.py:495} INFO - 24/09/03 16:23:34 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:23:35,025] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:35,025] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:35,026] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:35,240] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO CodeGenerator: Code generated in 56.976282 ms
[2024-09-03 16:23:35,300] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO CodeGenerator: Code generated in 36.372028 ms
[2024-09-03 16:23:35,310] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:23:35,325] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:23:35,327] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38331 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:23:35,328] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:35,331] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198804 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:23:35,405] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:35,409] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:23:35,409] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:23:35,409] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:23:35,410] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:23:35,410] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:23:35,459] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:23:35,462] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:23:35,462] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38331 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:23:35,463] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:23:35,463] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:23:35,463] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:23:35,468] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:23:35,468] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:23:35,529] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:35,530] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:35,531] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:35,642] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO CodeGenerator: Code generated in 44.666198 ms
[2024-09-03 16:23:35,647] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4500, partition values: [empty row]
[2024-09-03 16:23:35,687] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO CodeGenerator: Code generated in 36.431832 ms
[2024-09-03 16:23:35,712] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO CodeGenerator: Code generated in 6.124867 ms
[2024-09-03 16:23:35,745] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: Saved output of task 'attempt_202409031623358666281456022638961_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/twitter_df/process_data=20240826/_temporary/0/task_202409031623358666281456022638961_0001_m_000000
[2024-09-03 16:23:35,745] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkHadoopMapRedUtil: attempt_202409031623358666281456022638961_0001_m_000000_1: Committed
[2024-09-03 16:23:35,749] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:23:35,758] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 294 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:23:35,759] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:23:35,761] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,348 s
[2024-09-03 16:23:35,762] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:23:35,762] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:23:35,767] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,361479 s
[2024-09-03 16:23:35,800] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileFormatWriter: Write Job 094a6388-03ec-4a56-9e4a-1ee4867bf638 committed.
[2024-09-03 16:23:35,806] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileFormatWriter: Finished processing stats for write job 094a6388-03ec-4a56-9e4a-1ee4867bf638.
[2024-09-03 16:23:35,849] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:23:35,849] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:23:35,849] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:23:35,864] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:35,865] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:35,865] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:35,898] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO CodeGenerator: Code generated in 8.432987 ms
[2024-09-03 16:23:35,902] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:23:35,909] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:23:35,909] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38331 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:23:35,910] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:35,911] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198804 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:23:35,926] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:35,927] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:23:35,927] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:23:35,927] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:23:35,928] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:23:35,928] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:23:35,944] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:23:35,947] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.9 KiB, free 364.8 MiB)
[2024-09-03 16:23:35,949] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38331 (size: 63.9 KiB, free: 366.1 MiB)
[2024-09-03 16:23:35,950] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:23:35,953] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:23:35,953] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:23:35,954] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:23:35,954] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:23:35,966] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:35,967] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:35,967] {spark_submit.py:495} INFO - 24/09/03 16:23:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:36,007] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO CodeGenerator: Code generated in 10.223762 ms
[2024-09-03 16:23:36,008] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4500, partition values: [empty row]
[2024-09-03 16:23:36,021] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO CodeGenerator: Code generated in 10.712193 ms
[2024-09-03 16:23:36,029] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO FileOutputCommitter: Saved output of task 'attempt_202409031623354455800086424024632_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/user_df/process_data=20240826/_temporary/0/task_202409031623354455800086424024632_0002_m_000000
[2024-09-03 16:23:36,029] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO SparkHadoopMapRedUtil: attempt_202409031623354455800086424024632_0002_m_000000_2: Committed
[2024-09-03 16:23:36,030] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:23:36,033] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 80 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:23:36,033] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:23:36,035] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,105 s
[2024-09-03 16:23:36,035] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:23:36,035] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:23:36,035] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,109040 s
[2024-09-03 16:23:36,070] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO FileFormatWriter: Write Job 19cd78ad-2cf7-44a3-98d9-45885569b734 committed.
[2024-09-03 16:23:36,071] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO FileFormatWriter: Finished processing stats for write job 19cd78ad-2cf7-44a3-98d9-45885569b734.
[2024-09-03 16:23:36,097] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:23:36,109] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:23:36,118] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:23:36,126] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:23:36,127] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO BlockManager: BlockManager stopped
[2024-09-03 16:23:36,132] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:23:36,134] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:23:36,139] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:23:36,139] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:23:36,139] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2360fbd7-50dd-4c85-a0f9-fb19e2fe349c
[2024-09-03 16:23:36,141] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-1232678e-8d7e-42ad-9133-628b81e4cbd2/pyspark-b3934c64-05a8-4b29-8b25-f0b24472d5c9
[2024-09-03 16:23:36,144] {spark_submit.py:495} INFO - 24/09/03 16:23:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-1232678e-8d7e-42ad-9133-628b81e4cbd2
[2024-09-03 16:23:36,201] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240903T192326, end_date=20240903T192336
[2024-09-03 16:23:36,226] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:23:36,235] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:41:47,415] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 16:41:47,420] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 16:41:47,420] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:41:47,420] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:41:47,420] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:41:47,429] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 16:41:47,432] {standard_task_runner.py:52} INFO - Started process 295595 to run task
[2024-09-03 16:41:47,434] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '90', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp90l0b6l_', '--error-file', '/tmp/tmp_lqz29bo']
[2024-09-03 16:41:47,435] {standard_task_runner.py:80} INFO - Job 90: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:41:47,467] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:41:47,509] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 16:41:47,512] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:41:47,513] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 16:41:48,523] {spark_submit.py:495} INFO - 24/09/03 16:41:48 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:41:48,523] {spark_submit.py:495} INFO - 24/09/03 16:41:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:41:48,817] {spark_submit.py:495} INFO - 24/09/03 16:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:41:49,329] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:41:49,338] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:41:49,375] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO ResourceUtils: ==============================================================
[2024-09-03 16:41:49,376] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:41:49,376] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO ResourceUtils: ==============================================================
[2024-09-03 16:41:49,377] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:41:49,401] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:41:49,415] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:41:49,415] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:41:49,459] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:41:49,459] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:41:49,459] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:41:49,459] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:41:49,459] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:41:49,604] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO Utils: Successfully started service 'sparkDriver' on port 44179.
[2024-09-03 16:41:49,627] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:41:49,654] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:41:49,669] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:41:49,670] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:41:49,674] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:41:49,684] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-41f4c2d5-7441-4b2e-962c-d2b8c93d8693
[2024-09-03 16:41:49,704] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:41:49,718] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:41:49,908] {spark_submit.py:495} INFO - 24/09/03 16:41:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:41:49,915] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:41:49,960] {spark_submit.py:495} INFO - 24/09/03 16:41:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:41:50,146] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:41:50,169] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40253.
[2024-09-03 16:41:50,169] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO NettyBlockTransferService: Server created on 192.168.2.128:40253
[2024-09-03 16:41:50,171] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:41:50,176] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40253, None)
[2024-09-03 16:41:50,178] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40253 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40253, None)
[2024-09-03 16:41:50,180] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40253, None)
[2024-09-03 16:41:50,181] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40253, None)
[2024-09-03 16:41:50,614] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:41:50,614] {spark_submit.py:495} INFO - 24/09/03 16:41:50 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:41:51,342] {spark_submit.py:495} INFO - 24/09/03 16:41:51 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2024-09-03 16:41:51,393] {spark_submit.py:495} INFO - 24/09/03 16:41:51 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 16:41:53,016] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:41:53,018] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:41:53,021] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:41:53,274] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:41:53,313] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:41:53,315] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40253 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:41:53,323] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:53,331] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198835 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:41:53,474] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:53,489] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:41:53,490] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:41:53,490] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:41:53,491] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:41:53,498] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:41:53,572] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:41:53,576] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:41:53,576] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40253 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:41:53,577] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:41:53,587] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:41:53,588] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:41:53,631] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:41:53,651] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:41:53,919] {spark_submit.py:495} INFO - 24/09/03 16:41:53 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4531, partition values: [empty row]
[2024-09-03 16:41:54,181] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO CodeGenerator: Code generated in 132.076727 ms
[2024-09-03 16:41:54,224] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 16:41:54,232] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 610 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:41:54,234] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:41:54,239] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,728 s
[2024-09-03 16:41:54,341] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:41:54,342] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:41:54,347] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,870845 s
[2024-09-03 16:41:54,713] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:41:54,714] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:41:54,715] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:41:54,788] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:54,789] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:54,789] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:54,871] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO CodeGenerator: Code generated in 30.254046 ms
[2024-09-03 16:41:54,913] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO CodeGenerator: Code generated in 27.030243 ms
[2024-09-03 16:41:54,921] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:41:54,928] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:41:54,929] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40253 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:41:54,930] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:54,932] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198835 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:41:54,989] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:54,990] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:41:54,990] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:41:54,990] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:41:54,990] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:41:54,992] {spark_submit.py:495} INFO - 24/09/03 16:41:54 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:41:55,029] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:41:55,031] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:41:55,032] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40253 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:41:55,032] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:41:55,033] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:41:55,034] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:41:55,038] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:41:55,042] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:41:55,085] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:55,085] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:55,087] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:55,177] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO CodeGenerator: Code generated in 21.22405 ms
[2024-09-03 16:41:55,180] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4531, partition values: [empty row]
[2024-09-03 16:41:55,207] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO CodeGenerator: Code generated in 21.77587 ms
[2024-09-03 16:41:55,233] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO CodeGenerator: Code generated in 6.540759 ms
[2024-09-03 16:41:55,271] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: Saved output of task 'attempt_202409031641547093342088021402203_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_202409031641547093342088021402203_0001_m_000000
[2024-09-03 16:41:55,272] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkHadoopMapRedUtil: attempt_202409031641547093342088021402203_0001_m_000000_1: Committed
[2024-09-03 16:41:55,277] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:41:55,283] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 248 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:41:55,284] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,292 s
[2024-09-03 16:41:55,284] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:41:55,286] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:41:55,287] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:41:55,287] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,299196 s
[2024-09-03 16:41:55,307] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileFormatWriter: Write Job ce9f4c8a-eafc-4fca-9dfc-d32ceab3ba70 committed.
[2024-09-03 16:41:55,313] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileFormatWriter: Finished processing stats for write job ce9f4c8a-eafc-4fca-9dfc-d32ceab3ba70.
[2024-09-03 16:41:55,356] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:41:55,357] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:41:55,357] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:41:55,365] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:55,366] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:55,366] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:55,398] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO CodeGenerator: Code generated in 14.861697 ms
[2024-09-03 16:41:55,402] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:41:55,410] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:41:55,411] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40253 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:41:55,412] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:55,414] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198835 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:41:55,429] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:55,431] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:41:55,431] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:41:55,431] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:41:55,431] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:41:55,432] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:41:55,452] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:41:55,454] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:41:55,455] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40253 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:41:55,455] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:41:55,456] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:41:55,456] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:41:55,456] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:41:55,459] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:41:55,468] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:55,471] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:55,473] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:55,511] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO CodeGenerator: Code generated in 10.410917 ms
[2024-09-03 16:41:55,514] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4531, partition values: [empty row]
[2024-09-03 16:41:55,532] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO CodeGenerator: Code generated in 14.466318 ms
[2024-09-03 16:41:55,544] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileOutputCommitter: Saved output of task 'attempt_202409031641553166371718053494618_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409031641553166371718053494618_0002_m_000000
[2024-09-03 16:41:55,544] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkHadoopMapRedUtil: attempt_202409031641553166371718053494618_0002_m_000000_2: Committed
[2024-09-03 16:41:55,545] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:41:55,546] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 90 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:41:55,547] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,114 s
[2024-09-03 16:41:55,547] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:41:55,548] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:41:55,548] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:41:55,548] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,117812 s
[2024-09-03 16:41:55,579] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileFormatWriter: Write Job dcac0371-951b-48d5-90c8-42069070f890 committed.
[2024-09-03 16:41:55,580] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO FileFormatWriter: Finished processing stats for write job dcac0371-951b-48d5-90c8-42069070f890.
[2024-09-03 16:41:55,615] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:41:55,625] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:41:55,637] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:41:55,646] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:41:55,647] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO BlockManager: BlockManager stopped
[2024-09-03 16:41:55,654] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:41:55,657] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:41:55,670] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:41:55,671] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:41:55,672] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-aabf9c03-5216-45ce-b219-8e8104f7ecf3
[2024-09-03 16:41:55,674] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-18a09c31-8d11-498c-bbc4-742ec23a2ac4
[2024-09-03 16:41:55,677] {spark_submit.py:495} INFO - 24/09/03 16:41:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-aabf9c03-5216-45ce-b219-8e8104f7ecf3/pyspark-93219b96-b9e0-48a5-87de-092cd43859ce
[2024-09-03 16:41:55,797] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240903T194147, end_date=20240903T194155
[2024-09-03 16:41:55,816] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:41:55,825] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:55:22,761] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 17:55:22,767] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 17:55:22,768] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:55:22,768] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:55:22,768] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:55:22,779] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 17:55:22,781] {standard_task_runner.py:52} INFO - Started process 325671 to run task
[2024-09-03 17:55:22,784] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpp8xzg1q2', '--error-file', '/tmp/tmpsa7r7n6e']
[2024-09-03 17:55:22,785] {standard_task_runner.py:80} INFO - Job 96: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:55:22,816] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:55:22,856] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 17:55:22,861] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:55:22,862] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 17:55:23,809] {spark_submit.py:495} INFO - 24/09/03 17:55:23 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:55:23,809] {spark_submit.py:495} INFO - 24/09/03 17:55:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:55:24,148] {spark_submit.py:495} INFO - 24/09/03 17:55:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:55:24,684] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:55:24,691] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:55:24,735] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO ResourceUtils: ==============================================================
[2024-09-03 17:55:24,735] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:55:24,737] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO ResourceUtils: ==============================================================
[2024-09-03 17:55:24,737] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:55:24,762] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:55:24,774] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:55:24,775] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:55:24,816] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:55:24,816] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:55:24,816] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:55:24,817] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:55:24,817] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:55:24,964] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO Utils: Successfully started service 'sparkDriver' on port 46231.
[2024-09-03 17:55:24,986] {spark_submit.py:495} INFO - 24/09/03 17:55:24 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:55:25,012] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:55:25,026] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:55:25,027] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:55:25,030] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:55:25,040] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0bdaef08-e12e-460f-8d6a-62b8196b9537
[2024-09-03 17:55:25,058] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:55:25,073] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:55:25,252] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:55:25,299] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:55:25,476] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:55:25,498] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39383.
[2024-09-03 17:55:25,498] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO NettyBlockTransferService: Server created on 192.168.2.128:39383
[2024-09-03 17:55:25,500] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:55:25,504] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 39383, None)
[2024-09-03 17:55:25,506] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:39383 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 39383, None)
[2024-09-03 17:55:25,509] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 39383, None)
[2024-09-03 17:55:25,509] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 39383, None)
[2024-09-03 17:55:25,924] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:55:25,925] {spark_submit.py:495} INFO - 24/09/03 17:55:25 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:55:26,659] {spark_submit.py:495} INFO - 24/09/03 17:55:26 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.
[2024-09-03 17:55:26,709] {spark_submit.py:495} INFO - 24/09/03 17:55:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:55:28,371] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:55:28,372] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:55:28,375] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:55:28,615] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:55:28,663] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:55:28,666] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:39383 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:55:28,670] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:28,680] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:28,810] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:28,826] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:28,827] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:28,827] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:28,829] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:28,834] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:28,920] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:55:28,923] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:55:28,924] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:39383 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:55:28,924] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:28,941] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:28,942] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:55:28,989] {spark_submit.py:495} INFO - 24/09/03 17:55:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:29,010] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:55:29,311] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4589, partition values: [empty row]
[2024-09-03 17:55:29,528] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO CodeGenerator: Code generated in 125.747943 ms
[2024-09-03 17:55:29,577] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 17:55:29,586] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 606 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:29,588] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:55:29,593] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,737 s
[2024-09-03 17:55:29,707] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:29,709] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:55:29,717] {spark_submit.py:495} INFO - 24/09/03 17:55:29 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,902324 s
[2024-09-03 17:55:30,063] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:55:30,066] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:55:30,066] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:55:30,134] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:30,135] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:30,135] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:30,242] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 37.560063 ms
[2024-09-03 17:55:30,280] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 24.430511 ms
[2024-09-03 17:55:30,285] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:55:30,292] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:55:30,293] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:39383 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:55:30,295] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:30,299] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:30,352] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:30,353] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:30,353] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:30,353] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:30,353] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:30,356] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:30,416] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:55:30,418] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:55:30,419] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:39383 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:55:30,420] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:30,421] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:30,421] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:55:30,424] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:30,425] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:55:30,467] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:30,467] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:30,468] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:30,527] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 22.596126 ms
[2024-09-03 17:55:30,534] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4589, partition values: [empty row]
[2024-09-03 17:55:30,559] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 22.020064 ms
[2024-09-03 17:55:30,580] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 5.223241 ms
[2024-09-03 17:55:30,610] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: Saved output of task 'attempt_202409031755306608545577032834880_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_202409031755306608545577032834880_0001_m_000000
[2024-09-03 17:55:30,611] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkHadoopMapRedUtil: attempt_202409031755306608545577032834880_0001_m_000000_1: Committed
[2024-09-03 17:55:30,615] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:55:30,622] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 199 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:30,622] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:55:30,622] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,266 s
[2024-09-03 17:55:30,623] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:30,623] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:55:30,626] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,271653 s
[2024-09-03 17:55:30,639] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileFormatWriter: Write Job 12a3cd0a-ad78-4414-8dc0-7a278546c2d2 committed.
[2024-09-03 17:55:30,643] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileFormatWriter: Finished processing stats for write job 12a3cd0a-ad78-4414-8dc0-7a278546c2d2.
[2024-09-03 17:55:30,686] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:55:30,687] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:55:30,688] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:55:30,699] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:30,700] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:30,700] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:30,736] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 11.99375 ms
[2024-09-03 17:55:30,740] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:55:30,747] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:55:30,748] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:39383 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:55:30,749] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:30,750] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198893 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:30,765] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:30,766] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:30,766] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:30,766] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:30,766] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:30,767] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:30,784] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:55:30,786] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:55:30,787] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:39383 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:55:30,787] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:30,788] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:30,788] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:55:30,790] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:30,790] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:55:30,803] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:30,804] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:30,804] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:30,837] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 8.43652 ms
[2024-09-03 17:55:30,840] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4589, partition values: [empty row]
[2024-09-03 17:55:30,853] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO CodeGenerator: Code generated in 11.2045 ms
[2024-09-03 17:55:30,865] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileOutputCommitter: Saved output of task 'attempt_202409031755303455784828467739405_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409031755303455784828467739405_0002_m_000000
[2024-09-03 17:55:30,865] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkHadoopMapRedUtil: attempt_202409031755303455784828467739405_0002_m_000000_2: Committed
[2024-09-03 17:55:30,867] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:55:30,870] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:30,871] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:55:30,873] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,104 s
[2024-09-03 17:55:30,873] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:30,873] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:55:30,873] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,107192 s
[2024-09-03 17:55:30,886] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileFormatWriter: Write Job e31f2cfe-ee14-40d4-b056-a08f0bbe4510 committed.
[2024-09-03 17:55:30,886] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO FileFormatWriter: Finished processing stats for write job e31f2cfe-ee14-40d4-b056-a08f0bbe4510.
[2024-09-03 17:55:30,919] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:55:30,928] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:55:30,937] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:55:30,946] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:55:30,947] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO BlockManager: BlockManager stopped
[2024-09-03 17:55:30,952] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:55:30,956] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:55:30,963] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:55:30,963] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:55:30,964] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-30f177df-7839-4404-a892-b8e41c58a52d
[2024-09-03 17:55:30,967] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-3a869247-a20d-4af1-aee3-bc1a47287f15/pyspark-22e33ff8-0d47-473f-be86-7327f67a441b
[2024-09-03 17:55:30,970] {spark_submit.py:495} INFO - 24/09/03 17:55:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-3a869247-a20d-4af1-aee3-bc1a47287f15
[2024-09-03 17:55:31,036] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240903T205522, end_date=20240903T205531
[2024-09-03 17:55:31,069] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:55:31,090] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:12:13,770] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 18:12:13,777] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 18:12:13,777] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:12:13,777] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:12:13,777] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:12:13,789] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 18:12:13,791] {standard_task_runner.py:52} INFO - Started process 334259 to run task
[2024-09-03 18:12:13,794] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '97', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpzb0vf6ct', '--error-file', '/tmp/tmpa6pqr5n9']
[2024-09-03 18:12:13,795] {standard_task_runner.py:80} INFO - Job 97: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:12:13,838] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:12:13,891] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 18:12:13,897] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:12:13,898] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 18:12:15,073] {spark_submit.py:495} INFO - 24/09/03 18:12:15 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:12:15,074] {spark_submit.py:495} INFO - 24/09/03 18:12:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:12:15,469] {spark_submit.py:495} INFO - 24/09/03 18:12:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:12:16,094] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:12:16,106] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:12:16,191] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO ResourceUtils: ==============================================================
[2024-09-03 18:12:16,192] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:12:16,192] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO ResourceUtils: ==============================================================
[2024-09-03 18:12:16,193] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:12:16,220] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:12:16,233] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:12:16,233] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:12:16,287] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:12:16,287] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:12:16,293] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:12:16,293] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:12:16,294] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:12:16,488] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO Utils: Successfully started service 'sparkDriver' on port 43939.
[2024-09-03 18:12:16,527] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:12:16,583] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:12:16,606] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:12:16,610] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:12:16,614] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:12:16,630] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b8f92695-6a54-4649-b06a-c93de166df3b
[2024-09-03 18:12:16,657] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:12:16,680] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:12:16,935] {spark_submit.py:495} INFO - 24/09/03 18:12:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:12:17,008] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:12:17,300] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:12:17,346] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34729.
[2024-09-03 18:12:17,347] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO NettyBlockTransferService: Server created on 192.168.2.128:34729
[2024-09-03 18:12:17,349] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:12:17,357] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 34729, None)
[2024-09-03 18:12:17,363] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:34729 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 34729, None)
[2024-09-03 18:12:17,368] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 34729, None)
[2024-09-03 18:12:17,369] {spark_submit.py:495} INFO - 24/09/03 18:12:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 34729, None)
[2024-09-03 18:12:18,177] {spark_submit.py:495} INFO - 24/09/03 18:12:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:12:18,177] {spark_submit.py:495} INFO - 24/09/03 18:12:18 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:12:19,233] {spark_submit.py:495} INFO - 24/09/03 18:12:19 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.
[2024-09-03 18:12:19,312] {spark_submit.py:495} INFO - 24/09/03 18:12:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:12:21,737] {spark_submit.py:495} INFO - 24/09/03 18:12:21 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:12:21,738] {spark_submit.py:495} INFO - 24/09/03 18:12:21 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:12:21,743] {spark_submit.py:495} INFO - 24/09/03 18:12:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:12:22,062] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:12:22,116] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:12:22,119] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:34729 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:12:22,125] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:12:22,137] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649307 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:12:22,337] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:12:22,352] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:12:22,353] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:12:22,353] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:12:22,355] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:12:22,360] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:12:22,475] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:12:22,478] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:12:22,479] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:34729 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:12:22,480] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:12:22,497] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:12:22,499] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:12:22,563] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:12:22,589] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:12:22,943] {spark_submit.py:495} INFO - 24/09/03 18:12:22 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-455003, partition values: [empty row]
[2024-09-03 18:12:23,334] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO CodeGenerator: Code generated in 243.894226 ms
[2024-09-03 18:12:23,455] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:12:23,476] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 930 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:12:23,617] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:12:23,626] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,247 s
[2024-09-03 18:12:23,630] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:12:23,630] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:12:23,634] {spark_submit.py:495} INFO - 24/09/03 18:12:23 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,296400 s
[2024-09-03 18:12:24,221] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:12:24,223] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:12:24,224] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:12:24,324] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:12:24,324] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:12:24,325] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:12:24,442] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO CodeGenerator: Code generated in 41.820584 ms
[2024-09-03 18:12:24,508] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO CodeGenerator: Code generated in 42.794704 ms
[2024-09-03 18:12:24,517] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:12:24,530] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:12:24,532] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:34729 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:12:24,534] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:12:24,536] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649307 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:12:24,628] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:12:24,630] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:12:24,631] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:12:24,631] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:12:24,631] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:12:24,633] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:12:24,709] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 18:12:24,713] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:12:24,715] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:34729 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:12:24,718] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:12:24,719] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:12:24,720] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:12:24,724] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:12:24,726] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:12:24,803] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:12:24,803] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:12:24,813] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:12:24,914] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO CodeGenerator: Code generated in 32.764939 ms
[2024-09-03 18:12:24,920] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-455003, partition values: [empty row]
[2024-09-03 18:12:24,948] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO CodeGenerator: Code generated in 23.818763 ms
[2024-09-03 18:12:24,989] {spark_submit.py:495} INFO - 24/09/03 18:12:24 INFO CodeGenerator: Code generated in 6.67338 ms
[2024-09-03 18:12:25,156] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileOutputCommitter: Saved output of task 'attempt_202409031812245453822137733133602_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_202409031812245453822137733133602_0001_m_000000
[2024-09-03 18:12:25,157] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkHadoopMapRedUtil: attempt_202409031812245453822137733133602_0001_m_000000_1: Committed
[2024-09-03 18:12:25,161] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:12:25,169] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 448 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:12:25,173] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:12:25,173] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,533 s
[2024-09-03 18:12:25,175] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:12:25,175] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:12:25,176] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,543525 s
[2024-09-03 18:12:25,216] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileFormatWriter: Write Job 8c9671d2-06cc-4413-bf54-6c20f4837a10 committed.
[2024-09-03 18:12:25,222] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileFormatWriter: Finished processing stats for write job 8c9671d2-06cc-4413-bf54-6c20f4837a10.
[2024-09-03 18:12:25,284] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:12:25,284] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:12:25,286] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:12:25,303] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:12:25,303] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:12:25,304] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:12:25,370] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO CodeGenerator: Code generated in 16.719948 ms
[2024-09-03 18:12:25,376] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:12:25,386] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:12:25,387] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:34729 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:12:25,390] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:12:25,391] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649307 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:12:25,415] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:12:25,418] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:12:25,418] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:12:25,419] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:12:25,419] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:12:25,425] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:12:25,447] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:12:25,452] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:12:25,454] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:34729 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:12:25,455] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:12:25,456] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:12:25,456] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:12:25,458] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:12:25,458] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:12:25,488] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:12:25,488] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:12:25,489] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:12:25,551] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO CodeGenerator: Code generated in 25.116649 ms
[2024-09-03 18:12:25,555] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-455003, partition values: [empty row]
[2024-09-03 18:12:25,589] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO CodeGenerator: Code generated in 27.156008 ms
[2024-09-03 18:12:25,664] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileOutputCommitter: Saved output of task 'attempt_202409031812257377926298981951163_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409031812257377926298981951163_0002_m_000000
[2024-09-03 18:12:25,665] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkHadoopMapRedUtil: attempt_202409031812257377926298981951163_0002_m_000000_2: Committed
[2024-09-03 18:12:25,670] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:12:25,672] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 215 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:12:25,677] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,247 s
[2024-09-03 18:12:25,678] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:12:25,678] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:12:25,680] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:12:25,682] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,263619 s
[2024-09-03 18:12:25,703] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileFormatWriter: Write Job 5f29e175-a0b0-4e2e-a790-a056c2b96603 committed.
[2024-09-03 18:12:25,703] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO FileFormatWriter: Finished processing stats for write job 5f29e175-a0b0-4e2e-a790-a056c2b96603.
[2024-09-03 18:12:25,779] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:12:25,808] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:12:25,833] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:12:25,843] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:12:25,844] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO BlockManager: BlockManager stopped
[2024-09-03 18:12:25,855] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:12:25,858] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:12:25,874] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:12:25,874] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:12:25,874] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb20909e-6cf5-4f73-8946-856f5112252a
[2024-09-03 18:12:25,878] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb20909e-6cf5-4f73-8946-856f5112252a/pyspark-b3cd38f4-9bd0-471c-8bb8-878db197f57d
[2024-09-03 18:12:25,882] {spark_submit.py:495} INFO - 24/09/03 18:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-28d9c0c4-0725-4eeb-9a54-2c3797c7b9f1
[2024-09-03 18:12:25,979] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240903T211213, end_date=20240903T211225
[2024-09-03 18:12:26,016] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:12:26,030] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:16:26,068] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 18:16:26,076] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 18:16:26,076] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:16:26,076] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:16:26,076] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:16:26,089] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 18:16:26,092] {standard_task_runner.py:52} INFO - Started process 338076 to run task
[2024-09-03 18:16:26,096] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp65zuti_0', '--error-file', '/tmp/tmp8_whicuw']
[2024-09-03 18:16:26,097] {standard_task_runner.py:80} INFO - Job 96: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:16:26,143] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:16:26,206] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 18:16:26,209] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:16:26,210] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 18:16:27,430] {spark_submit.py:495} INFO - 24/09/03 18:16:27 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:16:27,430] {spark_submit.py:495} INFO - 24/09/03 18:16:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:16:27,833] {spark_submit.py:495} INFO - 24/09/03 18:16:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:16:28,473] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:16:28,490] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:16:28,551] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO ResourceUtils: ==============================================================
[2024-09-03 18:16:28,551] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:16:28,551] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO ResourceUtils: ==============================================================
[2024-09-03 18:16:28,552] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:16:28,587] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:16:28,604] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:16:28,604] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:16:28,656] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:16:28,656] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:16:28,656] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:16:28,656] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:16:28,656] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:16:28,866] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO Utils: Successfully started service 'sparkDriver' on port 45653.
[2024-09-03 18:16:28,893] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:16:28,928] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:16:28,949] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:16:28,950] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:16:28,953] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:16:28,967] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f8527f6c-cb47-478b-a031-6f3817ba5a05
[2024-09-03 18:16:28,990] {spark_submit.py:495} INFO - 24/09/03 18:16:28 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:16:29,009] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:16:29,233] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:16:29,291] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:16:29,504] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:16:29,550] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36519.
[2024-09-03 18:16:29,551] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO NettyBlockTransferService: Server created on 192.168.2.128:36519
[2024-09-03 18:16:29,551] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:16:29,559] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 36519, None)
[2024-09-03 18:16:29,565] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:36519 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 36519, None)
[2024-09-03 18:16:29,568] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 36519, None)
[2024-09-03 18:16:29,569] {spark_submit.py:495} INFO - 24/09/03 18:16:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 36519, None)
[2024-09-03 18:16:30,051] {spark_submit.py:495} INFO - 24/09/03 18:16:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:16:30,052] {spark_submit.py:495} INFO - 24/09/03 18:16:30 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:16:31,054] {spark_submit.py:495} INFO - 24/09/03 18:16:31 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
[2024-09-03 18:16:31,139] {spark_submit.py:495} INFO - 24/09/03 18:16:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:16:33,010] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:16:33,013] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:16:33,017] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:16:33,332] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:16:33,381] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:16:33,385] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:36519 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:16:33,400] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:33,410] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198899 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:33,595] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:33,609] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:33,610] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:33,613] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:33,613] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:33,618] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:33,702] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:16:33,705] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:16:33,706] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:36519 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:16:33,707] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:33,720] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:33,721] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:16:33,779] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:33,800] {spark_submit.py:495} INFO - 24/09/03 18:16:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:16:34,122] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4595, partition values: [empty row]
[2024-09-03 18:16:34,415] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO CodeGenerator: Code generated in 188.83566 ms
[2024-09-03 18:16:34,475] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:16:34,492] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 727 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:34,499] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:16:34,504] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,873 s
[2024-09-03 18:16:34,624] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:34,624] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:16:34,629] {spark_submit.py:495} INFO - 24/09/03 18:16:34 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,030306 s
[2024-09-03 18:16:35,080] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:16:35,082] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:16:35,082] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:16:35,156] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:35,156] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:35,157] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:35,237] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO CodeGenerator: Code generated in 28.530212 ms
[2024-09-03 18:16:35,280] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO CodeGenerator: Code generated in 28.204363 ms
[2024-09-03 18:16:35,286] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:16:35,296] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:16:35,296] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:36519 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:16:35,301] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:35,303] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198899 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:35,355] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:35,358] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:35,358] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:35,358] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:35,359] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:35,365] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:35,426] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:16:35,435] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:16:35,436] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:36519 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:16:35,436] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:35,437] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:35,438] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:16:35,445] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:35,446] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:16:35,535] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:35,535] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:35,536] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:35,621] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO CodeGenerator: Code generated in 31.669454 ms
[2024-09-03 18:16:35,625] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4595, partition values: [empty row]
[2024-09-03 18:16:35,659] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO CodeGenerator: Code generated in 26.566112 ms
[2024-09-03 18:16:35,697] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO CodeGenerator: Code generated in 8.132691 ms
[2024-09-03 18:16:35,740] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: Saved output of task 'attempt_20240903181635212774815700861588_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_20240903181635212774815700861588_0001_m_000000
[2024-09-03 18:16:35,740] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkHadoopMapRedUtil: attempt_20240903181635212774815700861588_0001_m_000000_1: Committed
[2024-09-03 18:16:35,746] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:16:35,751] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 310 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:35,759] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:16:35,761] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,392 s
[2024-09-03 18:16:35,761] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:35,761] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:16:35,766] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,409999 s
[2024-09-03 18:16:35,795] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileFormatWriter: Write Job 22463387-e0f7-4b57-8e7f-3b8c93994da9 committed.
[2024-09-03 18:16:35,805] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileFormatWriter: Finished processing stats for write job 22463387-e0f7-4b57-8e7f-3b8c93994da9.
[2024-09-03 18:16:35,867] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:16:35,867] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:16:35,867] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:16:35,881] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:35,882] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:35,882] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:35,913] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO CodeGenerator: Code generated in 8.262354 ms
[2024-09-03 18:16:35,919] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:16:35,926] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:16:35,926] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:36519 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:16:35,930] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:35,932] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198899 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:35,956] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:35,958] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:35,958] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:35,958] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:35,958] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:35,962] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:35,985] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:16:35,987] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:16:35,988] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:36519 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:16:35,989] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:35,990] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:35,990] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:16:35,992] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:35,992] {spark_submit.py:495} INFO - 24/09/03 18:16:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:16:36,012] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:36,013] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:36,013] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:36,061] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO CodeGenerator: Code generated in 8.833505 ms
[2024-09-03 18:16:36,066] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4595, partition values: [empty row]
[2024-09-03 18:16:36,086] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO CodeGenerator: Code generated in 15.717613 ms
[2024-09-03 18:16:36,097] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO FileOutputCommitter: Saved output of task 'attempt_202409031816353184803661519593466_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409031816353184803661519593466_0002_m_000000
[2024-09-03 18:16:36,097] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO SparkHadoopMapRedUtil: attempt_202409031816353184803661519593466_0002_m_000000_2: Committed
[2024-09-03 18:16:36,100] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:16:36,103] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 111 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:36,103] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:16:36,104] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,140 s
[2024-09-03 18:16:36,105] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:36,105] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:16:36,106] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,149081 s
[2024-09-03 18:16:36,137] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO FileFormatWriter: Write Job 7a075fcc-56e8-4d41-bb73-58863d3c2766 committed.
[2024-09-03 18:16:36,138] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO FileFormatWriter: Finished processing stats for write job 7a075fcc-56e8-4d41-bb73-58863d3c2766.
[2024-09-03 18:16:36,182] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:16:36,193] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:16:36,215] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:16:36,225] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:16:36,225] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO BlockManager: BlockManager stopped
[2024-09-03 18:16:36,236] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:16:36,239] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:16:36,253] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:16:36,253] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:16:36,254] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-06919986-2c61-414d-9df7-b92849a3762e
[2024-09-03 18:16:36,257] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-21479887-eec7-498b-8caf-2a4f30111bd9/pyspark-8de63041-a7e6-434f-aecc-34867997336a
[2024-09-03 18:16:36,264] {spark_submit.py:495} INFO - 24/09/03 18:16:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-21479887-eec7-498b-8caf-2a4f30111bd9
[2024-09-03 18:16:36,339] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240903T211626, end_date=20240903T211636
[2024-09-03 18:16:36,390] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:16:36,408] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:22:24,145] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 18:22:24,150] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 18:22:24,151] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:22:24,151] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:22:24,151] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:22:24,163] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 18:22:24,167] {standard_task_runner.py:52} INFO - Started process 344330 to run task
[2024-09-03 18:22:24,170] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpnaarlgbr', '--error-file', '/tmp/tmps2mhrarg']
[2024-09-03 18:22:24,171] {standard_task_runner.py:80} INFO - Job 95: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:22:24,206] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:22:24,255] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 18:22:24,261] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:22:24,262] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 18:22:25,424] {spark_submit.py:495} INFO - 24/09/03 18:22:25 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:22:25,425] {spark_submit.py:495} INFO - 24/09/03 18:22:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:22:25,807] {spark_submit.py:495} INFO - 24/09/03 18:22:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:22:26,359] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:22:26,372] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:22:26,415] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO ResourceUtils: ==============================================================
[2024-09-03 18:22:26,416] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:22:26,416] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO ResourceUtils: ==============================================================
[2024-09-03 18:22:26,416] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:22:26,441] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:22:26,462] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:22:26,463] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:22:26,516] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:22:26,516] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:22:26,516] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:22:26,516] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:22:26,516] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:22:26,708] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO Utils: Successfully started service 'sparkDriver' on port 35095.
[2024-09-03 18:22:26,739] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:22:26,781] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:22:26,801] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:22:26,802] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:22:26,805] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:22:26,820] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-64855ae9-5fbc-4ada-904d-b5e8cb19685f
[2024-09-03 18:22:26,843] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:22:26,863] {spark_submit.py:495} INFO - 24/09/03 18:22:26 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:22:27,171] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:22:27,246] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:22:27,484] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:22:27,510] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37809.
[2024-09-03 18:22:27,511] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO NettyBlockTransferService: Server created on 192.168.2.128:37809
[2024-09-03 18:22:27,516] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:22:27,523] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 37809, None)
[2024-09-03 18:22:27,535] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:37809 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 37809, None)
[2024-09-03 18:22:27,556] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 37809, None)
[2024-09-03 18:22:27,559] {spark_submit.py:495} INFO - 24/09/03 18:22:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 37809, None)
[2024-09-03 18:22:28,173] {spark_submit.py:495} INFO - 24/09/03 18:22:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:22:28,177] {spark_submit.py:495} INFO - 24/09/03 18:22:28 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:22:29,115] {spark_submit.py:495} INFO - 24/09/03 18:22:29 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
[2024-09-03 18:22:29,176] {spark_submit.py:495} INFO - 24/09/03 18:22:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:22:31,095] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:22:31,098] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:22:31,103] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:22:31,391] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:22:31,436] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:22:31,439] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:37809 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:22:31,450] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:31,460] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198824 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:22:31,644] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:31,671] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:22:31,671] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:22:31,671] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:22:31,673] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:22:31,679] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:22:31,824] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:22:31,828] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:22:31,830] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:37809 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:22:31,831] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:22:31,854] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:22:31,856] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:22:31,911] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:22:31,930] {spark_submit.py:495} INFO - 24/09/03 18:22:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:22:32,265] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4520, partition values: [empty row]
[2024-09-03 18:22:32,571] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO CodeGenerator: Code generated in 184.411423 ms
[2024-09-03 18:22:32,645] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2725 bytes result sent to driver
[2024-09-03 18:22:32,664] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 761 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:22:32,670] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:22:32,675] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,977 s
[2024-09-03 18:22:32,833] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:22:32,834] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:22:32,845] {spark_submit.py:495} INFO - 24/09/03 18:22:32 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,196514 s
[2024-09-03 18:22:33,358] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:22:33,361] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:22:33,361] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:22:33,465] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:33,466] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:33,467] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:33,585] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO CodeGenerator: Code generated in 35.915354 ms
[2024-09-03 18:22:33,656] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO CodeGenerator: Code generated in 36.561406 ms
[2024-09-03 18:22:33,666] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:22:33,674] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:22:33,675] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:37809 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:22:33,679] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:33,682] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198824 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:22:33,766] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:33,769] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:22:33,769] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:22:33,769] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:22:33,769] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:22:33,771] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:22:33,838] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:22:33,842] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:22:33,846] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:37809 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:22:33,847] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:22:33,848] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:22:33,849] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:22:33,853] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:22:33,853] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:22:33,924] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:33,924] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:33,925] {spark_submit.py:495} INFO - 24/09/03 18:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:34,038] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO CodeGenerator: Code generated in 36.534428 ms
[2024-09-03 18:22:34,042] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4520, partition values: [empty row]
[2024-09-03 18:22:34,089] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO CodeGenerator: Code generated in 32.937976 ms
[2024-09-03 18:22:34,122] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO CodeGenerator: Code generated in 6.364004 ms
[2024-09-03 18:22:34,171] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409031822331270419764108330295_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_202409031822331270419764108330295_0001_m_000000
[2024-09-03 18:22:34,172] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkHadoopMapRedUtil: attempt_202409031822331270419764108330295_0001_m_000000_1: Committed
[2024-09-03 18:22:34,178] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:22:34,189] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 337 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:22:34,189] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:22:34,189] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,416 s
[2024-09-03 18:22:34,189] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:22:34,189] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:22:34,190] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,423056 s
[2024-09-03 18:22:34,208] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileFormatWriter: Write Job 6c0d7547-2e62-4aa2-beae-2ce707c6bbb8 committed.
[2024-09-03 18:22:34,211] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileFormatWriter: Finished processing stats for write job 6c0d7547-2e62-4aa2-beae-2ce707c6bbb8.
[2024-09-03 18:22:34,288] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:22:34,289] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:22:34,289] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:22:34,319] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:34,319] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:34,320] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:34,366] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO CodeGenerator: Code generated in 15.138643 ms
[2024-09-03 18:22:34,371] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:22:34,382] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:22:34,384] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:37809 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:22:34,385] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:34,386] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198824 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:22:34,403] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:34,404] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:22:34,404] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:22:34,404] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:22:34,404] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:22:34,406] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:22:34,429] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:22:34,433] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:22:34,434] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:37809 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:22:34,435] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:22:34,436] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:22:34,436] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:22:34,438] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:22:34,442] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:22:34,453] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:34,453] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:34,454] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:34,487] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO CodeGenerator: Code generated in 11.061762 ms
[2024-09-03 18:22:34,490] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-4520, partition values: [empty row]
[2024-09-03 18:22:34,504] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO CodeGenerator: Code generated in 11.84066 ms
[2024-09-03 18:22:34,515] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileOutputCommitter: Saved output of task 'attempt_202409031822344038247606799667550_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409031822344038247606799667550_0002_m_000000
[2024-09-03 18:22:34,515] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkHadoopMapRedUtil: attempt_202409031822344038247606799667550_0002_m_000000_2: Committed
[2024-09-03 18:22:34,518] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:22:34,520] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 83 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:22:34,521] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,115 s
[2024-09-03 18:22:34,522] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:22:34,522] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:22:34,522] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:22:34,523] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,119829 s
[2024-09-03 18:22:34,553] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileFormatWriter: Write Job 93195a59-89b7-443a-9e7d-ca5cdf188ce1 committed.
[2024-09-03 18:22:34,554] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO FileFormatWriter: Finished processing stats for write job 93195a59-89b7-443a-9e7d-ca5cdf188ce1.
[2024-09-03 18:22:34,590] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:22:34,605] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:22:34,623] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:22:34,636] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:22:34,637] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO BlockManager: BlockManager stopped
[2024-09-03 18:22:34,664] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:22:34,669] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:22:34,676] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:22:34,676] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:22:34,676] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1d38925-f9da-4fab-98b5-8bdb5819efef
[2024-09-03 18:22:34,681] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-72172fa0-dc6e-4985-bfcc-20f98ba6761c
[2024-09-03 18:22:34,685] {spark_submit.py:495} INFO - 24/09/03 18:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1d38925-f9da-4fab-98b5-8bdb5819efef/pyspark-750b005e-ab33-497b-ab6b-78480fa3b622
[2024-09-03 18:22:34,772] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240903T212224, end_date=20240903T212234
[2024-09-03 18:22:34,807] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:22:34,824] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:44:06,408] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 21:44:06,413] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 21:44:06,414] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:44:06,414] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:44:06,414] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:44:06,427] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 21:44:06,430] {standard_task_runner.py:52} INFO - Started process 418508 to run task
[2024-09-03 21:44:06,434] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp41qj_vcf', '--error-file', '/tmp/tmpg061p4xf']
[2024-09-03 21:44:06,435] {standard_task_runner.py:80} INFO - Job 93: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:44:06,465] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:44:06,510] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 21:44:06,513] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:44:06,515] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 21:44:07,569] {spark_submit.py:495} INFO - 24/09/03 21:44:07 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:44:07,570] {spark_submit.py:495} INFO - 24/09/03 21:44:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:44:07,898] {spark_submit.py:495} INFO - 24/09/03 21:44:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:44:08,496] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:44:08,507] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:44:08,548] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO ResourceUtils: ==============================================================
[2024-09-03 21:44:08,550] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:44:08,550] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO ResourceUtils: ==============================================================
[2024-09-03 21:44:08,550] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:44:08,568] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:44:08,578] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:44:08,578] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:44:08,613] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:44:08,613] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:44:08,613] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:44:08,613] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:44:08,613] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:44:08,772] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO Utils: Successfully started service 'sparkDriver' on port 42223.
[2024-09-03 21:44:08,793] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:44:08,816] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:44:08,831] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:44:08,831] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:44:08,835] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:44:08,844] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1397f961-6a5e-4516-a11c-5814fbb517ec
[2024-09-03 21:44:08,861] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:44:08,874] {spark_submit.py:495} INFO - 24/09/03 21:44:08 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:44:09,058] {spark_submit.py:495} INFO - 24/09/03 21:44:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:44:09,074] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:44:09,141] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:44:09,378] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:44:09,399] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44597.
[2024-09-03 21:44:09,399] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO NettyBlockTransferService: Server created on 192.168.2.128:44597
[2024-09-03 21:44:09,401] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:44:09,405] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 44597, None)
[2024-09-03 21:44:09,408] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:44597 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 44597, None)
[2024-09-03 21:44:09,410] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 44597, None)
[2024-09-03 21:44:09,411] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 44597, None)
[2024-09-03 21:44:09,874] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:44:09,874] {spark_submit.py:495} INFO - 24/09/03 21:44:09 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:44:10,622] {spark_submit.py:495} INFO - 24/09/03 21:44:10 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
[2024-09-03 21:44:10,673] {spark_submit.py:495} INFO - 24/09/03 21:44:10 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:44:12,577] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:44:12,577] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:44:12,580] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:44:12,864] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:44:12,910] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:44:12,913] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:44597 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:44:12,938] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:44:12,960] {spark_submit.py:495} INFO - 24/09/03 21:44:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648799 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:44:13,143] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:44:13,159] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:44:13,159] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:44:13,160] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:44:13,161] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:44:13,167] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:44:13,260] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:44:13,263] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:44:13,263] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:44597 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:44:13,264] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:44:13,278] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:44:13,278] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:44:13,326] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:44:13,344] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:44:13,595] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-454495, partition values: [empty row]
[2024-09-03 21:44:13,851] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO CodeGenerator: Code generated in 160.264334 ms
[2024-09-03 21:44:13,934] {spark_submit.py:495} INFO - 24/09/03 21:44:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 21:44:14,070] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 639 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:44:14,073] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:44:14,080] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,900 s
[2024-09-03 21:44:14,089] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:44:14,090] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:44:14,095] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,949463 s
[2024-09-03 21:44:14,524] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:44:14,525] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:44:14,525] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:44:14,614] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:44:14,615] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:44:14,615] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:44:14,688] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO CodeGenerator: Code generated in 24.313053 ms
[2024-09-03 21:44:14,727] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO CodeGenerator: Code generated in 26.873915 ms
[2024-09-03 21:44:14,734] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:44:14,741] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:44:14,742] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:44597 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:44:14,742] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:44:14,745] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648799 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:44:14,804] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:44:14,806] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:44:14,809] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:44:14,809] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:44:14,809] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:44:14,809] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:44:14,860] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:44:14,862] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:44:14,862] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:44597 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:44:14,863] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:44:14,863] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:44:14,863] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:44:14,868] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:44:14,869] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:44:14,928] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:44:14,928] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:44:14,929] {spark_submit.py:495} INFO - 24/09/03 21:44:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:44:15,008] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO CodeGenerator: Code generated in 31.510156 ms
[2024-09-03 21:44:15,015] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-454495, partition values: [empty row]
[2024-09-03 21:44:15,050] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO CodeGenerator: Code generated in 28.897943 ms
[2024-09-03 21:44:15,084] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO CodeGenerator: Code generated in 8.042215 ms
[2024-09-03 21:44:15,256] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileOutputCommitter: Saved output of task 'attempt_20240903214414523816524817458744_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_20240903214414523816524817458744_0001_m_000000
[2024-09-03 21:44:15,257] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkHadoopMapRedUtil: attempt_20240903214414523816524817458744_0001_m_000000_1: Committed
[2024-09-03 21:44:15,264] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:44:15,278] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 412 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:44:15,278] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:44:15,280] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,470 s
[2024-09-03 21:44:15,287] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:44:15,289] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:44:15,291] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,486834 s
[2024-09-03 21:44:15,337] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileFormatWriter: Write Job 0bef8599-ecdc-45fa-9aad-13b14f25081e committed.
[2024-09-03 21:44:15,348] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileFormatWriter: Finished processing stats for write job 0bef8599-ecdc-45fa-9aad-13b14f25081e.
[2024-09-03 21:44:15,431] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:44:15,432] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:44:15,441] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:44:15,460] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:44:15,462] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:44:15,463] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:44:15,557] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO CodeGenerator: Code generated in 16.568158 ms
[2024-09-03 21:44:15,572] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:44:15,581] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:44:15,583] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:44597 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:44:15,595] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:44:15,595] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4648799 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:44:15,626] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:44:15,626] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:44:15,628] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:44:15,628] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:44:15,628] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:44:15,629] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:44:15,696] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:44:15,744] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:44:15,756] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:44597 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:44:15,759] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:44:15,759] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:44:15,759] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:44:15,759] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:44:15,764] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:44:15,778] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:44:15,779] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:44:15,779] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:44:15,788] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.2.128:44597 in memory (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:44:15,807] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.128:44597 in memory (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:44:15,845] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO CodeGenerator: Code generated in 8.15631 ms
[2024-09-03 21:44:15,847] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-454495, partition values: [empty row]
[2024-09-03 21:44:15,870] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO CodeGenerator: Code generated in 19.273947 ms
[2024-09-03 21:44:15,903] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileOutputCommitter: Saved output of task 'attempt_202409032144157716421683293268747_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409032144157716421683293268747_0002_m_000000
[2024-09-03 21:44:15,904] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkHadoopMapRedUtil: attempt_202409032144157716421683293268747_0002_m_000000_2: Committed
[2024-09-03 21:44:15,904] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:44:15,907] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 148 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:44:15,908] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:44:15,908] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,276 s
[2024-09-03 21:44:15,908] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:44:15,908] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:44:15,910] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,284649 s
[2024-09-03 21:44:15,939] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileFormatWriter: Write Job b3445d56-9a53-4ddc-b384-e3fbdbed8745 committed.
[2024-09-03 21:44:15,939] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO FileFormatWriter: Finished processing stats for write job b3445d56-9a53-4ddc-b384-e3fbdbed8745.
[2024-09-03 21:44:15,982] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:44:15,989] {spark_submit.py:495} INFO - 24/09/03 21:44:15 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:44:16,001] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:44:16,012] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:44:16,013] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO BlockManager: BlockManager stopped
[2024-09-03 21:44:16,020] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:44:16,022] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:44:16,027] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:44:16,027] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:44:16,028] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-1e20ce18-4afe-4821-b4f9-943623ba14ff/pyspark-a25604d0-580e-41a5-8cd0-f1696456ef95
[2024-09-03 21:44:16,034] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-4e296f8a-0718-48be-b9ed-a0aa9a85e90c
[2024-09-03 21:44:16,036] {spark_submit.py:495} INFO - 24/09/03 21:44:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-1e20ce18-4afe-4821-b4f9-943623ba14ff
[2024-09-03 21:44:16,092] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240904T004406, end_date=20240904T004416
[2024-09-03 21:44:16,105] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:44:16,121] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:52:29,714] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 22:52:29,720] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [queued]>
[2024-09-03 22:52:29,720] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:52:29,721] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:52:29,721] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:52:29,733] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-26 00:00:00+00:00
[2024-09-03 22:52:29,736] {standard_task_runner.py:52} INFO - Started process 451505 to run task
[2024-09-03 22:52:29,740] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-26T00:00:00+00:00', '--job-id', '139', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2au8pdbt', '--error-file', '/tmp/tmpfhyo5kgy']
[2024-09-03 22:52:29,740] {standard_task_runner.py:80} INFO - Job 139: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:52:29,783] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-26T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:52:29,829] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-26T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-26T00:00:00+00:00
[2024-09-03 22:52:29,832] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:52:29,833] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240826
[2024-09-03 22:52:30,945] {spark_submit.py:495} INFO - 24/09/03 22:52:30 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:52:30,945] {spark_submit.py:495} INFO - 24/09/03 22:52:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:52:31,277] {spark_submit.py:495} INFO - 24/09/03 22:52:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:52:31,809] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:52:31,817] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:52:31,852] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO ResourceUtils: ==============================================================
[2024-09-03 22:52:31,853] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:52:31,853] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO ResourceUtils: ==============================================================
[2024-09-03 22:52:31,853] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:52:31,871] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:52:31,882] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:52:31,883] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:52:31,924] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:52:31,924] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:52:31,924] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:52:31,924] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:52:31,924] {spark_submit.py:495} INFO - 24/09/03 22:52:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:52:32,073] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO Utils: Successfully started service 'sparkDriver' on port 45843.
[2024-09-03 22:52:32,095] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:52:32,124] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:52:32,139] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:52:32,139] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:52:32,143] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:52:32,154] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ea4654ed-04dd-44a6-90d0-ddff7ece1b60
[2024-09-03 22:52:32,173] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:52:32,188] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:52:32,370] {spark_submit.py:495} INFO - 24/09/03 22:52:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:52:32,377] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:52:32,439] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:52:32,612] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:52:32,639] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33679.
[2024-09-03 22:52:32,640] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO NettyBlockTransferService: Server created on 192.168.2.128:33679
[2024-09-03 22:52:32,642] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:52:32,647] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 33679, None)
[2024-09-03 22:52:32,655] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:33679 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 33679, None)
[2024-09-03 22:52:32,657] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 33679, None)
[2024-09-03 22:52:32,659] {spark_submit.py:495} INFO - 24/09/03 22:52:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 33679, None)
[2024-09-03 22:52:33,129] {spark_submit.py:495} INFO - 24/09/03 22:52:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:52:33,130] {spark_submit.py:495} INFO - 24/09/03 22:52:33 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:52:34,003] {spark_submit.py:495} INFO - 24/09/03 22:52:34 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.
[2024-09-03 22:52:34,066] {spark_submit.py:495} INFO - 24/09/03 22:52:34 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:52:35,864] {spark_submit.py:495} INFO - 24/09/03 22:52:35 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:52:35,865] {spark_submit.py:495} INFO - 24/09/03 22:52:35 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:52:35,868] {spark_submit.py:495} INFO - 24/09/03 22:52:35 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:52:36,130] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:52:36,182] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:52:36,185] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:33679 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:52:36,191] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:36,199] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649671 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:52:36,369] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:36,389] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:52:36,389] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:52:36,390] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:52:36,391] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:52:36,397] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:52:36,485] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:52:36,490] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:52:36,491] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:33679 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:52:36,491] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:52:36,510] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:52:36,510] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:52:36,560] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:52:36,581] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:52:36,868] {spark_submit.py:495} INFO - 24/09/03 22:52:36 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-455367, partition values: [empty row]
[2024-09-03 22:52:37,122] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO CodeGenerator: Code generated in 154.411119 ms
[2024-09-03 22:52:37,210] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:52:37,331] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 681 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:52:37,335] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:52:37,339] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,922 s
[2024-09-03 22:52:37,342] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:52:37,342] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:52:37,345] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,975022 s
[2024-09-03 22:52:37,757] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:52:37,759] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:52:37,759] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:52:37,831] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:37,832] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:37,833] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:37,928] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO CodeGenerator: Code generated in 43.31957 ms
[2024-09-03 22:52:37,995] {spark_submit.py:495} INFO - 24/09/03 22:52:37 INFO CodeGenerator: Code generated in 35.589561 ms
[2024-09-03 22:52:38,002] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:52:38,010] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:52:38,012] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:33679 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:52:38,013] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:38,015] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649671 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:52:38,081] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:38,083] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:52:38,084] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:52:38,084] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:52:38,084] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:52:38,090] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:52:38,143] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:52:38,144] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:52:38,145] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:33679 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:52:38,146] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:52:38,146] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:52:38,147] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:52:38,153] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:52:38,154] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:52:38,208] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:38,209] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:38,209] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:38,274] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO CodeGenerator: Code generated in 25.997338 ms
[2024-09-03 22:52:38,277] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-455367, partition values: [empty row]
[2024-09-03 22:52:38,307] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO CodeGenerator: Code generated in 26.892617 ms
[2024-09-03 22:52:38,333] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO CodeGenerator: Code generated in 5.26988 ms
[2024-09-03 22:52:38,454] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: Saved output of task 'attempt_202409032252388626156898595220863_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240826/_temporary/0/task_202409032252388626156898595220863_0001_m_000000
[2024-09-03 22:52:38,455] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkHadoopMapRedUtil: attempt_202409032252388626156898595220863_0001_m_000000_1: Committed
[2024-09-03 22:52:38,459] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:52:38,463] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 315 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:52:38,465] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,373 s
[2024-09-03 22:52:38,465] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:52:38,469] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:52:38,471] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:52:38,472] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,390712 s
[2024-09-03 22:52:38,488] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileFormatWriter: Write Job 8f395155-b329-4def-aa3c-67c9e50d0e90 committed.
[2024-09-03 22:52:38,493] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileFormatWriter: Finished processing stats for write job 8f395155-b329-4def-aa3c-67c9e50d0e90.
[2024-09-03 22:52:38,549] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:52:38,549] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:52:38,550] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:52:38,565] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:38,565] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:38,565] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:38,604] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO CodeGenerator: Code generated in 10.699218 ms
[2024-09-03 22:52:38,608] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:52:38,615] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:52:38,616] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:33679 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:52:38,617] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:38,618] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649671 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:52:38,637] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:38,639] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:52:38,639] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:52:38,640] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:52:38,640] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:52:38,641] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:52:38,672] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:52:38,678] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:52:38,680] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:33679 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:52:38,683] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:52:38,685] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:52:38,687] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:52:38,689] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:52:38,689] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:52:38,703] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:38,703] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:38,703] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:38,755] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO CodeGenerator: Code generated in 10.088538 ms
[2024-09-03 22:52:38,757] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-26T00:00:00.00Z/data_engineer_20240826.json, range: 0-455367, partition values: [empty row]
[2024-09-03 22:52:38,773] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO CodeGenerator: Code generated in 12.248501 ms
[2024-09-03 22:52:38,807] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileOutputCommitter: Saved output of task 'attempt_202409032252383130814276430510361_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240826/_temporary/0/task_202409032252383130814276430510361_0002_m_000000
[2024-09-03 22:52:38,808] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkHadoopMapRedUtil: attempt_202409032252383130814276430510361_0002_m_000000_2: Committed
[2024-09-03 22:52:38,809] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:52:38,811] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 122 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:52:38,811] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:52:38,813] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,169 s
[2024-09-03 22:52:38,813] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:52:38,813] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:52:38,813] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,174952 s
[2024-09-03 22:52:38,825] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileFormatWriter: Write Job f64c7407-1876-4fa2-b8be-cfb8338d6846 committed.
[2024-09-03 22:52:38,825] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO FileFormatWriter: Finished processing stats for write job f64c7407-1876-4fa2-b8be-cfb8338d6846.
[2024-09-03 22:52:38,861] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:52:38,870] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:52:38,903] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:52:38,915] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:52:38,915] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO BlockManager: BlockManager stopped
[2024-09-03 22:52:38,922] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:52:38,925] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:52:38,929] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:52:38,930] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:52:38,930] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-e88cafdb-e1cb-431a-bf50-1759e458e5d6
[2024-09-03 22:52:38,933] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-e88cafdb-e1cb-431a-bf50-1759e458e5d6/pyspark-0622857f-5617-4bf8-aca3-7c1f8764ff8d
[2024-09-03 22:52:38,938] {spark_submit.py:495} INFO - 24/09/03 22:52:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ff81f10-0a7c-46bf-81d6-1e863230fdac
[2024-09-03 22:52:39,054] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240826T000000, start_date=20240904T015229, end_date=20240904T015239
[2024-09-03 22:52:39,077] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:52:39,092] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
