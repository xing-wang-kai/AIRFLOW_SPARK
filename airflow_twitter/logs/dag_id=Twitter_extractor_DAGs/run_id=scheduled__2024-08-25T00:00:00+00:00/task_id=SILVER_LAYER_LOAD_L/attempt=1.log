[2024-09-03 16:23:15,159] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 16:23:15,167] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 16:23:15,167] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:23:15,167] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:23:15,167] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:23:15,180] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 16:23:15,183] {standard_task_runner.py:52} INFO - Started process 284425 to run task
[2024-09-03 16:23:15,188] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmppm3k8lab', '--error-file', '/tmp/tmpp5sne621']
[2024-09-03 16:23:15,189] {standard_task_runner.py:80} INFO - Job 88: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:23:15,230] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:23:15,278] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 16:23:15,283] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:23:15,284] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/ --process-data 20240825
[2024-09-03 16:23:16,285] {spark_submit.py:495} INFO - 24/09/03 16:23:16 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:23:16,286] {spark_submit.py:495} INFO - 24/09/03 16:23:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:23:16,637] {spark_submit.py:495} INFO - 24/09/03 16:23:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:23:17,127] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:23:17,135] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:23:17,170] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO ResourceUtils: ==============================================================
[2024-09-03 16:23:17,170] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:23:17,170] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO ResourceUtils: ==============================================================
[2024-09-03 16:23:17,170] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:23:17,189] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:23:17,203] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:23:17,203] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:23:17,243] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:23:17,244] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:23:17,244] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:23:17,244] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:23:17,244] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:23:17,385] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO Utils: Successfully started service 'sparkDriver' on port 33677.
[2024-09-03 16:23:17,406] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:23:17,433] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:23:17,447] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:23:17,448] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:23:17,451] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:23:17,461] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3e470714-4e47-4a66-be6d-35f4ab8bff3e
[2024-09-03 16:23:17,480] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:23:17,500] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:23:17,671] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 16:23:17,715] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 16:23:17,887] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:23:17,907] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38575.
[2024-09-03 16:23:17,907] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO NettyBlockTransferService: Server created on 192.168.2.128:38575
[2024-09-03 16:23:17,909] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:23:17,913] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 38575, None)
[2024-09-03 16:23:17,917] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:38575 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 38575, None)
[2024-09-03 16:23:17,918] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 38575, None)
[2024-09-03 16:23:17,919] {spark_submit.py:495} INFO - 24/09/03 16:23:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 38575, None)
[2024-09-03 16:23:18,280] {spark_submit.py:495} INFO - 24/09/03 16:23:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:23:18,281] {spark_submit.py:495} INFO - 24/09/03 16:23:18 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:23:19,066] {spark_submit.py:495} INFO - 24/09/03 16:23:19 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.
[2024-09-03 16:23:19,142] {spark_submit.py:495} INFO - 24/09/03 16:23:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 16:23:21,314] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:23:21,314] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:23:21,318] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:23:21,598] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:23:21,646] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:23:21,648] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:38575 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:23:21,655] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:21,663] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649535 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:23:21,825] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:21,842] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:23:21,843] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:23:21,843] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:23:21,844] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:23:21,856] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:23:21,963] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:23:21,970] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:23:21,971] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:38575 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:23:21,972] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:23:21,986] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:23:21,987] {spark_submit.py:495} INFO - 24/09/03 16:23:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:23:22,057] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4967 bytes) taskResourceAssignments Map()
[2024-09-03 16:23:22,077] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:23:22,362] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455231, partition values: [empty row]
[2024-09-03 16:23:22,596] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO CodeGenerator: Code generated in 144.140007 ms
[2024-09-03 16:23:22,680] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 16:23:22,796] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 642 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:23:22,797] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:23:22,805] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,925 s
[2024-09-03 16:23:22,809] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:23:22,811] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:23:22,817] {spark_submit.py:495} INFO - 24/09/03 16:23:22 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,989219 s
[2024-09-03 16:23:23,198] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:23:23,199] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:23:23,200] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:23:23,260] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:23,260] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:23,261] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:23,331] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO CodeGenerator: Code generated in 23.493465 ms
[2024-09-03 16:23:23,367] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO CodeGenerator: Code generated in 23.295282 ms
[2024-09-03 16:23:23,372] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:23:23,379] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:23:23,380] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:38575 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:23:23,381] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:23,383] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649535 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:23:23,446] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:23,447] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:23:23,448] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:23:23,448] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:23:23,448] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:23:23,455] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:23:23,506] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.0 KiB, free 365.5 MiB)
[2024-09-03 16:23:23,509] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:23:23,509] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:38575 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:23:23,510] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:23:23,511] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:23:23,511] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:23:23,514] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:23:23,515] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:23:23,600] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:23,601] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:23,603] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:23,680] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO CodeGenerator: Code generated in 20.735774 ms
[2024-09-03 16:23:23,687] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455231, partition values: [empty row]
[2024-09-03 16:23:23,715] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO CodeGenerator: Code generated in 24.997668 ms
[2024-09-03 16:23:23,743] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO CodeGenerator: Code generated in 8.491904 ms
[2024-09-03 16:23:23,876] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: Saved output of task 'attempt_20240903162323740720449186048963_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/twitter_df/process_data=20240825/_temporary/0/task_20240903162323740720449186048963_0001_m_000000
[2024-09-03 16:23:23,877] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SparkHadoopMapRedUtil: attempt_20240903162323740720449186048963_0001_m_000000_1: Committed
[2024-09-03 16:23:23,889] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:23:23,892] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 381 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:23:23,892] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:23:23,895] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,437 s
[2024-09-03 16:23:23,895] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:23:23,895] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:23:23,895] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,448317 s
[2024-09-03 16:23:23,917] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileFormatWriter: Write Job a0c41008-0b24-4923-a67f-70f5ed24cf3a committed.
[2024-09-03 16:23:23,921] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileFormatWriter: Finished processing stats for write job a0c41008-0b24-4923-a67f-70f5ed24cf3a.
[2024-09-03 16:23:23,958] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:23:23,959] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:23:23,959] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:23:23,967] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:23,967] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:23,968] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:23,994] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO CodeGenerator: Code generated in 10.698644 ms
[2024-09-03 16:23:23,997] {spark_submit.py:495} INFO - 24/09/03 16:23:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:23:24,004] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:23:24,005] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:38575 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:23:24,006] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:24,008] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649535 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:23:24,026] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:23:24,027] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:23:24,028] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:23:24,028] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:23:24,028] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:23:24,029] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:23:24,046] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:23:24,048] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 63.9 KiB, free 364.8 MiB)
[2024-09-03 16:23:24,048] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:38575 (size: 63.9 KiB, free: 366.1 MiB)
[2024-09-03 16:23:24,049] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:23:24,050] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:23:24,050] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:23:24,051] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5196 bytes) taskResourceAssignments Map()
[2024-09-03 16:23:24,051] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:23:24,063] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:23:24,064] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:23:24,064] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:23:24,095] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO CodeGenerator: Code generated in 9.312324 ms
[2024-09-03 16:23:24,098] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455231, partition values: [empty row]
[2024-09-03 16:23:24,111] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO CodeGenerator: Code generated in 10.302675 ms
[2024-09-03 16:23:24,161] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileOutputCommitter: Saved output of task 'attempt_202409031623244263086019638649634_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/user_df/process_data=20240825/_temporary/0/task_202409031623244263086019638649634_0002_m_000000
[2024-09-03 16:23:24,162] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkHadoopMapRedUtil: attempt_202409031623244263086019638649634_0002_m_000000_2: Committed
[2024-09-03 16:23:24,163] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:23:24,164] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 114 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:23:24,165] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,136 s
[2024-09-03 16:23:24,166] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:23:24,166] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:23:24,167] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:23:24,167] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,138809 s
[2024-09-03 16:23:24,192] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileFormatWriter: Write Job 1bba7bc6-04e6-49cf-8042-cb61e8faf4fe committed.
[2024-09-03 16:23:24,193] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO FileFormatWriter: Finished processing stats for write job 1bba7bc6-04e6-49cf-8042-cb61e8faf4fe.
[2024-09-03 16:23:24,237] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:23:24,247] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 16:23:24,260] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:23:24,273] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:23:24,274] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO BlockManager: BlockManager stopped
[2024-09-03 16:23:24,281] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:23:24,285] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:23:24,291] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:23:24,292] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:23:24,294] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-1675a38b-79cf-4db4-a454-e74140a434b7/pyspark-fbd6f094-8fb5-4bf0-acd2-54b55db04233
[2024-09-03 16:23:24,298] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-1675a38b-79cf-4db4-a454-e74140a434b7
[2024-09-03 16:23:24,303] {spark_submit.py:495} INFO - 24/09/03 16:23:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-a1598d42-5b5d-4a5e-9ae2-884e5e69b52f
[2024-09-03 16:23:24,366] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240903T192315, end_date=20240903T192324
[2024-09-03 16:23:24,421] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:23:24,430] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 16:41:21,363] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 16:41:21,369] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 16:41:21,369] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:41:21,369] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 16:41:21,369] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 16:41:21,379] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 16:41:21,382] {standard_task_runner.py:52} INFO - Started process 295169 to run task
[2024-09-03 16:41:21,385] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpmh7p2jig', '--error-file', '/tmp/tmp4cs0ngau']
[2024-09-03 16:41:21,386] {standard_task_runner.py:80} INFO - Job 88: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 16:41:21,421] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 16:41:21,462] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 16:41:21,465] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 16:41:21,468] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/notebooks/transforms_func.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 16:41:22,485] {spark_submit.py:495} INFO - 24/09/03 16:41:22 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 16:41:22,485] {spark_submit.py:495} INFO - 24/09/03 16:41:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 16:41:22,829] {spark_submit.py:495} INFO - 24/09/03 16:41:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 16:41:23,315] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 16:41:23,323] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 16:41:23,358] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO ResourceUtils: ==============================================================
[2024-09-03 16:41:23,358] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 16:41:23,359] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO ResourceUtils: ==============================================================
[2024-09-03 16:41:23,359] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 16:41:23,379] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 16:41:23,394] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 16:41:23,395] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 16:41:23,440] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 16:41:23,441] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 16:41:23,441] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 16:41:23,441] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 16:41:23,441] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 16:41:23,587] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO Utils: Successfully started service 'sparkDriver' on port 36545.
[2024-09-03 16:41:23,608] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 16:41:23,634] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 16:41:23,650] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 16:41:23,651] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 16:41:23,654] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 16:41:23,664] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-03417793-78df-4b3c-8562-04fadba37cf4
[2024-09-03 16:41:23,683] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 16:41:23,696] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 16:41:23,881] {spark_submit.py:495} INFO - 24/09/03 16:41:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 16:41:23,888] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 16:41:23,931] {spark_submit.py:495} INFO - 24/09/03 16:41:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 16:41:24,126] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 16:41:24,147] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43137.
[2024-09-03 16:41:24,147] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO NettyBlockTransferService: Server created on 192.168.2.128:43137
[2024-09-03 16:41:24,148] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 16:41:24,155] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 43137, None)
[2024-09-03 16:41:24,157] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:43137 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 43137, None)
[2024-09-03 16:41:24,160] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 43137, None)
[2024-09-03 16:41:24,162] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 43137, None)
[2024-09-03 16:41:24,589] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 16:41:24,589] {spark_submit.py:495} INFO - 24/09/03 16:41:24 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 16:41:25,327] {spark_submit.py:495} INFO - 24/09/03 16:41:25 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 16:41:25,382] {spark_submit.py:495} INFO - 24/09/03 16:41:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 16:41:26,961] {spark_submit.py:495} INFO - 24/09/03 16:41:26 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:41:26,962] {spark_submit.py:495} INFO - 24/09/03 16:41:26 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:41:26,964] {spark_submit.py:495} INFO - 24/09/03 16:41:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 16:41:27,198] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 16:41:27,239] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 16:41:27,242] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:43137 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 16:41:27,248] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:27,254] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649654 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:41:27,390] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:27,408] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:41:27,408] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:41:27,409] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:41:27,409] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:41:27,417] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:41:27,516] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 16:41:27,519] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 16:41:27,520] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:43137 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 16:41:27,521] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:41:27,547] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:41:27,547] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 16:41:27,591] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 16:41:27,603] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 16:41:27,847] {spark_submit.py:495} INFO - 24/09/03 16:41:27 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455350, partition values: [empty row]
[2024-09-03 16:41:28,063] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO CodeGenerator: Code generated in 128.40443 ms
[2024-09-03 16:41:28,147] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 16:41:28,258] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 587 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:41:28,260] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 16:41:28,265] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,835 s
[2024-09-03 16:41:28,275] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:41:28,276] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 16:41:28,285] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,888792 s
[2024-09-03 16:41:28,678] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 16:41:28,679] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 16:41:28,679] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 16:41:28,745] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:28,745] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:28,746] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:28,815] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO CodeGenerator: Code generated in 22.872079 ms
[2024-09-03 16:41:28,854] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO CodeGenerator: Code generated in 25.661324 ms
[2024-09-03 16:41:28,860] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 16:41:28,870] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 16:41:28,872] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:43137 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 16:41:28,873] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:28,875] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649654 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:41:28,926] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:28,928] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:41:28,928] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:41:28,928] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:41:28,929] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:41:28,930] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:41:28,971] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 16:41:28,973] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 16:41:28,973] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:43137 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 16:41:28,974] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:41:28,974] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:41:28,975] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 16:41:28,977] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:41:28,979] {spark_submit.py:495} INFO - 24/09/03 16:41:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 16:41:29,033] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:29,033] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:29,034] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:29,105] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO CodeGenerator: Code generated in 22.400171 ms
[2024-09-03 16:41:29,108] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455350, partition values: [empty row]
[2024-09-03 16:41:29,127] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO CodeGenerator: Code generated in 16.340147 ms
[2024-09-03 16:41:29,159] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO CodeGenerator: Code generated in 6.901751 ms
[2024-09-03 16:41:29,254] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409031641287129696996543843391_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_202409031641287129696996543843391_0001_m_000000
[2024-09-03 16:41:29,254] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkHadoopMapRedUtil: attempt_202409031641287129696996543843391_0001_m_000000_1: Committed
[2024-09-03 16:41:29,260] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 16:41:29,267] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 291 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:41:29,267] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 16:41:29,268] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,338 s
[2024-09-03 16:41:29,268] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:41:29,269] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 16:41:29,271] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,343107 s
[2024-09-03 16:41:29,284] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileFormatWriter: Write Job 1cdc240a-e8cd-438d-8971-a7003b148e2b committed.
[2024-09-03 16:41:29,287] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileFormatWriter: Finished processing stats for write job 1cdc240a-e8cd-438d-8971-a7003b148e2b.
[2024-09-03 16:41:29,329] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 16:41:29,330] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 16:41:29,331] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 16:41:29,341] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:29,342] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:29,343] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:29,380] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO CodeGenerator: Code generated in 13.790651 ms
[2024-09-03 16:41:29,385] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 16:41:29,395] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 16:41:29,397] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:43137 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 16:41:29,398] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:29,400] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649654 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 16:41:29,428] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 16:41:29,429] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 16:41:29,429] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 16:41:29,429] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 16:41:29,429] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Missing parents: List()
[2024-09-03 16:41:29,430] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 16:41:29,447] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 16:41:29,451] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 16:41:29,455] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:43137 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 16:41:29,455] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 16:41:29,455] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 16:41:29,456] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 16:41:29,456] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 16:41:29,456] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 16:41:29,466] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 16:41:29,466] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 16:41:29,467] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 16:41:29,497] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO CodeGenerator: Code generated in 10.478652 ms
[2024-09-03 16:41:29,501] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455350, partition values: [empty row]
[2024-09-03 16:41:29,514] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO CodeGenerator: Code generated in 11.024261 ms
[2024-09-03 16:41:29,547] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileOutputCommitter: Saved output of task 'attempt_202409031641296475650361815112832_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409031641296475650361815112832_0002_m_000000
[2024-09-03 16:41:29,547] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkHadoopMapRedUtil: attempt_202409031641296475650361815112832_0002_m_000000_2: Committed
[2024-09-03 16:41:29,548] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 16:41:29,554] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 99 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 16:41:29,554] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 16:41:29,555] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,124 s
[2024-09-03 16:41:29,556] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 16:41:29,556] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 16:41:29,557] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,128303 s
[2024-09-03 16:41:29,577] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileFormatWriter: Write Job 14055e77-2b5f-4843-a887-51a1ee3cbbcb committed.
[2024-09-03 16:41:29,577] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO FileFormatWriter: Finished processing stats for write job 14055e77-2b5f-4843-a887-51a1ee3cbbcb.
[2024-09-03 16:41:29,613] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 16:41:29,624] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 16:41:29,639] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 16:41:29,647] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO MemoryStore: MemoryStore cleared
[2024-09-03 16:41:29,647] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO BlockManager: BlockManager stopped
[2024-09-03 16:41:29,654] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 16:41:29,658] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 16:41:29,668] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 16:41:29,668] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 16:41:29,669] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-64f3c98d-7bd3-4c89-80f2-a5733eba80ad
[2024-09-03 16:41:29,673] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-64f3c98d-7bd3-4c89-80f2-a5733eba80ad/pyspark-8357501c-c5c5-42e8-a733-d2f08b16d946
[2024-09-03 16:41:29,676] {spark_submit.py:495} INFO - 24/09/03 16:41:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-fbc3c630-bc94-46ba-bef2-4f852d699ff2
[2024-09-03 16:41:29,812] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240903T194121, end_date=20240903T194129
[2024-09-03 16:41:29,822] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 16:41:29,829] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-09-03 17:54:57,292] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 17:54:57,298] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 17:54:57,298] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:54:57,299] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 17:54:57,299] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 17:54:57,309] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 17:54:57,312] {standard_task_runner.py:52} INFO - Started process 325241 to run task
[2024-09-03 17:54:57,315] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpr6mvidte', '--error-file', '/tmp/tmpnt67wuj2']
[2024-09-03 17:54:57,315] {standard_task_runner.py:80} INFO - Job 94: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 17:54:57,349] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 17:54:57,387] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 17:54:57,391] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 17:54:57,392] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 17:54:58,388] {spark_submit.py:495} INFO - 24/09/03 17:54:58 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 17:54:58,388] {spark_submit.py:495} INFO - 24/09/03 17:54:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 17:54:58,717] {spark_submit.py:495} INFO - 24/09/03 17:54:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 17:54:59,215] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 17:54:59,222] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 17:54:59,255] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO ResourceUtils: ==============================================================
[2024-09-03 17:54:59,256] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 17:54:59,256] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO ResourceUtils: ==============================================================
[2024-09-03 17:54:59,256] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 17:54:59,273] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 17:54:59,285] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 17:54:59,285] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 17:54:59,322] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 17:54:59,322] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 17:54:59,322] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 17:54:59,323] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 17:54:59,323] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 17:54:59,473] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO Utils: Successfully started service 'sparkDriver' on port 41633.
[2024-09-03 17:54:59,502] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 17:54:59,530] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 17:54:59,544] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 17:54:59,545] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 17:54:59,548] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 17:54:59,565] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75f09ade-3321-4ab7-9337-85c728cd2c10
[2024-09-03 17:54:59,585] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 17:54:59,598] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 17:54:59,815] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 17:54:59,858] {spark_submit.py:495} INFO - 24/09/03 17:54:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 17:55:00,040] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 17:55:00,063] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37447.
[2024-09-03 17:55:00,064] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO NettyBlockTransferService: Server created on 192.168.2.128:37447
[2024-09-03 17:55:00,065] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 17:55:00,069] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 37447, None)
[2024-09-03 17:55:00,071] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:37447 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 37447, None)
[2024-09-03 17:55:00,073] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 37447, None)
[2024-09-03 17:55:00,074] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 37447, None)
[2024-09-03 17:55:00,501] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 17:55:00,502] {spark_submit.py:495} INFO - 24/09/03 17:55:00 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 17:55:01,226] {spark_submit.py:495} INFO - 24/09/03 17:55:01 INFO InMemoryFileIndex: It took 29 ms to list leaf files for 1 paths.
[2024-09-03 17:55:01,275] {spark_submit.py:495} INFO - 24/09/03 17:55:01 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 17:55:02,925] {spark_submit.py:495} INFO - 24/09/03 17:55:02 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:55:02,925] {spark_submit.py:495} INFO - 24/09/03 17:55:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:55:02,928] {spark_submit.py:495} INFO - 24/09/03 17:55:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 17:55:03,172] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 17:55:03,216] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 17:55:03,218] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:37447 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 17:55:03,229] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:03,238] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649510 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:03,385] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:03,401] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:03,402] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:03,402] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:03,403] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:03,407] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:03,481] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 17:55:03,484] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 17:55:03,485] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:37447 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 17:55:03,485] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:03,497] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:03,498] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 17:55:03,567] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:03,582] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 17:55:03,806] {spark_submit.py:495} INFO - 24/09/03 17:55:03 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455206, partition values: [empty row]
[2024-09-03 17:55:04,038] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO CodeGenerator: Code generated in 140.500843 ms
[2024-09-03 17:55:04,130] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 17:55:04,245] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 579 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:04,251] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 17:55:04,266] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,844 s
[2024-09-03 17:55:04,269] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:04,270] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 17:55:04,273] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,887389 s
[2024-09-03 17:55:04,638] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 17:55:04,640] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 17:55:04,640] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 17:55:04,701] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:04,702] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:04,702] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:04,796] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO CodeGenerator: Code generated in 45.25942 ms
[2024-09-03 17:55:04,832] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO CodeGenerator: Code generated in 23.037434 ms
[2024-09-03 17:55:04,837] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 17:55:04,844] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 17:55:04,845] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:37447 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 17:55:04,846] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:04,848] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649510 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:04,900] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:04,902] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:04,902] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:04,902] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:04,903] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:04,909] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:04,947] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 17:55:04,949] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 17:55:04,950] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:37447 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 17:55:04,950] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:04,951] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:04,951] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 17:55:04,957] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:04,958] {spark_submit.py:495} INFO - 24/09/03 17:55:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 17:55:05,023] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:05,023] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:05,025] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:05,084] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO CodeGenerator: Code generated in 20.250115 ms
[2024-09-03 17:55:05,089] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455206, partition values: [empty row]
[2024-09-03 17:55:05,113] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO CodeGenerator: Code generated in 20.537546 ms
[2024-09-03 17:55:05,133] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO CodeGenerator: Code generated in 4.71831 ms
[2024-09-03 17:55:05,242] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409031755045310564522577447503_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_202409031755045310564522577447503_0001_m_000000
[2024-09-03 17:55:05,242] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkHadoopMapRedUtil: attempt_202409031755045310564522577447503_0001_m_000000_1: Committed
[2024-09-03 17:55:05,247] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 17:55:05,248] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 296 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:05,249] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 17:55:05,250] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,339 s
[2024-09-03 17:55:05,250] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:05,251] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 17:55:05,253] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,350233 s
[2024-09-03 17:55:05,269] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileFormatWriter: Write Job 54b78a8b-4045-438a-8969-c570dfe6a101 committed.
[2024-09-03 17:55:05,273] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileFormatWriter: Finished processing stats for write job 54b78a8b-4045-438a-8969-c570dfe6a101.
[2024-09-03 17:55:05,314] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 17:55:05,314] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 17:55:05,315] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 17:55:05,342] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:05,342] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:05,343] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:05,373] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO CodeGenerator: Code generated in 8.611339 ms
[2024-09-03 17:55:05,378] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 17:55:05,385] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 17:55:05,386] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:37447 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 17:55:05,386] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:05,387] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649510 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 17:55:05,402] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 17:55:05,403] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 17:55:05,403] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 17:55:05,403] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 17:55:05,404] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Missing parents: List()
[2024-09-03 17:55:05,405] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 17:55:05,425] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 17:55:05,428] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 17:55:05,429] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:37447 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 17:55:05,430] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 17:55:05,430] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 17:55:05,431] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 17:55:05,431] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 17:55:05,432] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 17:55:05,446] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 17:55:05,447] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 17:55:05,447] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 17:55:05,500] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO CodeGenerator: Code generated in 10.241204 ms
[2024-09-03 17:55:05,503] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455206, partition values: [empty row]
[2024-09-03 17:55:05,522] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO CodeGenerator: Code generated in 16.832769 ms
[2024-09-03 17:55:05,558] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileOutputCommitter: Saved output of task 'attempt_202409031755057643654432379798117_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409031755057643654432379798117_0002_m_000000
[2024-09-03 17:55:05,559] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkHadoopMapRedUtil: attempt_202409031755057643654432379798117_0002_m_000000_2: Committed
[2024-09-03 17:55:05,560] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 17:55:05,565] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 134 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 17:55:05,565] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 17:55:05,566] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,162 s
[2024-09-03 17:55:05,567] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 17:55:05,568] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 17:55:05,568] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,164933 s
[2024-09-03 17:55:05,582] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileFormatWriter: Write Job d7e04062-c3bd-4dec-9434-4ff760608ff1 committed.
[2024-09-03 17:55:05,583] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO FileFormatWriter: Finished processing stats for write job d7e04062-c3bd-4dec-9434-4ff760608ff1.
[2024-09-03 17:55:05,625] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 17:55:05,635] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 17:55:05,653] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 17:55:05,663] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO MemoryStore: MemoryStore cleared
[2024-09-03 17:55:05,664] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO BlockManager: BlockManager stopped
[2024-09-03 17:55:05,669] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 17:55:05,671] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 17:55:05,674] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 17:55:05,675] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 17:55:05,675] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a51b5078-ae6f-4094-bb19-9d17fa87eaad
[2024-09-03 17:55:05,679] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a51b5078-ae6f-4094-bb19-9d17fa87eaad/pyspark-57aad2f5-4c50-4ec0-adb8-9b67f54fe4da
[2024-09-03 17:55:05,685] {spark_submit.py:495} INFO - 24/09/03 17:55:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-6041e9e5-2b2e-459d-ab3b-6e825bf0b1c1
[2024-09-03 17:55:05,746] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240903T205457, end_date=20240903T205505
[2024-09-03 17:55:05,793] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 17:55:05,820] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:11:39,032] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 18:11:39,037] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 18:11:39,038] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:11:39,038] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:11:39,038] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:11:39,047] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 18:11:39,050] {standard_task_runner.py:52} INFO - Started process 333733 to run task
[2024-09-03 18:11:39,053] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpbglpoxe_', '--error-file', '/tmp/tmpgfl8fel_']
[2024-09-03 18:11:39,053] {standard_task_runner.py:80} INFO - Job 93: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:11:39,095] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:11:39,145] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 18:11:39,148] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:11:39,150] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 18:11:40,308] {spark_submit.py:495} INFO - 24/09/03 18:11:40 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:11:40,310] {spark_submit.py:495} INFO - 24/09/03 18:11:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:11:40,718] {spark_submit.py:495} INFO - 24/09/03 18:11:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:11:41,487] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:11:41,499] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:11:41,608] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO ResourceUtils: ==============================================================
[2024-09-03 18:11:41,610] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:11:41,610] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO ResourceUtils: ==============================================================
[2024-09-03 18:11:41,610] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:11:41,648] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:11:41,664] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:11:41,670] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:11:41,753] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:11:41,753] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:11:41,753] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:11:41,753] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:11:41,753] {spark_submit.py:495} INFO - 24/09/03 18:11:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:11:42,020] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO Utils: Successfully started service 'sparkDriver' on port 39163.
[2024-09-03 18:11:42,066] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:11:42,110] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:11:42,142] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:11:42,143] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:11:42,148] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:11:42,163] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bf309cc0-89d0-4903-97fc-b15c6ce62092
[2024-09-03 18:11:42,191] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:11:42,220] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:11:42,604] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:11:42,718] {spark_submit.py:495} INFO - 24/09/03 18:11:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:11:43,137] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:11:43,184] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42619.
[2024-09-03 18:11:43,184] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO NettyBlockTransferService: Server created on 192.168.2.128:42619
[2024-09-03 18:11:43,187] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:11:43,195] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 42619, None)
[2024-09-03 18:11:43,198] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:42619 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 42619, None)
[2024-09-03 18:11:43,201] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 42619, None)
[2024-09-03 18:11:43,202] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 42619, None)
[2024-09-03 18:11:43,774] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:11:43,775] {spark_submit.py:495} INFO - 24/09/03 18:11:43 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:11:44,817] {spark_submit.py:495} INFO - 24/09/03 18:11:44 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.
[2024-09-03 18:11:44,886] {spark_submit.py:495} INFO - 24/09/03 18:11:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:11:46,976] {spark_submit.py:495} INFO - 24/09/03 18:11:46 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:11:46,978] {spark_submit.py:495} INFO - 24/09/03 18:11:46 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:11:46,983] {spark_submit.py:495} INFO - 24/09/03 18:11:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:11:47,338] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:11:47,390] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:11:47,395] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:42619 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:11:47,414] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:47,435] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198790 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:11:47,663] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:47,687] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:11:47,688] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:11:47,689] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:11:47,691] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:11:47,698] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:11:47,832] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:11:47,835] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:11:47,836] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:42619 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:11:47,837] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:11:47,852] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:11:47,853] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:11:47,915] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:11:47,938] {spark_submit.py:495} INFO - 24/09/03 18:11:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:11:48,329] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-4486, partition values: [empty row]
[2024-09-03 18:11:48,671] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO CodeGenerator: Code generated in 214.870083 ms
[2024-09-03 18:11:48,727] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2768 bytes result sent to driver
[2024-09-03 18:11:48,741] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 837 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:11:48,745] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:11:48,751] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,027 s
[2024-09-03 18:11:48,886] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:11:48,887] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:11:48,892] {spark_submit.py:495} INFO - 24/09/03 18:11:48 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,226591 s
[2024-09-03 18:11:49,374] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:11:49,376] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:11:49,377] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:11:49,474] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:49,474] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:49,477] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:49,595] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO CodeGenerator: Code generated in 42.946755 ms
[2024-09-03 18:11:49,656] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO CodeGenerator: Code generated in 38.716355 ms
[2024-09-03 18:11:49,665] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:11:49,674] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:11:49,677] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:42619 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:11:49,679] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:49,683] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198790 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:11:49,764] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:49,766] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:11:49,766] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:11:49,766] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:11:49,766] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:11:49,767] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:11:49,829] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:11:49,832] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:11:49,833] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:42619 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:11:49,834] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:11:49,835] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:11:49,835] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:11:49,840] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:11:49,841] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:11:49,889] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:49,889] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:49,889] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:49,953] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO CodeGenerator: Code generated in 20.784886 ms
[2024-09-03 18:11:49,959] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-4486, partition values: [empty row]
[2024-09-03 18:11:49,993] {spark_submit.py:495} INFO - 24/09/03 18:11:49 INFO CodeGenerator: Code generated in 28.270019 ms
[2024-09-03 18:11:50,015] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO CodeGenerator: Code generated in 6.385687 ms
[2024-09-03 18:11:50,051] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileOutputCommitter: Saved output of task 'attempt_202409031811499054770726157644355_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_202409031811499054770726157644355_0001_m_000000
[2024-09-03 18:11:50,052] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkHadoopMapRedUtil: attempt_202409031811499054770726157644355_0001_m_000000_1: Committed
[2024-09-03 18:11:50,058] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:11:50,062] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 226 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:11:50,062] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:11:50,065] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,295 s
[2024-09-03 18:11:50,065] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:11:50,065] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:11:50,066] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,300857 s
[2024-09-03 18:11:50,097] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileFormatWriter: Write Job 6ea624c5-a88d-45ef-a589-6c27f9f20d6d committed.
[2024-09-03 18:11:50,100] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileFormatWriter: Finished processing stats for write job 6ea624c5-a88d-45ef-a589-6c27f9f20d6d.
[2024-09-03 18:11:50,165] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:11:50,165] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:11:50,167] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:11:50,181] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:50,181] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:50,182] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:50,228] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO CodeGenerator: Code generated in 16.681864 ms
[2024-09-03 18:11:50,232] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:11:50,240] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:11:50,241] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:42619 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:11:50,243] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:50,244] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198790 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:11:50,262] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:11:50,263] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:11:50,264] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:11:50,264] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:11:50,264] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:11:50,266] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:11:50,286] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:11:50,295] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:11:50,298] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:42619 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:11:50,298] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:11:50,299] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:11:50,299] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:11:50,301] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:11:50,301] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:11:50,316] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:11:50,316] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:11:50,317] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:11:50,353] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO CodeGenerator: Code generated in 8.513521 ms
[2024-09-03 18:11:50,357] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-4486, partition values: [empty row]
[2024-09-03 18:11:50,373] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO CodeGenerator: Code generated in 11.852146 ms
[2024-09-03 18:11:50,384] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileOutputCommitter: Saved output of task 'attempt_202409031811501321268410226750656_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409031811501321268410226750656_0002_m_000000
[2024-09-03 18:11:50,384] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkHadoopMapRedUtil: attempt_202409031811501321268410226750656_0002_m_000000_2: Committed
[2024-09-03 18:11:50,385] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:11:50,391] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 91 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:11:50,393] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,125 s
[2024-09-03 18:11:50,393] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:11:50,395] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:11:50,395] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:11:50,396] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,133184 s
[2024-09-03 18:11:50,427] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileFormatWriter: Write Job 2c9b662f-97a5-4a5d-ba9b-28447837a2ec committed.
[2024-09-03 18:11:50,427] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO FileFormatWriter: Finished processing stats for write job 2c9b662f-97a5-4a5d-ba9b-28447837a2ec.
[2024-09-03 18:11:50,462] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:11:50,471] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:11:50,487] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:11:50,498] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:11:50,498] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO BlockManager: BlockManager stopped
[2024-09-03 18:11:50,508] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:11:50,513] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:11:50,519] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:11:50,519] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:11:50,519] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb2261d1-dc26-4207-8249-5bd859368c19
[2024-09-03 18:11:50,521] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb2261d1-dc26-4207-8249-5bd859368c19/pyspark-80fc0c2f-61ce-490c-b0bf-71e7e9914a8b
[2024-09-03 18:11:50,527] {spark_submit.py:495} INFO - 24/09/03 18:11:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-4128a1db-aa4a-4806-a0a7-744e8e830058
[2024-09-03 18:11:50,595] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240903T211139, end_date=20240903T211150
[2024-09-03 18:11:50,632] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:11:50,649] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:16:10,228] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 18:16:10,235] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 18:16:10,236] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:16:10,236] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:16:10,236] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:16:10,248] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 18:16:10,251] {standard_task_runner.py:52} INFO - Started process 337742 to run task
[2024-09-03 18:16:10,254] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpcdjz5z41', '--error-file', '/tmp/tmpgiulradt']
[2024-09-03 18:16:10,255] {standard_task_runner.py:80} INFO - Job 93: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:16:10,292] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:16:10,336] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 18:16:10,340] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:16:10,341] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 18:16:11,433] {spark_submit.py:495} INFO - 24/09/03 18:16:11 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:16:11,433] {spark_submit.py:495} INFO - 24/09/03 18:16:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:16:11,777] {spark_submit.py:495} INFO - 24/09/03 18:16:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:16:12,296] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:16:12,304] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:16:12,344] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO ResourceUtils: ==============================================================
[2024-09-03 18:16:12,344] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:16:12,345] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO ResourceUtils: ==============================================================
[2024-09-03 18:16:12,346] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:16:12,366] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:16:12,379] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:16:12,380] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:16:12,423] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:16:12,423] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:16:12,424] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:16:12,424] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:16:12,425] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:16:12,582] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO Utils: Successfully started service 'sparkDriver' on port 43285.
[2024-09-03 18:16:12,617] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:16:12,650] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:16:12,669] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:16:12,670] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:16:12,674] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:16:12,690] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e09a63db-1ec5-4bd6-b39c-1aee3ce743c8
[2024-09-03 18:16:12,716] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:16:12,736] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:16:12,992] {spark_submit.py:495} INFO - 24/09/03 18:16:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:16:13,070] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:16:13,299] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:16:13,340] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41109.
[2024-09-03 18:16:13,340] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO NettyBlockTransferService: Server created on 192.168.2.128:41109
[2024-09-03 18:16:13,342] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:16:13,349] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 41109, None)
[2024-09-03 18:16:13,354] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:41109 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 41109, None)
[2024-09-03 18:16:13,358] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 41109, None)
[2024-09-03 18:16:13,360] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 41109, None)
[2024-09-03 18:16:13,891] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:16:13,892] {spark_submit.py:495} INFO - 24/09/03 18:16:13 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:16:14,822] {spark_submit.py:495} INFO - 24/09/03 18:16:14 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
[2024-09-03 18:16:14,877] {spark_submit.py:495} INFO - 24/09/03 18:16:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 18:16:16,663] {spark_submit.py:495} INFO - 24/09/03 18:16:16 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:16:16,665] {spark_submit.py:495} INFO - 24/09/03 18:16:16 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:16:16,670] {spark_submit.py:495} INFO - 24/09/03 18:16:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:16:16,938] {spark_submit.py:495} INFO - 24/09/03 18:16:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:16:17,016] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:16:17,020] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:41109 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:16:17,025] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:17,035] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649329 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:17,193] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:17,211] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:17,212] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:17,213] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:17,215] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:17,225] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:17,349] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:16:17,351] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:16:17,352] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:41109 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:16:17,353] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:17,380] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:17,382] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:16:17,431] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:17,447] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:16:17,694] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455025, partition values: [empty row]
[2024-09-03 18:16:17,977] {spark_submit.py:495} INFO - 24/09/03 18:16:17 INFO CodeGenerator: Code generated in 189.541459 ms
[2024-09-03 18:16:18,075] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 18:16:18,203] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 670 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:18,206] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:16:18,213] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,959 s
[2024-09-03 18:16:18,216] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:18,217] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:16:18,219] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,025502 s
[2024-09-03 18:16:18,666] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:16:18,667] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:16:18,668] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:16:18,749] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:18,749] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:18,750] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:18,841] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO CodeGenerator: Code generated in 28.268442 ms
[2024-09-03 18:16:18,884] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO CodeGenerator: Code generated in 28.52339 ms
[2024-09-03 18:16:18,890] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:16:18,901] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:16:18,902] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:41109 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:16:18,902] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:18,905] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649329 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:18,960] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:18,964] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:18,965] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:18,965] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:18,965] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:18,966] {spark_submit.py:495} INFO - 24/09/03 18:16:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:19,004] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:16:19,007] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:16:19,007] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:41109 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:16:19,008] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:19,009] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:19,009] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:16:19,014] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:19,018] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:16:19,112] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:19,112] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:19,113] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:19,185] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO CodeGenerator: Code generated in 28.246314 ms
[2024-09-03 18:16:19,188] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455025, partition values: [empty row]
[2024-09-03 18:16:19,208] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO CodeGenerator: Code generated in 17.026652 ms
[2024-09-03 18:16:19,236] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO CodeGenerator: Code generated in 6.298642 ms
[2024-09-03 18:16:19,362] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_20240903181618342416991825872751_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_20240903181618342416991825872751_0001_m_000000
[2024-09-03 18:16:19,364] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkHadoopMapRedUtil: attempt_20240903181618342416991825872751_0001_m_000000_1: Committed
[2024-09-03 18:16:19,369] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:16:19,371] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 361 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:19,371] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:16:19,376] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,404 s
[2024-09-03 18:16:19,376] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:19,376] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:16:19,376] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,412893 s
[2024-09-03 18:16:19,394] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileFormatWriter: Write Job ae276bc0-59b0-4dd8-8e6a-502acd269961 committed.
[2024-09-03 18:16:19,399] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileFormatWriter: Finished processing stats for write job ae276bc0-59b0-4dd8-8e6a-502acd269961.
[2024-09-03 18:16:19,450] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:16:19,450] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:16:19,450] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:16:19,459] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:19,461] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:19,461] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:19,499] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO CodeGenerator: Code generated in 12.55898 ms
[2024-09-03 18:16:19,502] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:16:19,508] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:16:19,509] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:41109 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:16:19,512] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:19,513] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649329 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:16:19,530] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:16:19,531] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:16:19,531] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:16:19,531] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:16:19,531] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:16:19,534] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:16:19,549] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:16:19,553] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:16:19,555] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:41109 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:16:19,557] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:16:19,559] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:16:19,559] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:16:19,561] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:16:19,561] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:16:19,584] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:16:19,585] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:16:19,585] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:16:19,625] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO CodeGenerator: Code generated in 8.946417 ms
[2024-09-03 18:16:19,629] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455025, partition values: [empty row]
[2024-09-03 18:16:19,642] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO CodeGenerator: Code generated in 9.498144 ms
[2024-09-03 18:16:19,672] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_202409031816192745753858385487317_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409031816192745753858385487317_0002_m_000000
[2024-09-03 18:16:19,672] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkHadoopMapRedUtil: attempt_202409031816192745753858385487317_0002_m_000000_2: Committed
[2024-09-03 18:16:19,673] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:16:19,675] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 115 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:16:19,675] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:16:19,677] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,142 s
[2024-09-03 18:16:19,677] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:16:19,678] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:16:19,679] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,150785 s
[2024-09-03 18:16:19,695] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileFormatWriter: Write Job 7b6cdf78-802d-4b5a-8710-9cc965d5c9c1 committed.
[2024-09-03 18:16:19,696] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO FileFormatWriter: Finished processing stats for write job 7b6cdf78-802d-4b5a-8710-9cc965d5c9c1.
[2024-09-03 18:16:19,767] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:16:19,781] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:16:19,790] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.2.128:41109 in memory (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:16:19,820] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:16:19,835] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:16:19,835] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO BlockManager: BlockManager stopped
[2024-09-03 18:16:19,839] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:16:19,841] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:16:19,848] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:16:19,848] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:16:19,849] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-2411fa9b-56c9-472b-902a-51e3c740a472
[2024-09-03 18:16:19,852] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-3522e5a3-2bf9-4147-ade7-0675796154b3/pyspark-5a7d41b7-332c-4c4b-b60a-91bf78238bcc
[2024-09-03 18:16:19,855] {spark_submit.py:495} INFO - 24/09/03 18:16:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-3522e5a3-2bf9-4147-ade7-0675796154b3
[2024-09-03 18:16:19,941] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240903T211610, end_date=20240903T211619
[2024-09-03 18:16:19,985] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:16:20,012] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 18:22:09,196] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 18:22:09,201] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 18:22:09,201] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:22:09,202] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 18:22:09,202] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 18:22:09,211] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 18:22:09,214] {standard_task_runner.py:52} INFO - Started process 343917 to run task
[2024-09-03 18:22:09,217] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpn2l91i2_', '--error-file', '/tmp/tmpoiqibb8n']
[2024-09-03 18:22:09,218] {standard_task_runner.py:80} INFO - Job 93: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 18:22:09,255] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 18:22:09,304] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 18:22:09,310] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 18:22:09,311] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 18:22:10,509] {spark_submit.py:495} INFO - 24/09/03 18:22:10 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 18:22:10,509] {spark_submit.py:495} INFO - 24/09/03 18:22:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 18:22:10,925] {spark_submit.py:495} INFO - 24/09/03 18:22:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 18:22:11,594] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 18:22:11,607] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 18:22:11,658] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO ResourceUtils: ==============================================================
[2024-09-03 18:22:11,658] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 18:22:11,659] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO ResourceUtils: ==============================================================
[2024-09-03 18:22:11,660] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 18:22:11,685] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 18:22:11,701] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 18:22:11,701] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 18:22:11,760] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 18:22:11,761] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 18:22:11,761] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 18:22:11,761] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 18:22:11,761] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 18:22:11,975] {spark_submit.py:495} INFO - 24/09/03 18:22:11 INFO Utils: Successfully started service 'sparkDriver' on port 39161.
[2024-09-03 18:22:12,002] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 18:22:12,047] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 18:22:12,069] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 18:22:12,070] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 18:22:12,074] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 18:22:12,087] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-27c841dc-961a-44f4-8d6d-6c10e15fd5e7
[2024-09-03 18:22:12,113] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 18:22:12,128] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 18:22:12,387] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-09-03 18:22:12,444] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4040
[2024-09-03 18:22:12,656] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 18:22:12,682] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46387.
[2024-09-03 18:22:12,682] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO NettyBlockTransferService: Server created on 192.168.2.128:46387
[2024-09-03 18:22:12,685] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 18:22:12,691] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 46387, None)
[2024-09-03 18:22:12,696] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:46387 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 46387, None)
[2024-09-03 18:22:12,699] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 46387, None)
[2024-09-03 18:22:12,701] {spark_submit.py:495} INFO - 24/09/03 18:22:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 46387, None)
[2024-09-03 18:22:13,333] {spark_submit.py:495} INFO - 24/09/03 18:22:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 18:22:13,334] {spark_submit.py:495} INFO - 24/09/03 18:22:13 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 18:22:14,319] {spark_submit.py:495} INFO - 24/09/03 18:22:14 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
[2024-09-03 18:22:14,383] {spark_submit.py:495} INFO - 24/09/03 18:22:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2024-09-03 18:22:16,428] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:22:16,429] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:22:16,433] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 18:22:16,718] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 18:22:16,774] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 18:22:16,777] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:46387 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 18:22:16,782] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:16,791] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649328 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:22:16,955] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:16,974] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:22:16,974] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:22:16,974] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:22:16,978] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:22:16,983] {spark_submit.py:495} INFO - 24/09/03 18:22:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:22:17,095] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 18:22:17,099] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 18:22:17,100] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:46387 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 18:22:17,101] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:22:17,116] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:22:17,117] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 18:22:17,201] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 18:22:17,219] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 18:22:17,546] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455024, partition values: [empty row]
[2024-09-03 18:22:17,877] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO CodeGenerator: Code generated in 173.784856 ms
[2024-09-03 18:22:17,974] {spark_submit.py:495} INFO - 24/09/03 18:22:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2798 bytes result sent to driver
[2024-09-03 18:22:18,111] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 811 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:22:18,116] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 18:22:18,121] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1,116 s
[2024-09-03 18:22:18,126] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:22:18,126] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 18:22:18,132] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1,172604 s
[2024-09-03 18:22:18,545] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 18:22:18,547] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 18:22:18,547] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 18:22:18,622] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:18,626] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:18,626] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:18,708] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO CodeGenerator: Code generated in 26.847205 ms
[2024-09-03 18:22:18,747] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO CodeGenerator: Code generated in 24.698131 ms
[2024-09-03 18:22:18,753] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 18:22:18,761] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 18:22:18,762] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:46387 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 18:22:18,763] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:18,765] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649328 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:22:18,835] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:18,837] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:22:18,837] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:22:18,838] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:22:18,838] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:22:18,845] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:22:18,910] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 18:22:18,914] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 18:22:18,915] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:46387 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 18:22:18,916] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:22:18,917] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:22:18,917] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 18:22:18,921] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:22:18,921] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 18:22:18,987] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:18,987] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:18,988] {spark_submit.py:495} INFO - 24/09/03 18:22:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:19,053] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO CodeGenerator: Code generated in 24.576012 ms
[2024-09-03 18:22:19,058] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455024, partition values: [empty row]
[2024-09-03 18:22:19,084] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO CodeGenerator: Code generated in 20.753749 ms
[2024-09-03 18:22:19,111] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO CodeGenerator: Code generated in 7.138327 ms
[2024-09-03 18:22:19,253] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileOutputCommitter: Saved output of task 'attempt_202409031822185978462147039130422_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_202409031822185978462147039130422_0001_m_000000
[2024-09-03 18:22:19,254] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkHadoopMapRedUtil: attempt_202409031822185978462147039130422_0001_m_000000_1: Committed
[2024-09-03 18:22:19,260] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 18:22:19,262] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 344 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:22:19,265] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 18:22:19,265] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,417 s
[2024-09-03 18:22:19,265] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:22:19,265] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 18:22:19,266] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,431596 s
[2024-09-03 18:22:19,287] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileFormatWriter: Write Job 5d9d02c6-a96d-4d14-aacf-39fc745d6cde committed.
[2024-09-03 18:22:19,291] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileFormatWriter: Finished processing stats for write job 5d9d02c6-a96d-4d14-aacf-39fc745d6cde.
[2024-09-03 18:22:19,334] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 18:22:19,335] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 18:22:19,336] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 18:22:19,352] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:19,352] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:19,352] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:19,382] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO CodeGenerator: Code generated in 11.779242 ms
[2024-09-03 18:22:19,386] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 18:22:19,396] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 18:22:19,397] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:46387 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 18:22:19,398] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:19,399] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649328 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 18:22:19,414] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 18:22:19,415] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 18:22:19,415] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 18:22:19,415] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 18:22:19,415] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Missing parents: List()
[2024-09-03 18:22:19,416] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 18:22:19,437] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 18:22:19,441] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 18:22:19,444] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:46387 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 18:22:19,445] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 18:22:19,446] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 18:22:19,446] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 18:22:19,447] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 18:22:19,448] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 18:22:19,466] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 18:22:19,466] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 18:22:19,466] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 18:22:19,505] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO CodeGenerator: Code generated in 17.746799 ms
[2024-09-03 18:22:19,510] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455024, partition values: [empty row]
[2024-09-03 18:22:19,525] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO CodeGenerator: Code generated in 11.803027 ms
[2024-09-03 18:22:19,563] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileOutputCommitter: Saved output of task 'attempt_202409031822196723863819101350323_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409031822196723863819101350323_0002_m_000000
[2024-09-03 18:22:19,563] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkHadoopMapRedUtil: attempt_202409031822196723863819101350323_0002_m_000000_2: Committed
[2024-09-03 18:22:19,564] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 18:22:19,566] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 120 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 18:22:19,566] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 18:22:19,567] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,151 s
[2024-09-03 18:22:19,568] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 18:22:19,568] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 18:22:19,568] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,154031 s
[2024-09-03 18:22:19,597] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileFormatWriter: Write Job 4f8a46a9-bc68-455b-9e8e-d59e003fe431 committed.
[2024-09-03 18:22:19,597] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO FileFormatWriter: Finished processing stats for write job 4f8a46a9-bc68-455b-9e8e-d59e003fe431.
[2024-09-03 18:22:19,639] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 18:22:19,650] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4040
[2024-09-03 18:22:19,662] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 18:22:19,671] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO MemoryStore: MemoryStore cleared
[2024-09-03 18:22:19,671] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO BlockManager: BlockManager stopped
[2024-09-03 18:22:19,679] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 18:22:19,682] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 18:22:19,688] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 18:22:19,689] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 18:22:19,689] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1cf4ac9-ace8-4b00-965f-a06029c74094/pyspark-acac5de2-467e-4026-b31e-eab4b7e0ae81
[2024-09-03 18:22:19,692] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1cf4ac9-ace8-4b00-965f-a06029c74094
[2024-09-03 18:22:19,696] {spark_submit.py:495} INFO - 24/09/03 18:22:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c50a235-eee6-4498-b4e0-1159b789a5a6
[2024-09-03 18:22:19,773] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240903T212209, end_date=20240903T212219
[2024-09-03 18:22:19,820] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 18:22:19,839] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 21:43:36,453] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 21:43:36,461] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 21:43:36,461] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:43:36,461] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 21:43:36,461] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 21:43:36,475] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 21:43:36,479] {standard_task_runner.py:52} INFO - Started process 417914 to run task
[2024-09-03 21:43:36,482] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '90', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp7o4ipwi2', '--error-file', '/tmp/tmpriga7evw']
[2024-09-03 21:43:36,482] {standard_task_runner.py:80} INFO - Job 90: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 21:43:36,520] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 21:43:36,563] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 21:43:36,567] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 21:43:36,567] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 21:43:37,576] {spark_submit.py:495} INFO - 24/09/03 21:43:37 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 21:43:37,577] {spark_submit.py:495} INFO - 24/09/03 21:43:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 21:43:37,990] {spark_submit.py:495} INFO - 24/09/03 21:43:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 21:43:38,507] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 21:43:38,516] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 21:43:38,561] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO ResourceUtils: ==============================================================
[2024-09-03 21:43:38,562] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 21:43:38,562] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO ResourceUtils: ==============================================================
[2024-09-03 21:43:38,563] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 21:43:38,580] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 21:43:38,594] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 21:43:38,596] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 21:43:38,637] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 21:43:38,637] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 21:43:38,637] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 21:43:38,637] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 21:43:38,638] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 21:43:38,786] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO Utils: Successfully started service 'sparkDriver' on port 46751.
[2024-09-03 21:43:38,815] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 21:43:38,847] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 21:43:38,863] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 21:43:38,864] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 21:43:38,868] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 21:43:38,881] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-06240b9f-5482-4327-a817-45de7d0a732c
[2024-09-03 21:43:38,899] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 21:43:38,924] {spark_submit.py:495} INFO - 24/09/03 21:43:38 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 21:43:39,103] {spark_submit.py:495} INFO - 24/09/03 21:43:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 21:43:39,110] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 21:43:39,168] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 21:43:39,352] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 21:43:39,377] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45189.
[2024-09-03 21:43:39,377] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO NettyBlockTransferService: Server created on 192.168.2.128:45189
[2024-09-03 21:43:39,378] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 21:43:39,383] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 45189, None)
[2024-09-03 21:43:39,386] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:45189 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 45189, None)
[2024-09-03 21:43:39,389] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 45189, None)
[2024-09-03 21:43:39,390] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 45189, None)
[2024-09-03 21:43:39,807] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 21:43:39,808] {spark_submit.py:495} INFO - 24/09/03 21:43:39 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 21:43:40,621] {spark_submit.py:495} INFO - 24/09/03 21:43:40 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
[2024-09-03 21:43:40,705] {spark_submit.py:495} INFO - 24/09/03 21:43:40 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 21:43:42,343] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:43:42,344] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:43:42,348] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 21:43:42,591] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 21:43:42,634] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 21:43:42,639] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:45189 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 21:43:42,644] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:42,653] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649556 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:43:42,797] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:42,810] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:43:42,811] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:43:42,812] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:43:42,813] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:43:42,823] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:43:42,918] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 21:43:42,921] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 21:43:42,922] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:45189 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 21:43:42,923] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:43:42,933] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:43:42,934] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 21:43:42,985] {spark_submit.py:495} INFO - 24/09/03 21:43:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 21:43:43,001] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 21:43:43,254] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455252, partition values: [empty row]
[2024-09-03 21:43:43,498] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO CodeGenerator: Code generated in 157.356313 ms
[2024-09-03 21:43:43,585] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 21:43:43,714] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 631 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:43:43,717] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 21:43:43,723] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,878 s
[2024-09-03 21:43:43,736] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:43:43,740] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 21:43:43,743] {spark_submit.py:495} INFO - 24/09/03 21:43:43 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,939817 s
[2024-09-03 21:43:44,156] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 21:43:44,157] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 21:43:44,157] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 21:43:44,223] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:43:44,224] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:43:44,224] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:44,295] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO CodeGenerator: Code generated in 24.622685 ms
[2024-09-03 21:43:44,332] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO CodeGenerator: Code generated in 24.037565 ms
[2024-09-03 21:43:44,337] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 21:43:44,344] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 21:43:44,345] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:45189 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 21:43:44,346] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:44,350] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649556 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:43:44,407] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:44,408] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:43:44,409] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:43:44,409] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:43:44,409] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:43:44,410] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:43:44,463] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 21:43:44,466] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 21:43:44,467] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:45189 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 21:43:44,467] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:43:44,468] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:43:44,468] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 21:43:44,473] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:43:44,474] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 21:43:44,529] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:43:44,530] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:43:44,531] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:44,591] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO CodeGenerator: Code generated in 24.918646 ms
[2024-09-03 21:43:44,594] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455252, partition values: [empty row]
[2024-09-03 21:43:44,614] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO CodeGenerator: Code generated in 17.642603 ms
[2024-09-03 21:43:44,653] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO CodeGenerator: Code generated in 5.637677 ms
[2024-09-03 21:43:44,774] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: Saved output of task 'attempt_202409032143443394154122385644775_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_202409032143443394154122385644775_0001_m_000000
[2024-09-03 21:43:44,774] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkHadoopMapRedUtil: attempt_202409032143443394154122385644775_0001_m_000000_1: Committed
[2024-09-03 21:43:44,776] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 21:43:44,781] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 310 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:43:44,787] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 21:43:44,787] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,370 s
[2024-09-03 21:43:44,787] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:43:44,788] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 21:43:44,788] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,375901 s
[2024-09-03 21:43:44,808] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileFormatWriter: Write Job d6b518cd-8bc3-4154-b7a5-2dc6266573ee committed.
[2024-09-03 21:43:44,811] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileFormatWriter: Finished processing stats for write job d6b518cd-8bc3-4154-b7a5-2dc6266573ee.
[2024-09-03 21:43:44,854] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 21:43:44,854] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 21:43:44,854] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 21:43:44,863] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:43:44,863] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:43:44,863] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:44,897] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO CodeGenerator: Code generated in 10.352587 ms
[2024-09-03 21:43:44,901] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 21:43:44,908] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 21:43:44,909] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:45189 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 21:43:44,910] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:44,911] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649556 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 21:43:44,936] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 21:43:44,937] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 21:43:44,938] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 21:43:44,938] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 21:43:44,938] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Missing parents: List()
[2024-09-03 21:43:44,940] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 21:43:44,959] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 21:43:44,961] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 21:43:44,962] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:45189 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 21:43:44,962] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 21:43:44,963] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 21:43:44,963] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 21:43:44,967] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 21:43:44,967] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 21:43:44,979] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 21:43:44,979] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 21:43:44,979] {spark_submit.py:495} INFO - 24/09/03 21:43:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 21:43:45,009] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO CodeGenerator: Code generated in 10.721129 ms
[2024-09-03 21:43:45,011] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-455252, partition values: [empty row]
[2024-09-03 21:43:45,023] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO CodeGenerator: Code generated in 10.192051 ms
[2024-09-03 21:43:45,052] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO FileOutputCommitter: Saved output of task 'attempt_202409032143443789518518486787780_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409032143443789518518486787780_0002_m_000000
[2024-09-03 21:43:45,052] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO SparkHadoopMapRedUtil: attempt_202409032143443789518518486787780_0002_m_000000_2: Committed
[2024-09-03 21:43:45,053] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 21:43:45,055] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 89 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 21:43:45,056] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,116 s
[2024-09-03 21:43:45,056] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 21:43:45,056] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 21:43:45,056] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 21:43:45,056] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,120265 s
[2024-09-03 21:43:45,083] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO FileFormatWriter: Write Job ef5219af-e1dc-47f9-a4c5-1a83d5acecc4 committed.
[2024-09-03 21:43:45,083] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO FileFormatWriter: Finished processing stats for write job ef5219af-e1dc-47f9-a4c5-1a83d5acecc4.
[2024-09-03 21:43:45,113] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 21:43:45,122] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 21:43:45,135] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 21:43:45,145] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO MemoryStore: MemoryStore cleared
[2024-09-03 21:43:45,145] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO BlockManager: BlockManager stopped
[2024-09-03 21:43:45,158] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 21:43:45,158] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 21:43:45,164] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 21:43:45,164] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 21:43:45,164] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-17df9c37-f115-4544-b980-e31920b4febd
[2024-09-03 21:43:45,168] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-96eae461-88f8-4e5d-8b9d-1f9562a82f88
[2024-09-03 21:43:45,171] {spark_submit.py:495} INFO - 24/09/03 21:43:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-17df9c37-f115-4544-b980-e31920b4febd/pyspark-90706daf-78a0-4faa-a3a6-7eff01dcc92b
[2024-09-03 21:43:45,252] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240904T004336, end_date=20240904T004345
[2024-09-03 21:43:45,305] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 21:43:45,319] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-09-03 22:52:19,464] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 22:52:19,471] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [queued]>
[2024-09-03 22:52:19,471] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:52:19,472] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2024-09-03 22:52:19,472] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2024-09-03 22:52:19,480] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): SILVER_LAYER_LOAD_L> on 2024-08-25 00:00:00+00:00
[2024-09-03 22:52:19,483] {standard_task_runner.py:52} INFO - Started process 451249 to run task
[2024-09-03 22:52:19,486] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'Twitter_extractor_DAGs', 'SILVER_LAYER_LOAD_L', 'scheduled__2024-08-25T00:00:00+00:00', '--job-id', '138', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_hkh5rjg', '--error-file', '/tmp/tmpdkljhoer']
[2024-09-03 22:52:19,487] {standard_task_runner.py:80} INFO - Job 138: Subtask SILVER_LAYER_LOAD_L
[2024-09-03 22:52:19,541] {task_command.py:370} INFO - Running <TaskInstance: Twitter_extractor_DAGs.SILVER_LAYER_LOAD_L scheduled__2024-08-25T00:00:00+00:00 [running]> on host kaiwangairflow
[2024-09-03 22:52:19,584] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=Twitter_extractor_DAGs
AIRFLOW_CTX_TASK_ID=SILVER_LAYER_LOAD_L
AIRFLOW_CTX_EXECUTION_DATE=2024-08-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-08-25T00:00:00+00:00
[2024-09-03 22:52:19,587] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2024-09-03 22:52:19,588] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default src/scripts/load_silver_functions.py --src datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z --dest datalake/silver/data_engineer/ --process-data 20240825
[2024-09-03 22:52:20,663] {spark_submit.py:495} INFO - 24/09/03 22:52:20 WARN Utils: Your hostname, kaiwangairflow resolves to a loopback address: 127.0.1.1; using 192.168.2.128 instead (on interface ens33)
[2024-09-03 22:52:20,663] {spark_submit.py:495} INFO - 24/09/03 22:52:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-09-03 22:52:20,979] {spark_submit.py:495} INFO - 24/09/03 22:52:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-09-03 22:52:21,538] {spark_submit.py:495} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2024-09-03 22:52:21,551] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SparkContext: Running Spark version 3.1.3
[2024-09-03 22:52:21,590] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO ResourceUtils: ==============================================================
[2024-09-03 22:52:21,590] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-09-03 22:52:21,590] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO ResourceUtils: ==============================================================
[2024-09-03 22:52:21,591] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SparkContext: Submitted application: twitter_transformation
[2024-09-03 22:52:21,615] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-09-03 22:52:21,629] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO ResourceProfile: Limiting resource is cpu
[2024-09-03 22:52:21,629] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-09-03 22:52:21,672] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SecurityManager: Changing view acls to: kai
[2024-09-03 22:52:21,672] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SecurityManager: Changing modify acls to: kai
[2024-09-03 22:52:21,672] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SecurityManager: Changing view acls groups to:
[2024-09-03 22:52:21,672] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SecurityManager: Changing modify acls groups to:
[2024-09-03 22:52:21,672] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kai); groups with view permissions: Set(); users  with modify permissions: Set(kai); groups with modify permissions: Set()
[2024-09-03 22:52:21,835] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO Utils: Successfully started service 'sparkDriver' on port 45609.
[2024-09-03 22:52:21,858] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SparkEnv: Registering MapOutputTracker
[2024-09-03 22:52:21,883] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SparkEnv: Registering BlockManagerMaster
[2024-09-03 22:52:21,904] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-09-03 22:52:21,904] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-09-03 22:52:21,908] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-09-03 22:52:21,919] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b364925f-173f-43b3-8abc-e2b99cf79794
[2024-09-03 22:52:21,940] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-09-03 22:52:21,961] {spark_submit.py:495} INFO - 24/09/03 22:52:21 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-09-03 22:52:22,147] {spark_submit.py:495} INFO - 24/09/03 22:52:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-09-03 22:52:22,154] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2024-09-03 22:52:22,201] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.128:4041
[2024-09-03 22:52:22,386] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO Executor: Starting executor ID driver on host 192.168.2.128
[2024-09-03 22:52:22,408] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40415.
[2024-09-03 22:52:22,409] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO NettyBlockTransferService: Server created on 192.168.2.128:40415
[2024-09-03 22:52:22,410] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-09-03 22:52:22,415] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.128, 40415, None)
[2024-09-03 22:52:22,419] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.128:40415 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.2.128, 40415, None)
[2024-09-03 22:52:22,422] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.128, 40415, None)
[2024-09-03 22:52:22,424] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.2.128, 40415, None)
[2024-09-03 22:52:22,895] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse').
[2024-09-03 22:52:22,895] {spark_submit.py:495} INFO - 24/09/03 22:52:22 INFO SharedState: Warehouse path is 'file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/spark-warehouse'.
[2024-09-03 22:52:23,709] {spark_submit.py:495} INFO - 24/09/03 22:52:23 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.
[2024-09-03 22:52:23,767] {spark_submit.py:495} INFO - 24/09/03 22:52:23 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2024-09-03 22:52:25,501] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:52:25,502] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:52:25,504] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2024-09-03 22:52:25,775] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)
[2024-09-03 22:52:25,819] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)
[2024-09-03 22:52:25,822] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.128:40415 (size: 27.5 KiB, free: 366.3 MiB)
[2024-09-03 22:52:25,827] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:25,837] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649150 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:52:25,996] {spark_submit.py:495} INFO - 24/09/03 22:52:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:26,013] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:52:26,014] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:52:26,014] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:52:26,015] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:52:26,021] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:52:26,102] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.0 MiB)
[2024-09-03 22:52:26,107] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2024-09-03 22:52:26,108] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.128:40415 (size: 6.3 KiB, free: 366.3 MiB)
[2024-09-03 22:52:26,109] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:52:26,121] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:52:26,123] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-09-03 22:52:26,191] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()
[2024-09-03 22:52:26,213] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-09-03 22:52:26,497] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-454846, partition values: [empty row]
[2024-09-03 22:52:26,749] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO CodeGenerator: Code generated in 149.923481 ms
[2024-09-03 22:52:26,844] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2841 bytes result sent to driver
[2024-09-03 22:52:26,974] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 675 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:52:26,978] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-09-03 22:52:26,982] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,947 s
[2024-09-03 22:52:26,987] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:52:26,988] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-09-03 22:52:26,991] {spark_submit.py:495} INFO - 24/09/03 22:52:26 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,993095 s
[2024-09-03 22:52:27,434] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2024-09-03 22:52:27,436] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileSourceStrategy: Post-Scan Filters: (size(data#7, true) > 0),isnotnull(data#7)
[2024-09-03 22:52:27,437] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2024-09-03 22:52:27,537] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:27,538] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:27,538] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:27,624] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO CodeGenerator: Code generated in 29.359015 ms
[2024-09-03 22:52:27,668] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO CodeGenerator: Code generated in 28.574222 ms
[2024-09-03 22:52:27,673] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.0 KiB, free 365.7 MiB)
[2024-09-03 22:52:27,683] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)
[2024-09-03 22:52:27,683] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.128:40415 (size: 27.5 KiB, free: 366.2 MiB)
[2024-09-03 22:52:27,684] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:27,689] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649150 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:52:27,749] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:27,750] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:52:27,751] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:52:27,752] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:52:27,753] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:52:27,754] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:52:27,804] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 190.1 KiB, free 365.5 MiB)
[2024-09-03 22:52:27,807] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 365.4 MiB)
[2024-09-03 22:52:27,808] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.128:40415 (size: 66.6 KiB, free: 366.2 MiB)
[2024-09-03 22:52:27,808] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:52:27,809] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[9] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:52:27,809] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-09-03 22:52:27,812] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:52:27,813] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-09-03 22:52:27,882] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:27,883] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:27,884] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:27,949] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO CodeGenerator: Code generated in 19.446361 ms
[2024-09-03 22:52:27,952] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-454846, partition values: [empty row]
[2024-09-03 22:52:27,978] {spark_submit.py:495} INFO - 24/09/03 22:52:27 INFO CodeGenerator: Code generated in 22.472602 ms
[2024-09-03 22:52:28,002] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO CodeGenerator: Code generated in 5.178978 ms
[2024-09-03 22:52:28,110] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileOutputCommitter: Saved output of task 'attempt_202409032252279088014760287833711_0001_m_000000_1' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/twitter_df/process_data=20240825/_temporary/0/task_202409032252279088014760287833711_0001_m_000000
[2024-09-03 22:52:28,110] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkHadoopMapRedUtil: attempt_202409032252279088014760287833711_0001_m_000000_1: Committed
[2024-09-03 22:52:28,114] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2901 bytes result sent to driver
[2024-09-03 22:52:28,121] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 311 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:52:28,121] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-09-03 22:52:28,123] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,367 s
[2024-09-03 22:52:28,123] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:52:28,124] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-09-03 22:52:28,124] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,375874 s
[2024-09-03 22:52:28,143] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileFormatWriter: Write Job 9558642f-9baf-4d7a-8c86-e88ee6499b42 committed.
[2024-09-03 22:52:28,149] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileFormatWriter: Finished processing stats for write job 9558642f-9baf-4d7a-8c86-e88ee6499b42.
[2024-09-03 22:52:28,204] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileSourceStrategy: Pushed Filters:
[2024-09-03 22:52:28,205] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileSourceStrategy: Post-Scan Filters:
[2024-09-03 22:52:28,206] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2024-09-03 22:52:28,221] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:28,222] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:28,223] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:28,251] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO CodeGenerator: Code generated in 9.257514 ms
[2024-09-03 22:52:28,257] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.0 KiB, free 365.1 MiB)
[2024-09-03 22:52:28,264] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.1 MiB)
[2024-09-03 22:52:28,266] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.128:40415 (size: 27.5 KiB, free: 366.1 MiB)
[2024-09-03 22:52:28,267] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:28,270] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4649150 bytes, open cost is considered as scanning 4194304 bytes.
[2024-09-03 22:52:28,294] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-09-03 22:52:28,296] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-09-03 22:52:28,296] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2024-09-03 22:52:28,296] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Parents of final stage: List()
[2024-09-03 22:52:28,297] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Missing parents: List()
[2024-09-03 22:52:28,299] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-09-03 22:52:28,318] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.3 KiB, free 364.9 MiB)
[2024-09-03 22:52:28,321] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.8 MiB)
[2024-09-03 22:52:28,322] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.128:40415 (size: 64.0 KiB, free: 366.1 MiB)
[2024-09-03 22:52:28,323] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2024-09-03 22:52:28,324] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[16] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-09-03 22:52:28,336] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-09-03 22:52:28,338] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.128, executor driver, partition 0, PROCESS_LOCAL, 5210 bytes) taskResourceAssignments Map()
[2024-09-03 22:52:28,339] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-09-03 22:52:28,354] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-09-03 22:52:28,354] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-09-03 22:52:28,354] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2024-09-03 22:52:28,392] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO CodeGenerator: Code generated in 14.692169 ms
[2024-09-03 22:52:28,396] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileScanRDD: Reading File path: file:///home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/bronze/data_engineer/extract_data=2024-08-25T00:00:00.00Z/data_engineer_20240825.json, range: 0-454846, partition values: [empty row]
[2024-09-03 22:52:28,415] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO CodeGenerator: Code generated in 14.455097 ms
[2024-09-03 22:52:28,448] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileOutputCommitter: Saved output of task 'attempt_202409032252284333259491687015020_0002_m_000000_2' to file:/home/kai/Documents/PROJETOS/twitter_extractor_airflow/datalake/silver/data_engineer/user_df/process_data=20240825/_temporary/0/task_202409032252284333259491687015020_0002_m_000000
[2024-09-03 22:52:28,448] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkHadoopMapRedUtil: attempt_202409032252284333259491687015020_0002_m_000000_2: Committed
[2024-09-03 22:52:28,451] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2734 bytes result sent to driver
[2024-09-03 22:52:28,453] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 114 ms on 192.168.2.128 (executor driver) (1/1)
[2024-09-03 22:52:28,453] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-09-03 22:52:28,454] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,153 s
[2024-09-03 22:52:28,454] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-09-03 22:52:28,455] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-09-03 22:52:28,455] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,160428 s
[2024-09-03 22:52:28,488] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileFormatWriter: Write Job 2f73bf01-397f-446b-b8ef-a71545a1dce6 committed.
[2024-09-03 22:52:28,489] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO FileFormatWriter: Finished processing stats for write job 2f73bf01-397f-446b-b8ef-a71545a1dce6.
[2024-09-03 22:52:28,525] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkContext: Invoking stop() from shutdown hook
[2024-09-03 22:52:28,535] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkUI: Stopped Spark web UI at http://192.168.2.128:4041
[2024-09-03 22:52:28,547] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-09-03 22:52:28,558] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO MemoryStore: MemoryStore cleared
[2024-09-03 22:52:28,558] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO BlockManager: BlockManager stopped
[2024-09-03 22:52:28,565] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-09-03 22:52:28,568] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-09-03 22:52:28,575] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO SparkContext: Successfully stopped SparkContext
[2024-09-03 22:52:28,575] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO ShutdownHookManager: Shutdown hook called
[2024-09-03 22:52:28,576] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-9df3e341-0d8a-4850-ad30-0f9ab4d1f302/pyspark-c14e1c73-a95f-4cb9-bef6-d28bed44324f
[2024-09-03 22:52:28,578] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-9df3e341-0d8a-4850-ad30-0f9ab4d1f302
[2024-09-03 22:52:28,580] {spark_submit.py:495} INFO - 24/09/03 22:52:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-8461ebef-ee61-4480-a307-d7391f7ed0c0
[2024-09-03 22:52:28,694] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=Twitter_extractor_DAGs, task_id=SILVER_LAYER_LOAD_L, execution_date=20240825T000000, start_date=20240904T015219, end_date=20240904T015228
[2024-09-03 22:52:28,751] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-09-03 22:52:28,780] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
